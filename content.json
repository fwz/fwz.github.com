{"meta":{"title":"Wenzhong's Playground","subtitle":"half coder, full player","description":"Wenzhong's blog","author":"Wenzhong","url":"http://fwz.github.io"},"pages":[{"title":"","date":"2012-04-07T12:38:01.000Z","updated":"2019-06-10T16:17:21.863Z","comments":true,"path":"about-me/index.html","permalink":"http://fwz.github.io/about-me/index.html","excerpt":"","text":"简介 6年后端系统设计和开发经验，主要包含 调度系统 / 订单系统 架构和业务设计 数据处理流水线，数据仓库，商业智能工具，数据可视化项目和数据异常检测的设计 / 开发 用户行为数据分析 &amp; 推荐算法模型训练 工作经历NIO 蔚来汽车策略分析部门 高级技术经理 （ 2016/09 - now ） 蔚来服务云 (NIO Service Cloud)：后端技术负责人 负责 Power Express（一键加电） 的调度系统、订单系统、电力资源管理系统、服务推荐系统、支付系统、A/B Testing 平台、仿真测试平台的开发 Tech Stack Server Side Dev: Spring Boot / MySQL(Percona) / Redis / Cassandra / Kafka FE Side Dev: deck.gl / d3.js / Echarts DevOps: Kubernetes / Docker / Consul / Nginx / Jenkins / Gitlab CI 价值委员会产品-工具组副组长 Qunar 酒店云平台 高级工程师 &amp; 研发经理 （ 2015/04 - 2016/08 ） 去呼呼管家：后端技术负责人 酒店云平台事业部商户端系统。为酒店管理者提供完整的订单控制、房间控制、智能门锁控制服务。（10人团队）。 Tech Stack: Spring / MySQL / Memcached / Message Queue 数据产品线：负责人 酒店云平台的埋点系统、数据导出平台、Dashboard、A/B Test 框架、实时可视化系统。 Tech Stack: Spring / MySQL / Javascript / Bootstrap / Echarts / Python / Pig / Node.js / Grafana 持续集成：技术负责人 自动化环境构建，单元测试、集成测试框架落地 Tech Stack: Docker / Jmeter / Robot Framework 其他角色 技术委员会成员、Qunar Hackathon 命题者和评委 工作绩效——Top 5% Yahoo! 北京研究中心 数据工程师 （ 2012/04 - 2015/04 ） CORE/Compass 数据系统 利用 TB 级别用户数据和日志，从多角度多维度计算用户参与度指标和系统性能指标;指标是决定雅虎个性化推荐平台产品逻辑和推荐算法上线的主要参考因素;参与系统所有模块架构设计和开发 Tech stack: MySQL / Hadoop / Pig / Oozie / HBase / Git / Hudson 雅虎个性化平台分布式运行时日志传输系统 该日志系统提供灵活的可扩展性以及客户端数据格式定制支持，并能对接其它外部数据源，真实还原系统服务状态和用户响应事件，为提高推荐质量提供精确数据基础。主导所有模块架构设计和实现。 Tech stack: Java / Flume / Hadoop Compass 数据系统前端和推荐算法可视化工具 参与模块设计和实现 Tech stack: JavaScript / Node / Angular / Bootstrap Compass Captain Morgan, 异常检测系统 该系统对 Compass 数据系统的 数据状态进行检测,对数据源缺失,数据延迟,数值异常,数据质量降低等问题进行监测和报警,降低 Compass 系统的维护和问题定位成本。参与模块设计和实现。 Tech stack: Hadoop / Java / Python / Selenium 其它项目 数据报表和分析报告可视化。对不同数据采用不同展示方式令数据更易理解 推荐模型快速更新。将推荐算法的 模型生成扩展到 Hadoop 集群上,使得模型训练时间缩短,能更快将用户的 反馈应用到推荐算法上 拼写检查模型训练 Tech stack: D3.js / R / Processing / GBDT / N-gram LM 工作绩效——Top 20% 教育信息 北京邮电大学 计算机科学与技术 2009-2012 (Rank: 2/90) 中国科学院计算所 客座学生 2008-2010 北京邮电大学 信息安全 2005-2009 (Rank: 3/100) 奖项 「Power Express Service - 蔚来价值荣誉大奖」 (2018) 「蔚来之星」 (2018) 「Quhuhu Coding Contest 2016」冠军 （2016） 「Mailgazine」, 邮件阅读及归类Chrome Extension，雅虎 2014 Q1 Hackday 北京最佳应用大奖 （2014） 「Compass Grid Usage Visualization Tool」，雅虎 2014 Q2 Hackday 亚太地区最佳基础服务应用 （2014） 「第五届北京邮电大学 ACM-ICPC 竞赛银牌」（2012） 「腾讯高校创新俱乐部最佳主席」（2010） 出版物 「敏捷数据科学:用 Hadoop 构建数据分析应用」译者，O’Reilly Press（2014） 音乐日记 「基石学院课程：打造高效率的学习工作宇宙」，北辰青年未来大学（2019） 专业证书 &amp; 课程 Machine Learning / Data Analysis and Statistical Inference / Computing for Data Analysis / Game Theory / Model Thinking (Coursera certified) Modern Musician (Coursera) Self Driving Engineer (Udacity，WIP) Language: BEC(High) / TOEIC / CET-6 业余爱好 &amp; 兴趣 游泳：Qunar 2015 运动会：200m 男子自由泳冠军，4*100m 男子蛙泳接力冠军 音乐：键盘 / 鼓 / 吉他 / 主唱"}],"posts":[{"title":"未来大学：文中的职场生命故事","slug":"未来大学：文中的职场生命故事","date":"2019-05-01T16:28:22.000Z","updated":"2019-05-02T05:29:33.044Z","comments":true,"path":"2019/05/02/未来大学：文中的职场生命故事/","link":"","permalink":"http://fwz.github.io/2019/05/02/未来大学：文中的职场生命故事/","excerpt":"受未来大学的邀请，我为未来大学的学员主讲了一期课程。其中有一段内容，是向学员介绍我的「职场故事」。我选择了「如何建立主动意识，明确方向快速蜕变」这个方向。 课程由北辰青年策划，课程内容由昕睿、文中共同完成创作。","text":"受未来大学的邀请，我为未来大学的学员主讲了一期课程。其中有一段内容，是向学员介绍我的「职场故事」。我选择了「如何建立主动意识，明确方向快速蜕变」这个方向。 课程由北辰青年策划，课程内容由昕睿、文中共同完成创作。 Part 1今天我给大家介绍一下我职业生涯迄今最重要的一个挫折。 2012 年的春天，我研究生毕业，入职 Yahoo。这个时候，我的部门正准备研发一个新的「个性化推荐系统」，用来支撑 Yahoo 全球的首页推荐。 推荐系统是一个非常庞大和复杂的项目，它要根据你的浏览行为，学习你的兴趣口味，尽可能给你推荐丰富多样而又能吸引你的内容。Yahoo 的首页有日均亿级别的流量，对系统的性能也有不小的挑战。所以对于北京团队来说，这是一个让人非常兴奋的研发任务； 我当时被直属经理安排在为推荐系统服务的数据系统工作。数据系统最主要的作用，就是帮助评估推荐系统的效果，负责数据系统的技术 Leader 也成为了我的导师。 入职后的半年里，我对一切都感到新奇，周围有陌生又有趣的知识，友善又资深的同事，我如饥似渴地学习工作。对我来说，那半年的技术成长是巨大的。 时间很快就到了年度的绩效考核。 作为新人，我当时其实并不知道绩效考核意味着什么，所以我也在汇报的时候，自豪地总结了一下「自己过去半年的成长」。因为，对我而言，那半年我的技术能力得到了很大的提升，即使不能说特别满意，但是肯定也没有虚度时光。 一段时间之后，我拿到了我的绩效反馈，总体评价是： Occasional Miss。 Occasional Miss 是什么意思呢？后来我才知道，在外企中，拿到这个结果，意味着你的工作成绩还没到及格线，只是还不至于无药可救需要被踢出团队（也就是倒数第二档）。 就好像自己花了半年准备去考试，估分起码有 85 分，结果只拿到 50 分。 当时拿到这个结果，我自己还是非常委屈的，毕竟自己这半年都是全力投入的，觉得我既没有辜负自己，也没有辜负团队，凭什么会拿到不及格这个成绩呢？ 思来想去，心里有些愤愤不平，我特别希望我的 Manager（也就是部门经理） 告诉我，我为什么会得到这样的评价？至少也需要告诉我该怎么去提升，以及如何才能获得和我的付出相匹配 的评价。 虽然当时团队架构做了一次调整，我原来的 Manager 被调到了其他部门，但我还是试着约了他一场单独的对话。 这场对话进行了半个小时，谈完之后，我领悟到了很多道理。 从我 Manager 的角度，之所以给了我那样的评价，是因为以下的原因： 当时我做过一次对外沟通，结果我当时大脑犯蒙，问了一个比较低级的问题，我的 Manager 立马出来帮我打了圆场。 这个绩效考核在我身上，更多的是一个相对值。我可能做得还不错，但我同期的同事有更亮眼的地方。 回到自己的座位上，我又多想了一点：全球首页推荐这个项目非常庞大，而那个阶段整个部门的重心都在推荐系统的建设上，所以他精力很难顾及到每个团队，每个个人身上；再加上当时直接带我的是我的技术 Leader，所以 Manager 对我的工作内容和产出，不一定特别了解。再说，跟他直接干活的兄弟，他肯定也要照顾好。 所以，总结下来就是： Manager 不知道我在做什么； Manager 看见了我不专业的一面； 我没有表现出竞争优势；所以我得到了这个虽然不应该属于我但事实上又挺合理的结果。 当时多少是有点觉得人生苦涩的。兢兢业业干活得了个这样的结果。不过感伤归感伤，知道了背后的原因以后，就应该振作起来扭转局面了。 Part 2我梳理了一下 Manager 告诉我的信息和环境局面，重新反思了一下： 第一点是，我不应该只埋头做自己觉得正确的事，要顾及到 Manager 和他的整个团队的目标和重心。我应该主动认领一些部门 Leader 觉得比较重要甚至是棘手的事情，尝试来解决。 第二点是，我应该多向上沟通，多获取反馈，做事情的方法有了偏差，就要尽早拉回来。而不是等到绩效考核的时候被动地去接收。 那次反省之后，我重新调整了我的工作状态。然后在接下来的两个季度里，我去主动认领了和推荐系统有关的核心工作任务。 我当时需要和美国的科学家一起，统计用户在相关推荐产品上的停留时间。当时 Yahoo 每天的页面浏览量是亿级别的，这个任务需要分析和管理海量的用户行为数据，对我来说挑战十分大。虽然那段时间非常难熬和痛苦，但最终还是扛下来了。 在和 Manager 的接触上，我也有了很大的变化，我会定期去找我的新 Manager 一对一谈话，理解他目前想达成的目标、了解他对我的工作表现的反馈。因为有了这种积极沟通，我的工作始终保持在正轨之上，和团队重心保持一致。 半年过去了，我有了更好的产出、和 Manager 沟通更密切、变得更职业，这些成果帮助我取得了团队和 Manager 的信任，很快我的绩效就上升到一个合理的水平了。 在庆幸自己做了正确的选择之余，我也重新审视了一下自己：我应该怎么做才能为团队更好的作出贡献，怎么做才能成为团队中不可或缺的成员呢？ 这个时候，我想起了刘未鹏老师的文章《什么才是你的不可替代性和核心竞争力》，觉得也是时候开始探寻和建立我自己的核心竞争力，为团队创造不一样的价值了。 在综合考虑了个人兴趣、技能树和团队职能以后，我选择了「数据可视化」这个方向。然后我就在周末的时间不断地去学习相关的理论和前沿的编程工具，研究怎么样可以在这个体系里，把用户所关心的数据以更好的方式，像讲故事一样展示出来，研究怎么样可以更好地揭示数据之间的联系，研究怎么样可以 赋予用户能力，让他们进一步探索数据。 一段时间后，我独立做出了几个小应用，把由枯燥数字组成的「数据处理流程」和「数据存储空间消耗分布」，用一个更清晰美观的方式展现了出来。我拿着它们展示给我的 Manager，她当时非常惊喜，马上就转发给我们的用户看。用户也给出了好评，还说「一定要加到我们的产品中去，文中，Good Job！」。 后来在公司的 Hackday （黑客日）中，我给这个功能又加入了一个物理引擎，数据的变化趋势展示得更为自然和真实，得到了评委的青睐，拿到了一个分区大奖。最后，这个作品还入选了 Yahoo 年度技术大会 Tech Pulse 的 Poster。 自那之后，我在这个领域慢慢树立了自己独特的影响力，而我职业的道路也因此变得越来越顺畅。 Part 3再到后来，我也成为了技术 Leader，开始带团队。我也开始做绩效沟通、考虑如何激发团队的斗志，考虑如何把人放在最合适的位置上，考虑如何让团队成员增加曝光，建立技术声誉。渐渐地，我会发现，一个团队真的很需要那些主动思考，提出自己见解，并自发执行的人。 有人说，船长在船舱里和大家是闹成一片的，但当他独自站在船头看方向时，他的内心是孤独的。 作为一个 Leader，除了要结合公司使命，确定团队的整体目标，带领团队完成任务以外，还要应对很多隐形问题：例如考虑团队的整体发展和建设方向，作出利害攸关的决策，攻克复杂困难的问题。总会有一些时候，Leader 需要一些有力的帮助。假如有伙伴可以主动站出来，帮助 Leader 和整个团队分担一些责任，或者处理一些难题，Leader 一定会更信任和看重这样的伙伴。 更多时候，Leader 能在核心方向上进行把控，但很难有精力事无巨细地去了解每个小伙伴的工作状况和心理状态。很多具体的工作，作为 Leader 其实并没有一线的伙伴清楚其中的难处。这个时候，Leader 其实非常需要大家的主动沟通和反馈的。 所以成为 Leader 后，我也更能理解，当年我的老板在绩效考核中给出的评价。我误以为他会看到我的努力和表现。但事实上，如果我不及时地跟他反馈和沟通，他在那个时候，是很难深入地了解我的，他也只能凭借和我的一两次接触，作出推理和评价。 现在，我再去回过头看我刚毕业时的经历。我非常庆幸能提早就遇到那样的小挫折，那是我职业生涯里一笔非常巨大的财富。 这个挫折来得不晚也不狠，它给了我充足的时间去反思和调整，我应该如何主动去保持和团队Leader的良性沟通，不然很容易自己埋头苦干，然后偏离了轨道。 而且那次也促使我去意识到，我应该去主动寻求和建立自己的差异化优势，一方面可以为团队带来更大的价值，另外一方面，也极大地帮助了我自己的职业发展。 正是在这种主动意识的推动下，我也是那个时候开始去承担团队里更多有挑战的任务。现在回头看，当我去主动选择了工作内容，而不是被动地等待任务掉到头上时，我能够获得更多的主动权，甚至一定程度上会得到更多的宽容和理解。 神奇的是，这些有挑战性的事情，（一般情况下）只要找对方法，总是可以顺利推进，得到好的结果的。当一件件事情相继有好的产出以后，我们在团队中的影响力，就会不断被强化，然后就会逐渐承担更有挑战性的工作。往往就是在这个过程中，我们能够加速提升个人能力，扩大工作职责，得到更多的发展机会。这是一个正向循环和强化的过程。 Summary总结一下，我在职业生涯的早期，学到了三个「主动」：分别是「主动向上沟通」、「主动建立自己的差异化优势」以及「主动承担责任」。有了这样的意识和担当，你会发现自己能够以他人不能理解的速度，在职业发展道路上飞速（地）成长。 遇见未来更好的自己，这里是未来大学。我是文中，谢谢你的阅读。","categories":[{"name":"Career","slug":"Career","permalink":"http://fwz.github.io/categories/Career/"},{"name":"Growth","slug":"Career/Growth","permalink":"http://fwz.github.io/categories/Career/Growth/"}],"tags":[]},{"title":"Sim Cloud：蔚来服务云仿真平台设计","slug":"sim-cloud","date":"2019-02-20T15:02:54.000Z","updated":"2019-05-02T08:21:39.049Z","comments":true,"path":"2019/02/20/sim-cloud/","link":"","permalink":"http://fwz.github.io/2019/02/20/sim-cloud/","excerpt":"「一键加电」是蔚来为车主提供的代客加电服务。用户在 App 中下单后，系统的调度模式如下：用户下单后，调度系统会根据用户期望的取车位置 / 取车时间、车辆状态（例如续航里程）等因素，求解服务专员和服务电力资源的组合方案。系统在求解过程中会考虑一系列的地理信息因素（如限行围栏、道路状况等），然后结合服务人员的空闲情况、电力资源的服务能力和再生能力，以及系统的配置，求出服务距离、服务时长综合最优的服务方案。 为用户提供的加电资源主要有两种：换电站和移动充电车。换电站可以在 3 分钟内将旧电池拆下，换上新电池，提供非常好的加电体验。移动充电车则适合于远郊或低电量场景，除换电站和移动充电车以外，在部分资源还没有密集布局的地区，我们也会利用三方的充电桩进行加电服务。 以下是蔚来一键加电的服务流程图： (其中虚线框表示服务流程，绿色段表示对资源的占用，蓝色段表示对服务专员的占用)。 随着车辆的逐渐交付，线上一键加电的订单需求日益增多。为保障一键加电的服务完成率和准时率，我们有大量的优化问题需要被解决，如： 预测 traffic model 的变化可能带来的服务时长、人员利用效率的影响 预测不同的人力和电力资源供给情况下，系统的服务能力上限 为服务人员的资源布局提出建议 评测待上线的算法和调参的效果 这些问题对系统的调度性能提出了离线和可测量要求。 对于包含调度行为的 O2O 系统，服务承载能力难以通过一个简单的模型或者线性公式进行数学推演，直接得出有参考意义的指标。为了更好地评测和调优系统，以及给出一线运营资源的配置建议，我们设计并实现了 Sim Cloud —— 一个可以精确重建线上系统和环境，引入线上流量或自定义流量，估算系统供需能力曲线、评估调度策略性能的仿真平台 。","text":"「一键加电」是蔚来为车主提供的代客加电服务。用户在 App 中下单后，系统的调度模式如下：用户下单后，调度系统会根据用户期望的取车位置 / 取车时间、车辆状态（例如续航里程）等因素，求解服务专员和服务电力资源的组合方案。系统在求解过程中会考虑一系列的地理信息因素（如限行围栏、道路状况等），然后结合服务人员的空闲情况、电力资源的服务能力和再生能力，以及系统的配置，求出服务距离、服务时长综合最优的服务方案。 为用户提供的加电资源主要有两种：换电站和移动充电车。换电站可以在 3 分钟内将旧电池拆下，换上新电池，提供非常好的加电体验。移动充电车则适合于远郊或低电量场景，除换电站和移动充电车以外，在部分资源还没有密集布局的地区，我们也会利用三方的充电桩进行加电服务。 以下是蔚来一键加电的服务流程图： (其中虚线框表示服务流程，绿色段表示对资源的占用，蓝色段表示对服务专员的占用)。 随着车辆的逐渐交付，线上一键加电的订单需求日益增多。为保障一键加电的服务完成率和准时率，我们有大量的优化问题需要被解决，如： 预测 traffic model 的变化可能带来的服务时长、人员利用效率的影响 预测不同的人力和电力资源供给情况下，系统的服务能力上限 为服务人员的资源布局提出建议 评测待上线的算法和调参的效果 这些问题对系统的调度性能提出了离线和可测量要求。 对于包含调度行为的 O2O 系统，服务承载能力难以通过一个简单的模型或者线性公式进行数学推演，直接得出有参考意义的指标。为了更好地评测和调优系统，以及给出一线运营资源的配置建议，我们设计并实现了 Sim Cloud —— 一个可以精确重建线上系统和环境，引入线上流量或自定义流量，估算系统供需能力曲线、评估调度策略性能的仿真平台 。 名词简介 服务专员：为用户进行加电服务的专员（以下简称专员）。 换电站：蔚来自研的电池更换站点，可以在3-5分钟内为蔚来的全系车辆更换电池，占地约3个正常车位。 充电桩：一个连接电网，通过充电插头向电动汽车充电的充电设备。 充电桩群：充电桩一般批量建设，一个批量建设并提供充换电停车空间的场所即为桩群。 移动充电车：蔚来自研的移动充电设备。充电车自带电池，可以利用自身的电量向其他车辆充电。 加电资源：在下文中特指 移动充电车、换电站、充电桩群。 资源：在下文中特指专员、移动充电车、换电站、充电桩群。 SOC: State of Charge，反映电池包内当前电量和总体可用电量之间百分比比值的电量。 取车位置：用户下单时指定的专员取车位置。 ETA：Estimated Time of Arrival，（特定事件的）预计完成时间。 服务步骤：包括前往取车、取车地点寻车、前往到服务点、充换电、还车到原定位置等一系列为完成一键加电订单的专员活动。 架构目标和挑战如上文所述，我们希望 Sim Cloud 系统可以精确仿真线上环境，回放线上下单流量，达成以下业务目标： 模拟线上系统在给定资源数量、状态及分布情况下的服务承载能力。 测试线上系统在不同的业务配置和算法策略下的服务能力表现。并通过以下指标，指导运营及开发人员调整配置及调度算法，以达到调度效率与准时的最佳平衡点。 系统效率指标 [下单成功率，最大下单成功数，专员移动距离及服务时长] 用户满意度指标 [准时率,用户感知服务时长] 通过控制变量，模拟出资源的位置对系统效率指标及用户满意度指标的影响，以指导未来资源的地理分布和数量规划。 除了上述业务以外，从工程层面，我们也致力于： 提高订单流量的模拟速度 降低开发和维护成本 保证模拟能力，能满足各种各样的线上事件模拟 要实现上述的设计目标，会面临一些挑战。在整个仿真流程中，确定性仿真是我们最关注的问题，仿真是否有效会直接影响到实验结果。此外，数据安全、仿真速度、重复仿真也是 Simulator 成败的关键。基于此，我们把 Simulator 需要解决的关键问题归类如下： 确定性仿真 一个订单从开始到结束的一个完整生命周期内，模拟器需要根据每一步的状态决定下一步的操作及时间。 在保证安全性和速度的情况下拉取线上数据，尽可能真实的还原线上状态。（专员的排班，服务能力，服务城市，使用的工具，上班位置，加电资源的分布，状态，在各个系统中关于专员、加电资源、用户车的状态及位置） 用户车、专员、用户、及其手机号等机密信息，需要在使用之前替换掉，并保证所有依赖数据同步。 有些服务由于需要车机联网或者证书等依赖，模拟环境无法调通，需要mock。 加快模拟器中的时间流速 重复仿真：仿真线上环境在不同的业务配置和算法策略下的服务能力表现，需要有重复还原线上环境的能力。 整体流程介绍Sim Cloud 进行一次完整仿真有以下的几个步骤 拉取 / 准备数据 初始化环境 回放流量 / 事件 效果评估 下面逐一介绍每个过程要解决的问题和方法： 拉取 / 准备数据Sim Cloud 目前能根据两种方式生成实验数据输入（Simulation Input)：线上真实数据的分布 / 实验者自定义分布。通过指定不同的数据生成配置，实验者可以使用数据生成器，生成不同的实验数据输入。 对于不同的数据生成方式，我们通过不同的 generator 来生成对应的 simulation input data。以线上流量分布生成为例，generator 会生成以下四类数据 专员：拉取线上专员的排班、岗位数据 加电资源：拉取线上加电资源数据，包括移动充电车、换电站、充电车的状态、位置、服务能力配置 下单意图：拉取线上订单信息，将所有有下单行为的用户和车辆信息，使用资源库数据进行脱敏，并增加下单时间 系统及业务配置：拉取将线上核心系统的配置快照 通过以下的方式，我们就能开始将服务专员、加电资源、系统及业务配置，用户车信息等同步到仿真环境。 对自定义的供需分布，我们可以产生根据参数，生成确定性的输入和输出，并允许用户手动调整最终的结果。 初始化环境环境的初始化主要包含两个步骤： 将业务数据注入到 Sim Cloud 环境（用户 / 车辆 / 服务人员 / 电力资源） 初始化 Sim Cloud 各子系统的配置 完成后，Sim Cloud 能感知到的环境上下文就完成构建了。 回放流量 / 事件动态事件模拟由于拉取线上的数据是瞬态的，用线上数据初始化仿真环境的做法对于某些动态事件（例如专员上下班、绑定/解绑工具、系统派单、服务步骤、用户下单、下单时的用户车SOC及位置变化）就无能为力了。根据发生的原因这些事件可以分为以下两种： 非依赖事件：不依赖其它事件，到固定的时间就会执行。比如专员上下班，系统派单。依赖事件：依赖其它事件，比如服务步骤里面的几个事件都依赖前面的步骤，毕竟取车失败也就不会有后面的到服务点加电事件了。 我们使用一个按时间自动排序的 Event Queue 来模拟动态事件。在 SIM 运行之前，会初始化一些非依赖事件以及下单事件到 Event Queue 中。SIM 运行时内部会维护一个当前时间（与现实的当前时间无关），每次都会将预期该时间执行的 Event 取出并执行。那么对于依赖事件，比如服务步骤中取车 Event 执行完并且成功，会根据返回向 Event Queue 放入服务步骤的下一步及其触发时间。当 Queue 中没有 Event 时，SIM 运行也随之停止。 效果评估 在仿真完成以后，会进入效果评估阶段。 效果评估和线上的指标统计 pipeline 使用相同的 code base，通过从业务数据的存储介质中拉取数据，对整体的指标进行汇总计算。 Tricks在开发 Sim Cloud 的过程中，遇到过不少痛点，这些困难在所有仿真系统都可能遇到，这里也分享一下我们的方案。 依赖管理为了准确定义系统行为和获取评测数据，仿真环境最好能提供系统层面和数据层面的隔离。这里就存在系统要完整跑通时，依赖系统过多的问题。一键加电业务的核心模块约有10个，而其他相关的服务子模块 / 微服务大概有 40 多个，涉及的后台基础设施（例如存储）可能会更多。部分系统也有跨组或跨部门的依赖，环境的协调和搭建会比较耗时。 目前我们进行了几个改动，尽可能减少依赖： 对于非核心模块的调用，假如不影响系统调度行为，则 NSC 中配置开关进行降级。线上开启，仿真关闭。关闭时使用默认降级行为进行处理。 部分难以跳过，但又不甚稳定的模块，自己进行 mock。 经过一段时间的优化，要单独维护仿真环境的模块下降至 5 个左右。比起 40，属于比较理想的状态了。 有同学可能会问，有没有考虑过线上压测的方式？这里有几方面的考虑： 离线仿真和线上压测的频率不同。线上压测可能只做一两次，每次用特定流量进行测试。离线仿真会抢时间做实验，一系列的离线实验可能排队 7*24 小时高速运转，所以产生的数据量会比压测大很多。 离线的实验会对调度逻辑进行改动，这部分的代码更多是为了验证想法，花在性能优化和业务逻辑的回归覆盖上的时间有限。即使在只加流量的场景中，相比于离线仿真，线上压测也会增加线上系统的稳定性风险。 数据标注。线上压测的情况下，全链路都需要对数据进行标注，避免统计数据产生偏差。 当然单独维护一套环境也会带来额外的维护成本，例如线上版本的更新就会带来 Sim Cloud 的事件队列处理和线上版本的兼容性维护工作。 仿真批量编排想要高效逼近优化目标，需要有能力编排出一系列的仿真，对不同的调度策略和参数配置进行验证，甚至参数的自动化调整。Sim Cloud 在前端提供了类似 DSL 的支持，实验者可基于自己的想法或需求，生成一系列用于各次仿真生成仿真环境的配置，然后批量提交给仿真管理器。仿真管理器接收这一批次的仿真以后，就能根据实验者的指示自动触发。 举个例子，当需要估算线上服务承载能力极限的时候，我们需要不断调整输入单量和分布。这种情况下，我们可以先指定系统拉取最近 10 个周六的线上流量，然后指定 10 个采样比例，然后编排 10 个实验，每个实验都将采样后的流量压缩到一天，统一提交仿真测试。所有仿真完成以后，在报告页面查看各次仿真的结果对比。 确定性我们对仿真的基本要求是相同的输入能够得到相同的输出。对于 O2O 的业务系统，有较多的自由度或随机性，需要被管理或规范化，例如： 根据数据分布生成仿真事件时的随机性。指仿真发起者在指定输入数据分布时，要确保相同的输入产生相同的输出或采样结果，这一般可以通过指定随机数种子保证。 调度系统中实现层面的随机性。指对于相同的上下文环境，系统需要从实现层面上保证确定性的输入和输出的对应一致。这是对业务系统的正常要求，但在并发且对资源存在竞争的情况下，也需要有能力保证。 仿真系统中定义的事件和调度系统的时钟系统不一致。事件的触发时间和真实输入仿真系统的时间肯定是不一样的，因此仿真系统需要有能力在系统内部指定和应用事件的「发生时间」。这里我们对每个仿真事件都增加了时间信息，而调度系统和资源系统的全部接口通过 AOP 增加了 custom_timestamp 的公共参数用以接收外部的特定时间信息。然后，调度系统封装出一个获取当前时间的 Helper 函数，当调用中存在 custom_timestamp 信息时，Helper.getCurrentDate() 会返回自定义时间，而非 new Date()，这样可以无侵入地让系统增加指定接口调用时间的能力。为了避免人为的疏漏，我们还增加了对 new Date() 代码检测的单元测试，确保没有新代码因为不熟悉此规则进入主干分支，静默引起不确定性。 仿真效率 上述的独立时钟系统的设定，也能起到加速作用 —— Simulator 不再需要等待真实时间的消逝，只需要按照时间戳维护好事件队列，就可以通过按序执行事件，得到和真实世界相同的时序和数据变化。这能够大大加速仿真的执行效率 和很多时间敏感性型的功能（例如爬虫）一样，Sim Cloud 不希望自己的时间是有闲置的，而效果评估（Spark Job）部分，因为依赖仿真产出的业务数据，而本次仿真产生的业务数据，又应该在下一次仿真发起之前清除。所以假如要基于业务系统的存储进行评估，就会阻塞下次的仿真的触发。为了提高效率，我们将业务数据先 dump 到一个额外的存储上，然后并行通知：1. Evaluation 模块利用额外存储数据进行评估；2. Simulator 清理上次仿真生成的数据，并开始进行下一次仿真。统计和仿真互不阻塞，这样就为连续仿真节省了较多的时间。 小结Sim Cloud 系统目前的特性： 高精还原线上环境，能指定回放特定的下单流量，评估系统效率指标 仿真速度快，一天订单流量在 5 分钟以内就回放完成 提供实验管理器，支持基于配置的参数化实验 提供不同实验之间的对比功能 蔚来的「一键加电」项目在 2018 年获得了蔚来年度「超越期待的用户体验」价值成就大奖。在不断优化调度策略、缩短服务时长方面，Sim Cloud 也发挥了举足轻重的作用。","categories":[{"name":"Engineering","slug":"Engineering","permalink":"http://fwz.github.io/categories/Engineering/"},{"name":"Simulation","slug":"Engineering/Simulation","permalink":"http://fwz.github.io/categories/Engineering/Simulation/"}],"tags":[{"name":"Simulation, NIO","slug":"Simulation-NIO","permalink":"http://fwz.github.io/tags/Simulation-NIO/"}]},{"title":"Practical Apache Kafka Usage","slug":"Practical-Kafka-Usage","date":"2018-07-28T16:24:02.000Z","updated":"2019-05-02T17:20:41.772Z","comments":true,"path":"2018/07/29/Practical-Kafka-Usage/","link":"","permalink":"http://fwz.github.io/2018/07/29/Practical-Kafka-Usage/","excerpt":"I collect some of Apache Kafka usage in our team and write an internal post in Chinese. And I would like to pull the fundamental part into this post.","text":"I collect some of Apache Kafka usage in our team and write an internal post in Chinese. And I would like to pull the fundamental part into this post. 重要配置Producer生产者有很多可以配置的参数，大部分有合理的默认值，详情可以参考 Kafka 的文档。这里摘录了部分会影响生产环境中数据正确性的关键参数，并进一步解释如下。 parameters description acks 指定有多少个分区副本确认收到消息，生产者才会认为消息写入是成功的。 如果 ack = 0, 那么生产者在成功写入消息之前不会等待任何来自服务器的响应。假如消息发送过程中出现问题，导致服务器没有收到消息，生产者也无从得知，消息也会丢失。这种模式适合高吞吐量但对可靠性没有很高要求的场景（例如埋点数据的收集）但由于不需要等待服务器的响应，所以能够达到很高的吞吐量。（注意此时 retries 参数也不会起作用）如果 ack = 1, 那么只要集群的 Leader 收到消息，生产者就认为服务器成功接收消息。如果消息无法达到首领节点，那么生产者会收到错误响应并进行重试。但这里可能出现的问题是，如果一个没有收到消息的节点在旧首领下线以后成为新的首领，消息还是会丢失。如果 ack = all, 当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。这种模式是最为安全的，适合严格要求消息不能丢失的情况（例如支付、订单等相关消息）。然而由于要等待所有节点接收消息，所以延迟会比 ack = 1 时更高。 retries 决定 Producer 在临时性的错误下的重试次数，如果达到这个次数， Producer 会放弃重试并返回错误。每次重试之间的间隔，也可以通过 retry.backoff.ms 来配置。重试次数和重试间隔的乘积，应该大于一个常规的节点崩溃并回复的所需时间（例如所有分区选举出首领需要多少时间），以避免 Producer 过早放弃重试。一般情况下，我们在代码中只需要处理那些无法通过重试成功发送的错误，或重试次数超出上限以后的情况。 linger.ms Producer 会在 batch 被填满或者当前 batch 已经等待了 linger.ms 时，把 batch 中的所有消息一起发送出去。对于吞吐量较小的主题，这个值会影响消息的延迟。 max.in.flight.requests.per.connection 指定 Producer 在收到服务器响应之前可以发送多少消息。默认为5，提高此值可以提高吞吐量。但当值大于1时，在发送出现失败后重试时，将有几率无法保证消息按照发送的顺序写入服务器。在高吞吐量且无需关心数据顺序（例如日志或埋点记录）时，可适量提高。 request.timeout.ms Producer 在发送数据时等待服务器返回响应的时间 timeout.ms 指定 broker在等待同步副本返回消息确认的时间（与 acks 配置相互作用），如果在指定时间内没有收到同步副本的确认，broker会抛出错误。 enable.idempotence 设置为true时，Producer会保证消息只发送一次。设置为 False 时，重试就可能会引起多条数据发送到集群中。注意， enabling idempotence 要求 max.in.flight.requests.per.connection = 1 且 retries &gt; 0 且 acks = ‘all’ Consumer parameters description fetch.min.wait.ms 用于描述broker的等待时间，默认是500ms。如果没有足够的数据（fetch.min.bytes)流入，那么消费者会等待该参数指定的时间，然后返回所有可用的数据 max.partition.fetch.bytes 指定分区返回给消费者的最大字节数，默认为1MB。 KafkaConsumer.poll() 方法从每个分区里面返回的记录，不会超过配置中返回的字节数。如果一个主题有20个分区、5个消费者，那么每个消费者需要至少4MB的可用内存来接收记录。 session.timeout.ms consumer 与 coordinator 之间 session 超时时间，heartbeat request 将刷新此 timeout。如果 coordinator 发现 session 超时，将触发 rebalance enable.auto.commit 如果设置为 true，那么 poll 方法中接收到的最大偏移量就会被提交。提交间隔由 auto.commit.interval.ms 控制。自动提交是在轮询里进行的，假如每次在轮询时会检查是否改提交偏移量，如果是 max.poll.records 控制单次轮询处理的记录数 max.poll.interval.ms consumer 两次 poll 消息之间的最大间隔时间，poll request 将刷新此 timeout。如果 consumer 发现两次 poll 中间间隔时间超出此值，将主动发出 leave group 请求，该请求会使得此 consumer 离开所在的 consumer group，并触发 rebalance （注：如果 consumer 主动离开所在的 consumer group， 那么将会暂停 heartbeat request 的发送，下一次 poll 发生时会尝试重新加入 group） 同时改值也代表着 rebalance timeout，在一次 rebalance 中，如果在该时间内 consumer 未 join group，那么 consumer 将被认为离开了 group heartbeat.interval.ms consumer 两次发起 heartbeat 请求之间的间隔。该请求会刷新 session timeout 时间。如果 coordinator 发起了 rebalance，consumer 会通过该请求得知 rebalance 的发生，并发送 join group request 加入该 group 可靠性在讨论消息系统的可靠性时，要先考虑两个问题： 是否需要保证消息的可靠发送和消费，不丢数据。 是否需要保证消息的发送和业务数据的一致性 （一般是最终一致性，不要发送错误/不一致的数据） 假如 1 的答案为「是」，那么请继续阅读「Producer 可靠性 / Consumer 可靠性」章节；假如 2 中的答案为「是」，可以参考「事务型消息」章节来进行处理。 Producer 可靠性Producer 需要处理的错误包括两部分：Producer 可以自动处理的错误 / Producer 无法自动处理，需要开发者单独处理的错误。 Producer 可以自动处理的错误，依赖于 Kafka 集群的自愈机制，例如 Leader Election / 网络问题。在这些情况下，Kafka 是可以在几秒到几十秒之间得到解决。而假如返回「INVALID_CONFIG」/ 「认证错误」之类的错误，那就不是重试可以解决的了，此时需要开发者单独处理。 大部分情况下，我们希望不丢失消息，那么最好让 Producer 在遇到可重试错误时能够保持重试，这样开发者就不需要加入额外手段去处理这些问题。 重试的过程中，Producer有一定可能向 broker 写入不止一条的消息：例如由于网络原因，消息实际已经写入，但 Broker 没有在超时时间内返回确认给 Producer ，那么 Producer 就会重试。但如下文所述，假如 Consumer 本身或业务本身可以做到消息消费的幂等性，那 Producer 就可以放心地使用内置的重试机制来重发消息。 Consumer 可靠性在服务重启的时候，Consumer 就会有一段时间无法工作，或引起 re-balance，在这个过程中 Kafka 是如何保证消息会被一定可以被消费的呢？ Consumer 设置中最重要的一个参数，就是 enable.auto.commit。对可靠性有要求的业务，强烈建议将这个偏移量设置为手动提交。并在业务代码中贯彻以下的几个思想。 先处理，后提交偏移量。 若处理失败，留存消息进行补偿处理或报警（不阻塞后续消费），提交偏移量。 对消息的处理支持幂等。 Consumer Rebalance子系统之间的交互大量的依赖于消息组件 Kafka 的支持。由此引发的一个问题是，当同一个 consumer group 中的机器集群的某一台机器宕机后，Kafka 以何种机制触发 rebalance，在多长的时间内可以将宕机 consumer 所分配的 partitions 重新分配，并使得消息可以继续得以消费。 Kafka Consumer Rebalance 过程 Coordinator 通过在 HeartbeatResponse 中返回 IllegalGeneration 错误码发起 Rebalance 操作。 Consumer 发送 JoinGroupRequest Coordinator 在 Zookeeper 中增加 Group 的 Generation ID 并将新的 Partition 分配情况写入 Zookeeper Coordinator 发送 JoinGroupResponse 综上所述，如果 Consumer宕机。我们可以假设 Consumer 与 Coordinator 网络请求耗时 = Ti，本地操作用时忽略不计，则一次 rebalance 从某个 consumer 宕机到整个 rebalance 完成的最大可能时长为： R = session.timeout.ms + heartbeat.interval.ms + max{Ti} 在此期间，宕机的 consumer 所被分配到的 partitions 的消息无法被处理，直至 rebalance 完成后，迟滞的消息将由新的 consumer 处理。 Rebalance 相关内容参考 https://stackoverflow.com/questions/43991845/kafka10-1-heartbeat-interval-ms-session-timeout-ms-and-max-poll-interval-ms https://cwiki.apache.org/confluence/display/KAFKA/KIP-62%3A+Allow+consumer+to+send+heartbeats+from+a+background+thread http://www.infoq.com/cn/articles/kafka-analysis-part-4?utm_source=infoq&amp;utm_campaign=user_page&amp;utm_medium=link https://github.com/apache/kafka/tree/0.11.0 事务型消息对需要进行业务解耦，要求业务状态消息更新高可靠性的系统，需要有一个能确保业务数据状态、消息发送成功一致性的消息队列，来进行业务解耦。与之对应，消息需要完成以下核心功能： Producer: 若业务状态变化了，则与之相关的对应消息最终会被发送成功。 Message Broker：acks = all Consumer: 当消息没有被成功消费时，需要被记录并再次消费。 实现 Producer 事务性的方式： 【A1】将 业务改动 和 消息日志记录落库 放在同一个事务中（留痕迹后再进行发送）。 【B1/B2】消息的可靠发送：每个 MQ 实例中，会用一个 deamon 会不断地轮训一段时间内应被发送但未发送成功的消息（带重试次数） 【A2】提高消息的实时性的方法：状态和消息一起 Commit 以后，在业务代码中的 After Commit 动作中里，自行读取消息进行发送。","categories":[{"name":"Engineering","slug":"Engineering","permalink":"http://fwz.github.io/categories/Engineering/"},{"name":"Message Queue","slug":"Engineering/Message-Queue","permalink":"http://fwz.github.io/categories/Engineering/Message-Queue/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://fwz.github.io/tags/Kafka/"}]},{"title":"A 2D Nim Game","slug":"A-2D-Nim-Game","date":"2018-02-21T13:15:26.000Z","updated":"2019-05-02T17:22:51.605Z","comments":true,"path":"2018/02/21/A-2D-Nim-Game/","link":"","permalink":"http://fwz.github.io/2018/02/21/A-2D-Nim-Game/","excerpt":"IntroThis article is about a Nim game I played in my primary school. The rule are simple: There are 16 stones, arranged in a 4 row * 4 column grid. Each player take stone(s) in turn. Once the stone is taken, there is a gap on it’s original location on the grid. In each take, one should take no more than 3 stones (1/2/3). Only those stones in the same row or in the same column without gap between them could be taken. Player who takes the last stone LOSE Let me demonstrate it. Assuming 1 means there is stone on the specific cell and 0 represent a gap in the following chart. And those state that would definitely lose are called Losing State. the initial state could be represented as (I) the ending state could be represented as (II) one of the losing state (LS) for current turn player could be represented as (III) 123451111 0000 00001111 0000 00001111 0000 00101111 0000 0000(I) (II) (III) To be more specific, the following state is a possible game.","text":"IntroThis article is about a Nim game I played in my primary school. The rule are simple: There are 16 stones, arranged in a 4 row * 4 column grid. Each player take stone(s) in turn. Once the stone is taken, there is a gap on it’s original location on the grid. In each take, one should take no more than 3 stones (1/2/3). Only those stones in the same row or in the same column without gap between them could be taken. Player who takes the last stone LOSE Let me demonstrate it. Assuming 1 means there is stone on the specific cell and 0 represent a gap in the following chart. And those state that would definitely lose are called Losing State. the initial state could be represented as (I) the ending state could be represented as (II) one of the losing state (LS) for current turn player could be represented as (III) 123451111 0000 00001111 0000 00001111 0000 00101111 0000 0000(I) (II) (III) To be more specific, the following state is a possible game. 12345Org (A) (B) (A) (B) (A) (B) (A) 1111 1110 1110 0110 0110 0110 0000 00001111 1110 1110 1110 1110 1110 1110 00001111 &gt; 1111 &gt; 1000 &gt; 1000 &gt; 1000 &gt; 0000 &gt; 0000 &gt; 00001111 1111 1111 1111 1100 0100 0100 0100 Initial analysisAccording to my primary school playing experience, all of the following states are LS (and welcome to try). 12341100 1001 1011 10101100 1001 0111 10100000 1001 0000 00000000 0000 0000 0000 Believe it or not, knowing this states make you 99% unbeatable in primary school. But I want to discover the essence of this game. So let’s do some deep dive in 20 years later. Instead of staying on the number axis, such as some basic Nim game subtraction problem) , this game is in a 2D space, and there are a lot of variation in each take. The first thought come to me is that this game is about graph theory and the state analysis becoming connected graph analysis since removing stones require connectivity. After a simple estimation of my poor graph theory knowledge, I give up this direction shamelessly and try to analyse the state space of this game. Thanks to the simple nature of this game, there are only 2^16 = 65536 states in this game. And we know that the following 16 states are absolutely going to lose the game. If we could leave this 16 states to our opponent, then we could win the game. 12341000 0100 0010 0001 0000 0000 ... 0000 00000000 0000 0000 0000 1000 0100 ... 0000 00000000 0000 0000 0000 0000 0000 ... 0000 00000000 0000 0000 0000 0000 0000 ... 0010 0001 So any state that transfer to these 16 states in one move, will be our winning state. Because if we get any of those states, then we have at least one way to transfer this state to the above 16 lose states for our opponent. For example, these states are some of the winning state towards the first state in above situations. 12341100 1110 1111 1000 1000 10000000 0000 0000 1000 1000 01000000 0000 0000 0000 1000 00000000 0000 0000 0000 0000 0000 ... After calculate all such states, we can easily get 600+ win states. Great, but how to proceed? Any state could transfer to a win-state in one move is lose-state? Not necessarily, check the first 2 states: 1100 and 1110, in the above graph. 1110 can transfer to 1100 but it still win because we could choose to leave only 1000 to our opponent. But it give us a hint for the deduction for two smart enough player, if ANY next state of current state LOSE, current state WIN And with further consideration, Only ALL next state(s) of current state WIN, current state LOSE Let’s do a quick test with ZERO state. If a player get this state, means his opponent take the last stones, so ZERO state is a win state. The previous 16 states have only 1 next state, the ZERO state. So they are indeed the lose state. Our game is a typical case of zero-sum perfect-information game and it match the Zermelo’s Theorem). In such a game, for any state, one side of the game will have a series of strategy that could definitely win the game. So it also mean any state in the game will be either a win-state or a lose-state. Seems we are making progress, but we still need a way to expand the lose-state / win-state set. This game will be conquered once all 65536 states are figured out. Solution generationWith the support of Zermelo, and the previous deduction that: if ANY next state of current state LOSE, current state WIN. else, current state LOSE we are able design the following algorithm to figure out and expand the states set from ZERO state. First, define a binary notation for a state. For example, the following state could be converted as (1111000011111111)2 = 6169512341111 00001111 1111 In this notation, for any state S and all its next states N, (S)2 &gt; (N)2, because we are taking one or more 1 from S to get N. According to deduction mentioned, if the win/lose state of S’s all next states are known, then the win/lose state of S could be figure out. It’s a quite straight forward problem that could be solved by dynamic programming – After we initialize ZERO state as winning state, we could iterate the whole state space. 1234567891011121314151617181920lose_set = set()win_set = set()# initwin_set.add(0)# start calcfor i in range(1, 65536): flag = True # if all next step of current board is a win board, current board lose for board, number in find_all_adj_board(num2board(i), 1): if number not in win_set: flag = False break if flag: lose_set.add(i) else: # must be win or lose, according to Zermelo win_set.add(i) The upper bound of count for next states is smaller than 4*4*2*3, because for any possible cell, one could at most remove 3 types of length for 2 directions (down / right) To be more generalize, for an N*N Grid with above rules, this algorithm take O(2^(N*N)*2*3*N*N) to iterate all possible states. If N = 4, it takes 30 seconds to generate all states on my computer. so it might take much more for N = 5. If you want to know whether the first player or the second player could force a win, checkout the game script I wrote on github: 2d-nim and test it! SummaryWe have discussed the 2d nim game, and use Zermelo’s Theorem and Dynamic Programming to get the ultimate strategy for this game. Although we reached the fact of the game, I still believe that there are some topology based solutions with a lower complexity for larger board.","categories":[{"name":"Game","slug":"Game","permalink":"http://fwz.github.io/categories/Game/"},{"name":"Board Game","slug":"Game/Board-Game","permalink":"http://fwz.github.io/categories/Game/Board-Game/"}],"tags":[{"name":"Game Theory","slug":"Game-Theory","permalink":"http://fwz.github.io/tags/Game-Theory/"}]},{"title":"Understanding Fstab","slug":"Understanding-Fstab","date":"2017-06-07T14:35:56.000Z","updated":"2019-05-02T05:26:30.254Z","comments":true,"path":"2017/06/07/Understanding-Fstab/","link":"","permalink":"http://fwz.github.io/2017/06/07/Understanding-Fstab/","excerpt":"Today I encounter a very strange problem that all recent deployed applications in a specific host fail to start, with a simple error message “Permission Denied. xxxx.sh could not be executed”. Nonsense! They’ve run for a very long time with a Jenkins driven deployment, started by a deploy script. And a binary “could not be executed” might be controlled by user &amp; file access flag. Both checked, using root user &amp; 755 access flag. Suddenly I remember that the hard-disk on this host have been re-mounted by Ops a few days ago, this operation might corrupt some filesystem runtime context. So I wrote a simple test script a.sh, place it into different mounting point /tmp and /data and try to execute them via ./a.sh (not bash ./a.sh because in this case we are using r way instead of x to access the script). And script could be executed under /tmp but not /data. Seems we are close to the root cause, but what stop us from executing a binary in different mounting point? The answer is the fstab.","text":"Today I encounter a very strange problem that all recent deployed applications in a specific host fail to start, with a simple error message “Permission Denied. xxxx.sh could not be executed”. Nonsense! They’ve run for a very long time with a Jenkins driven deployment, started by a deploy script. And a binary “could not be executed” might be controlled by user &amp; file access flag. Both checked, using root user &amp; 755 access flag. Suddenly I remember that the hard-disk on this host have been re-mounted by Ops a few days ago, this operation might corrupt some filesystem runtime context. So I wrote a simple test script a.sh, place it into different mounting point /tmp and /data and try to execute them via ./a.sh (not bash ./a.sh because in this case we are using r way instead of x to access the script). And script could be executed under /tmp but not /data. Seems we are close to the root cause, but what stop us from executing a binary in different mounting point? The answer is the fstab. fstab is a configuration file that contains information of all the partitions and storage devices in your computer. The file is located under /etc, so the full path to this file is /etc/fstab. /etc/fstab contains information of where your partitions and storage devices should be mounted and how. After viewing the content of /etc/fstab, we then know why things happen. Here are the content: 123456789101112## /etc/fstab# Created by anaconda on Fri Aug 26 16:02:35 2016## Accessible filesystems, by reference, are maintained under &apos;/dev/disk&apos;# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info#UUID=7e452929-a3a3-4f1f-80a3-91eced90b453 / xfs defaults 0 0UUID=b797b27d-0499-451e-a1cc-8d7fc014777c /boot xfs defaults 0 0UUID=a47c3334-ae2c-486f-9756-7172b5570035 swap swap defaults 0 0/dev/mapper/centos-data /data xfs defaults,noexec 0 0 There are 6 columns for a mount option, each represent: block special device or remote filesystem to be mounted mount point of file system file system type mount option associate with the mount dump(8) flag boot check sequence With help of man 5 fstab and man 8 mount we could see the /data mounting point is bound with a ridiculous noexec option. According to man page of fstab(5) about fourth field (fs_mntops): This field describes the mount options associated with the filesystem. It is formatted as a comma separated list of options. It contains at least the type of mount plus any additional options appropriate to the filesystem type. For documentation on the available mount options, see mount(8). For documentation on the available swap options, see swapon(8). then check mount(8) noexec Do not allow direct execution of any binaries on the mounted filesystem. (Until recently it was possible to run binaries anyway using a command like /lib/ld*.so /mnt/binary. This trick fails since Linux 2.4.25 / 2.6.0.) So problem resolved after we remove the “,noexec” option from the /data mount point. The previous statement “(Binary) could not be executed might be controlled by user &amp; file access flag.” are not accurate enough. Binary execution could also be controlled by File system mount options under “/etc/fstab”. Now it’s time to ask why Ops assigned such a flag on this mount…","categories":[{"name":"Engineering","slug":"Engineering","permalink":"http://fwz.github.io/categories/Engineering/"},{"name":"Dev Ops","slug":"Engineering/Dev-Ops","permalink":"http://fwz.github.io/categories/Engineering/Dev-Ops/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://fwz.github.io/tags/Linux/"}]},{"title":"2017 Resolution","slug":"2017-Resolution","date":"2017-02-01T09:37:27.000Z","updated":"2019-05-02T17:29:22.325Z","comments":true,"path":"2017/02/01/2017-Resolution/","link":"","permalink":"http://fwz.github.io/2017/02/01/2017-Resolution/","excerpt":"I drafted this new year resolution on the plane. Looking forwards to a fruitful year!","text":"I drafted this new year resolution on the plane. Looking forwards to a fruitful year! Professional Skills Write at least 5 blog posts on method and principle for real world technical problem solving. Finish a Machine Learning / Deep learning related course Investigate 5 &amp; Master 1 Open-Source Machine Learning Libraries Build a strong team – a team that is capable to deliver high quality product, that is self driven and motivated, that is with high reputation. Decision to work on AI was encouraged by performance of AlphaGo and Libratus in 2016. They draw an impressive picture for us and I am pretty sure that AI would be our next tipping point. I’m already late, but it’s still the best time to start now. MusicSince I moved out and have more time to practise instrument playing, I make very aggressive goals on music this year. Produce 2 Songs (not necessarily original) &amp; Design the cover for them Practice the first half of “66 drum set training” Practice the first half of “Berkley A Modern Method for Guitar : Volume 1” Get in touch with a new instrument and be able to play a simple melody (I guess violin or villa be a good choice, but I finally choose electric guitar and hopefully I am able to play some punk next year) FitnessPhysical happiness guarantee everything! Get Level 7 on Keep Generate 6 packs Swim 40000 m Practise butterfly and be able to swim 100m without exhaustion Capable to ski on Nanshan’s medium tracks. (Achieved) Financial Get 100K RMB non-salary income Health Sleep before 12:30 am on weekday (Failed) Travels Pay a visit to Iceland!","categories":[{"name":"Career","slug":"Career","permalink":"http://fwz.github.io/categories/Career/"},{"name":"Resolution","slug":"Career/Resolution","permalink":"http://fwz.github.io/categories/Career/Resolution/"}],"tags":[{"name":"2017","slug":"2017","permalink":"http://fwz.github.io/tags/2017/"},{"name":"Resolution","slug":"Resolution","permalink":"http://fwz.github.io/tags/Resolution/"}]},{"title":"The Data Migration Problem","slug":"The-Data-Migration-Problem","date":"2016-08-18T09:37:27.000Z","updated":"2019-05-02T17:28:39.603Z","comments":true,"path":"2016/08/18/The-Data-Migration-Problem/","link":"","permalink":"http://fwz.github.io/2016/08/18/The-Data-Migration-Problem/","excerpt":"Today I am trying to solve a very interesting problem, I would like to call it the “data migration problem”.","text":"Today I am trying to solve a very interesting problem, I would like to call it the “data migration problem”. BackgroundLet me illustrate to problem in short. We are working on data migration of a PMS system to a newer version. This system contains 2 major entities: USER &amp; HOTEL Each USER could operate 1 or more HOTELS Each HOTELS could be operated by 1 or more USERS Migrations are operated by batch of hotels. Each batch we could migrate 1 or more hotels. Requirement After each batch of migration, all hotels operated by the same user should at the same status (migrated / non-migrated). In the migration process, all hotels in the same batch could not be operate (downtime happens). So we are trying to minimize size of each batch with above requirement. ExamplesEach input line implies a relation of operation / management123456789input: user_1 hotel_1 user_2 hotel_2 user_3 hotel_3output: [hotel_1] [hotel_2] [hotel_3] Each user operate individual hotels, so each hotel could be migrated in separate batch. 123456789input: user_1 hotel_1 user_2 hotel_1 user_2 hotel_2 user_3 hotel_2 user_3 hotel_3output: [hotel_1, hotel_2, hotel_3] All hotels should be migrate in the same batch. Once hotel_1 is migrated, all other hotels user_2 operates (here, hotel_2) should be migrated as well because user_2 also operate hotel_1. Similarly, once hotel_2 migrated, hotel_3 should also be migrated (in the same batch) because user_3 operate these two hotels. SolutionsAt the first glance, The data structure are very similar to BIPARTITE GRAPH. The nodes consist of two part are the hotels and the users, edges are relations between hotels and users. There no edges between hotels, neither do users. However, we are not trying to apply capable users to manage hotel as much as possible. So it’s not a classical bipartite graph MATCH PROBLEM. Instead, we are trying to judge whether nodes are connected via edges, which could be convert to another problem – “Finding all CONNECTED COMPONENT“. Of course we have BFS/DFS to answer such questions. However, some information is not important to our current problem – the structure. We don’t need to know which manager make ‘Ritz Carlton’ and ‘Hilton’ connected, we just need to make sure both hotel are in the same batch. So we are able to make our algorithm more effective using DISJOINT SET. ExplanationsLet’s see the Elem class. Actually it’s an implementation of the Disjoint Set. When initialize, Each Elem could be seen as an single element set. parent is a pointer to its parent Elem. To judge whether two element is in the same set, check whether their root (utmost parent / ancestor) are the same element To union two set, we find the root of each set, set parent of one root to the other root 12345678910def root(self): if self.parent == self: return self else: # recursively find the ancestor return self.parent.root() def union(self, y): my_root = self.root() my_root.parent = y.root() And the iteration algorithm is: Each user / hotels are initialized as a individual set. Iterate all hotels, for each hotel: iterate each user it belongs to: If the user has not been union into any hotel, then union with current hotel Else, the user has been union into an existing hotel set. In this situation, current hotel should also be merged into the same hotel. 123456for h in hotels: for u in hotels[h]: if up[u].parent == up[u]: up[u].parent = hp[h] else: up[u].union(hp[h]) The Loop Invariant of this algorithm guarantee that: After each hotel looping, the root of any hotel Elem is always another hotel or itself. This means it’s correctly unitied with all those hotels known and supposed to be migrated together, if any, in the same batch. After each user looping for any hotel, the root of the user Elem is always a hotel. That’s it. CodeHere I feed this python script with a comma separated csv file, marking users and hotels. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import sysclass Elem(): parent = None value = None def __init__(self, value): self.value = value self.parent = self def root(self): if self.parent == self: return self else: return self.parent.root() def union(self, y): my_root = self.root() my_root.parent = y.root() def __str__(self): return \"[value: %s, parent: %s]\" % (self.value, self.root().value)if __name__ == \"__main__\": users = set() hotels = &#123;&#125; hp = &#123;&#125; up = &#123;&#125; res = &#123;&#125; for line in sys.stdin: (u, h) = line.strip().split(\",\") if u not in users: users.add(u) if h in hotels: hotels[h].add(u) else: hotels[h] = set([u]) # init hotel_sets for h in hotels: hp[h] = Elem(h) for u in users: up[u] = Elem(u) # start partitions for h in hotels: for u in hotels[h]: if up[u].parent == up[u]: up[u].parent = hp[h] else: up[u].union(hp[h]) for h in hp: root = hp[h].root().value if root in res: res[root].append(h) else: res[root] = [h] for r in res: print res[r]","categories":[{"name":"Engineering","slug":"Engineering","permalink":"http://fwz.github.io/categories/Engineering/"},{"name":"Algorithm","slug":"Engineering/Algorithm","permalink":"http://fwz.github.io/categories/Engineering/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://fwz.github.io/tags/Algorithm/"}]},{"title":"Soft Skills Programmers Need","slug":"Soft-Skills-Programmers-Need","date":"2016-07-21T02:00:26.000Z","updated":"2019-05-02T05:25:48.849Z","comments":true,"path":"2016/07/21/Soft-Skills-Programmers-Need/","link":"","permalink":"http://fwz.github.io/2016/07/21/Soft-Skills-Programmers-Need/","excerpt":"程序员都需要哪些软技能在某问答网站看到了一个非常很有意思的问题：程序员都需要哪些软技能？前段时间正好看完了一本神书，稍微归纳出一些不成熟的想法： 持续学习意愿 口头表达和写作能力 建立合理预期的能力 管理事务复杂度的能力 个人品牌建设的能力 财务管理能力 下面一一叙述。","text":"程序员都需要哪些软技能在某问答网站看到了一个非常很有意思的问题：程序员都需要哪些软技能？前段时间正好看完了一本神书，稍微归纳出一些不成熟的想法： 持续学习意愿 口头表达和写作能力 建立合理预期的能力 管理事务复杂度的能力 个人品牌建设的能力 财务管理能力 下面一一叙述。 持续学习意愿其实这都不能算软技能了，明明就是吃饭的家伙。持续学习的意愿低的话，自身的进步会非常有限。 举个最普遍的例子：小明在工作中遇到个新问题 假如小明自学能力和意愿较低，会更倾向用旧知识解决工作中遇到的新问题。特定的知识一般适合解决特定的问题，因此用已有的知识费力地完成，不一定是最优解。做完项目，没有什么成长，心里有气——怎么这些破事都忘我头上堆。 假如小明自学意愿强烈，可能就会考虑这个问题是否有别人遇到过，是否有成熟方法或者框架解决。有的话调研一下，看看能不能借鉴或者复用。做完项目，时间花得少，技能点还多点了一堆。 两种不同的习惯经过几年积攒下来，差别甚大。一个人是实打实的n年工作经验，一个人是1年工作经验 + n-1年重复劳动。 口头表达和写作能力简单来说，表达就是要把一件事情说明白。能力高低基本取决于两个因素：思维能力和表述思维结论的能力。 思维能力帮助我们把事情的关系在脑中理顺理清，让我们得到对事物的总体印象。思维能力强的人可以滔滔不绝，能力差的人可能就没什么能说的。所以我平时也会观察哪些人的语速快一些，这些人很可能是比较聪明的人。为什么？脑子得跟得上嘴啊。 表述思维的能力帮助我们把结论尽可能完整地传达给受众。除了要换位思考考虑对方的背景和接受能力以外，一个很重要的方法是少用代词，将代词都具体化，这样理解的成本会变低，受众可以更顺畅地接收信息并作出反馈。 任意一个能力有短板，都会在开口的时候暴露。例如跟产品同学讨(si)论(bi)出现你在github上提issue，别人压根听不懂，总结一个事情逻辑乱七八糟。最后落下一个「沟通能力需要提高的名声」。 想方设法去进行技术分享对提高表达和写作能力大有裨益。 插个题外话：关于「什么是最重要的语言」这个问题，一直有一个无可争议的解：英语。至少在程序员界。 建立合理预期的能力这里的预期并非特指别人的预期，还包含对自己的预期。 他人的预期是个非常神秘的意象，结合我们谜一般的自尊心，非常能激发程序员的斗志，但有极可能把自己拉进一个又一个大坑（逾期交付、深夜加班）里。这种坑除了能够影响自己的信誉，影响其他人的进度安排以外，并不会带来任何好处。因此虽然我们推崇「Fake it till you make it」，但并不是说任何乱七八糟的任务我们都应该笑着扛下来然后玩命做出来。 以上的预期都需要时间慢慢树立和校正。即使是资深的程序员，更多的时候都容易高估自己。因此一个推荐方法是建立监控，每天早上看看预期需要做哪些事情，然后看看每天做了多少事情，是不是按照你的预估来的，慢慢就会得到一个对自己能力的合理认知。 管理事务复杂度的能力程序员的一项核心工作就是降低复杂度，这样才能将更多的精力放在其他有意义的事情上。 有三个简单的技巧可以协助程序员管理身边事务的复杂度：自动化、一致性、优先级。 自动化：其实这个能力对于程序员来讲是最容易的。有任务需要做重复3次或以上，就应该把他自动化起来。举几个耳熟能详的例子：登录远程主机是不是都配好了SSH做免密码登录？线上系统的严重错误是自己通过监控推送报警还是每天检查一遍？每天老板要看的统计数据是每天手工查一遍拼一封邮件还是到点自动查询自动发送？这些事情每一件都只需要花很少的时间，但是假如每天都要花时间在这些琐碎的任务上，很难得到一个流畅的工作状态。 一致性：假如对于某项工作，我们的处理方式始终如一，我们就更容易享受这种一致性带来的红利，举几个例子： 以前写python都用vim手写，那个年代youcompleteme这种补全神器还没出来，而且函数名称写错也不会有提示，因此我经常会纠结这个函数名我之前是不是定义成这样的。还要翻来覆去的看好几遍。后来好好看了一下编程风格，所有函数的取名都根据编程风格来，这种情况下，我可以保证两次命名同一个函数会命名成同样的，调用的时候也是如此。 还是命令行，做git diff的时候，总是把基准放在左边，要提交的放在右边。有些时候其他同学在主持code review，两者的顺序时不时就颠倒一下，看起来就容易心烦意乱了。 优先级：一大堆事情过来了，除了老板吩咐的高优先级的事情，我该先做哪些事情？知乎上的采铜大大有过一个非常精辟的回答：做收益半衰期高的事情。简而言之就是做有长期正面影响的事情。例如花半个小时打上一局游戏，能在短期内满足我们娱乐的欲望，但难对我们有长远正面的影响。假如能够意识到这一点并切实执行，说不定假如花半小时去提高职业相关技能，可能就能受用很久。 个人品牌建设的能力个人品牌建设是一件不会在短期内能看到明显收益的事情，但从长远看来，比很多其他东西都重要（收益半衰期）。适当地营销自己，会默默地改变你的曝光率和知名度，更容易和他人取得合作的机会。 例如以下的事情可以抽空做： 持续地写博客文章 总结好自己的技术能力和经历 录制技术教程 公开演讲 不要太担心质量，文章会越写越好，前提是你一直在写，而且有经过自己思考的内容。 有了一点积累以后甚至可以更进一步，写书。现在也已经不是传统的年代了，只要内容有料，很容易就能够成书并散播出去。GitBook 就是一个非常好的平台。 财务知识和识别忽悠虽然程序员的收入也不少，但是人生阅历和社会经验，比起老板和CEO，都是鸡毛蒜皮。一不小心就会掉进哪些以为保护着自己的陷阱之中，所以学一点财务知识也非常重要。这里有两个可能很多人搞不清的概念：期权和VIE。 总结程序员需要的软技能比大部分职业都多太多了……","categories":[{"name":"Career","slug":"Career","permalink":"http://fwz.github.io/categories/Career/"},{"name":"Growth","slug":"Career/Growth","permalink":"http://fwz.github.io/categories/Career/Growth/"}],"tags":[{"name":"Thinking","slug":"Thinking","permalink":"http://fwz.github.io/tags/Thinking/"}]},{"title":"Getting Started With Grafana","slug":"Getting-Started-With-Grafana","date":"2016-07-20T12:49:51.000Z","updated":"2019-05-02T17:31:59.436Z","comments":true,"path":"2016/07/20/Getting-Started-With-Grafana/","link":"","permalink":"http://fwz.github.io/2016/07/20/Getting-Started-With-Grafana/","excerpt":"PrefaceWhen I first saw Grafana, I was astonished by its beauty immediately and I believe it should be the very tools for most dashboard / monitor use cases. This 10,000+ stared project provide a complete solution for metrics and analytics. Check this live demo and you should feel the same pleasure as I do. How does Grafana workIn short, Grafana is a metric solution which include UI Render / Query / Data Source features.UI Render is amazing; Different types of Queries are supported and it provide a wide range of Data source (mostly time series DB) support. So, to setup a Grafana for our own use, what should we do? First we need to setup the data source. Second, we setup Grafana server and connect it with the data source. Thrid, we feed data into the data source That’s it! Setting upThe following article will cover how to set all the stuff up under Mac OSX. Although we have homebrew and pip, but it take more effort than it seems to be. Graphite as Data SourceFirst of all, we are going to choose the data source first. Grafana provide a lot of DB support including: Graphite, Elasticsearch, CloudWatch, InfluxDB, OpenTSDB, KairosDB, Prometheus, I will take Graphite as our primary data source, with the following reasons: powerful data APIs friendly render APIs with image accessing using whisper file to store data, operation friendly overall the design of Graphite is very clean, every layer of the design is scalable.","text":"PrefaceWhen I first saw Grafana, I was astonished by its beauty immediately and I believe it should be the very tools for most dashboard / monitor use cases. This 10,000+ stared project provide a complete solution for metrics and analytics. Check this live demo and you should feel the same pleasure as I do. How does Grafana workIn short, Grafana is a metric solution which include UI Render / Query / Data Source features.UI Render is amazing; Different types of Queries are supported and it provide a wide range of Data source (mostly time series DB) support. So, to setup a Grafana for our own use, what should we do? First we need to setup the data source. Second, we setup Grafana server and connect it with the data source. Thrid, we feed data into the data source That’s it! Setting upThe following article will cover how to set all the stuff up under Mac OSX. Although we have homebrew and pip, but it take more effort than it seems to be. Graphite as Data SourceFirst of all, we are going to choose the data source first. Grafana provide a lot of DB support including: Graphite, Elasticsearch, CloudWatch, InfluxDB, OpenTSDB, KairosDB, Prometheus, I will take Graphite as our primary data source, with the following reasons: powerful data APIs friendly render APIs with image accessing using whisper file to store data, operation friendly overall the design of Graphite is very clean, every layer of the design is scalable. Setting up GraphiteTo setup Graphite, please refer to the installation doc but some of them are not quite straight forward, so here is the steps I used to install everything. Before we getting start, make sure you have Homebrew and pip well-setup. Install all dependenciesCairo and Django is needed to render on Graphites’ own. And we should use a specific version of Cairo since the latest 14.x version make fonts on graphite web very huge. cd /usr/local/Library/ git checkout 7073788 /usr/local/Library/Formula/cairo.rb brew install cairo brew install py2cairo sudo pip install cairocffi pip install Django==1.8 pip install django-tagging Install Graphite with pippip install https://github.com/graphite-project/ceres/tarball/master pip install whisper pip install carbon pip install graphite-web it’s also recommended change the owner of Graphite directory if graphite is installed by root viasudo chown -R &lt;your username&gt; /opt/graphite Configure Graphiteusing the following commands make all default setting works. cd /opt/graphite cp conf/carbon.conf{.example,} cp conf/storage-schemas.conf{.example,} cd webapp/graphite # Modify this file to change database backend (default is sqlite). cp local_settings.py{.example,} python manage.py syncdb Launch Carbon &amp; Graphitepython /opt/graphite/bin/carbon-cache.py start python /opt/graphite/bin/run-graphite-devel-server.py /opt/graphite (We could just ignore this error: WHISPER_FALLOCATE_CREATE is enabled but linking failed.) Running Graphite from /opt/graphite under django development server /usr/local/bin/django-admin runserver --pythonpath /opt/graphite/webapp --settings graphite.settings 0.0.0.0:8080 erforming system checks... System check identified no issues (0 silenced). July 20, 2016 - 10:05:58 Django version 1.8, using settings &apos;graphite.settings&apos; Starting development server at http://0.0.0.0:8080/ Quit the server with CONTROL-C. If everything goes well, we should see something like this and see Graphite runs well without broken image. Once you see the broken Image icon, go to http://0.0.0.0:8080/dashboard/ and you should see some python stack trace. My issue is that cairocffi is not correctly installed. Fix it till the web page doesn’t complain any more. Feeding data into data sourceNow you might be curious, what’s “carbon”? Carbon is a daemon listen for time-series data and can accept it over a common set of protocols. carbon-cache.py accepts metrics over various protocols and writes them to disk as efficiently as possible. This requires caching metric values in RAM as they are received, and flushing them to disk on an interval using the underlying whisper library. It also provides a query service for in-memory metric data points, used by the Graphite webapp to retrieve “hot data”. So following the documentation, we now use the “plaintext” protocal to send some data to the data sources every 60 seconds via the following commands 1watch -n 60 -d &apos;echo &quot;local.random.diceroll `jot -r 1 1 6` `date +%s`&quot; | nc -c 127.0.0.1 2003&apos; basically it generate a random integer between 1 and 6 for metric “local.random.diceroll” with a timestamp, send it to the data source which listen port 2003 on local host every 60 seconds. you could dig deeper by pasting the command to explainshell.com if you don’t quite understand how it works. Now if we check the log via1tailf /opt/graphite/storage/log/carbon-cache/&lt;instance name&gt;/console.log You should see something like:120/07/2016 21:58:36 :: Sorted 1 cache queues in 0.000091 seconds which mean carbon have successfully receive the data Then, go to the graphite webapp on localhost:8080, there should be another new node under “Tree” Tab, “Metrics” -&gt; “local” -&gt; “random” -&gt; “diceroll” Bingo! Now only one step to visualize it on Grafana Caution, There is one config file storage-schema you should pay attention to. With default settings, one could only feed data with a timestamp which is less than 24 hours because it will match the pattern here. [default_1min_for_1day] pattern = .* retentions = 60s:1d As the manual suggest, ‘The first pattern that matches the metric name is used’, so the new sections for your own data should be place on top of the ‘default_1min_for_1day’ section. For example, if all of my metrics are started with ‘koro’, then the config file will be like: 1234567[koro]pattern = ^koro.*retentions = 60s:1y[default_1min_for_1day]pattern = .*retentions = 60s:1d GrafanaNow instal Grafana via Homebrew and find where Grafana is installed, 123$ brew install grafana$ brew list grafana | head -1/usr/local/Cellar/grafana/3.0.1/bin/grafana-cli It’s suggested to check the Grafana Doc about configurations:http://docs.grafana.org/installation/configuration/https://www.linode.com/docs/uptime/monitoring/deploy-graphite-with-grafana-on-ubuntu-14-04 but it’s also ok to use the default settings. Starting Grafana ServerWell, it seems there is a bug on 3.0.1 so starting Grafana from command line is tricky. we should go to the ./share/grafana to run following command: 12/usr/local/Cellar/grafana/3.0.1/share/grafana (master)$ ../../bin/grafana-server Or else You could get error like: 1[log.go:84 Fatal()] [E] Failed to parse defaults.ini, open /usr/local/Cellar/grafana/3.0.1/conf/defaults.ini: no such file or directory See issue #4531 for more information. then goto localhost:3000 you should see the login UI, use the default admin user/password pair in the defaults.ini to login. Connect Grafana with Graphiteafter login, we could navigate from ‘top left icon’ -&gt; ‘data sources’ to configure which data source grafana could use (or we could use http://localhost:3000/datasources/edit/1 directly). And now we could setup a new dashboard!‘top left icon’ -&gt; ‘Dashboards’ -&gt; ‘New’ to add a new graphThen go to the ‘Graph’ section of current Dashboard, go to the metrics tab, add “local”|”random”|”diceroll” as metrics name, and you should see the data points pops into the panel immediately.","categories":[{"name":"Engineering","slug":"Engineering","permalink":"http://fwz.github.io/categories/Engineering/"},{"name":"Dashboard","slug":"Engineering/Dashboard","permalink":"http://fwz.github.io/categories/Engineering/Dashboard/"}],"tags":[{"name":"Grafana","slug":"Grafana","permalink":"http://fwz.github.io/tags/Grafana/"},{"name":"Graphite","slug":"Graphite","permalink":"http://fwz.github.io/tags/Graphite/"},{"name":"Carbon","slug":"Carbon","permalink":"http://fwz.github.io/tags/Carbon/"}]},{"title":"技术项目管理随想","slug":"Thoughts-of-Project-Management","date":"2015-11-21T09:08:35.000Z","updated":"2019-05-02T17:28:58.926Z","comments":true,"path":"2015/11/21/Thoughts-of-Project-Management/","link":"","permalink":"http://fwz.github.io/2015/11/21/Thoughts-of-Project-Management/","excerpt":"最近看了一些技术项目排雷书，分几个方面总结了一下雷区，以及可以做得更好的地方。 工程质量 &amp; 复杂度控制 预计变化的影响：需求会变化，系统扩展性需要加强，人员在流动。架构师的职责是管理变化，更确切的说，确保变化的影响是受控的。所以以下的技巧是必须的： 只进行小型、增量的改动 写可重复运行的测试用例，经常执行 让编写和运行测试用例变得容易 追踪依赖事项的变化 自动化重复事项 舍弃自作聪明的设计。这样的设计让系统变得脆弱和难以维护。越老实的系统越容易扩展。 关注性能。假如不从一开始就严格控制性能，「暂时不用关注性能」的想法就会在团队中蔓延。很快就变成「不得不马上解决线上的性能问题」。尽早在团队里面传授性能测试工具的用法是很有效的方法，真没那个时间的话，对依赖SQL的团队，普及一下explain的使用和结果解析都会有很大的提高。 要评估系统的扩展性，那就尝试向多个方向增长业务规模，看看哪个方面会先支撑不住，那个就是你下个要关注的方向。 重视界面设计。对于产品的用户来说，用户交互界面才是系统本身。系统的响应速度提高50%当然是好事，但废老大力气的优化很容易就被糟糕的交互带来的坏心情影响。","text":"最近看了一些技术项目排雷书，分几个方面总结了一下雷区，以及可以做得更好的地方。 工程质量 &amp; 复杂度控制 预计变化的影响：需求会变化，系统扩展性需要加强，人员在流动。架构师的职责是管理变化，更确切的说，确保变化的影响是受控的。所以以下的技巧是必须的： 只进行小型、增量的改动 写可重复运行的测试用例，经常执行 让编写和运行测试用例变得容易 追踪依赖事项的变化 自动化重复事项 舍弃自作聪明的设计。这样的设计让系统变得脆弱和难以维护。越老实的系统越容易扩展。 关注性能。假如不从一开始就严格控制性能，「暂时不用关注性能」的想法就会在团队中蔓延。很快就变成「不得不马上解决线上的性能问题」。尽早在团队里面传授性能测试工具的用法是很有效的方法，真没那个时间的话，对依赖SQL的团队，普及一下explain的使用和结果解析都会有很大的提高。 要评估系统的扩展性，那就尝试向多个方向增长业务规模，看看哪个方面会先支撑不住，那个就是你下个要关注的方向。 重视界面设计。对于产品的用户来说，用户交互界面才是系统本身。系统的响应速度提高50%当然是好事，但废老大力气的优化很容易就被糟糕的交互带来的坏心情影响。 不要迷信设计模式，不要为了使用设计模式而使用。 所谓复用，不仅仅与架构有关，还和人有关。如果没有人知道还有个框架，没有人知道怎么复用，再厉害的框架也是白扯。所以文档是很重要的。 构建系统并不是一场选美竞赛，不要为了追求完美而主动寻找错误。 追踪每周有多少时间用于解决线上问题。 一朵起错名的玫瑰会长成卷心菜（我真喜欢外国人的比喻啊）。做某个事情之前，假如连这个事情应该怎么称呼都没想好，那就别开始做了，想清楚再做。 如果解决的问题稳定，那么很容易能得到一个可靠的解决方案。但无论开工之前想得多完善，最后的产出总不会是一模一样的。 开发过程中的“新想法”要要慎重考虑。因为一个技术很酷而使用，因为框架升级而要系统跟着升级，因为做A而要重构B，因为有一个关于架构的想法而进行讨论，都隐藏着危险的信号。这些事情会带来系统的变化，使项目变得逐渐不可控。 选择当下的最佳解决方案去解决问题。假如想太多关于未来的问题，可能既无法解决未来的问题，而且连当下的问题都不能好好解决。 量化你的数据。不断加入统计或埋点数据。 不走正路的Bugfix就像借贷，是要还利息的。利息主要表现为：系统变得更难以扩展。为了降低影响，合理的做法是，先上bugfix，并投入时间在下次发布之前做一个合理的bugfix。 团队建设 提高团队战斗力（确保他们已有对应的工具，CI，code formatter，性能测试工具；确保有对应的技能，每周五对需要用到的技能进行培训，在书本和讨论上投入；引入流程的时候，确保是用来解决问题，而非引入新问题） 寻找并留住有热情的工程师 自己写JD。 技术人员的顾客并不是真正的顾客（是产品人员），我们顾客的顾客才是真正的顾客。因此，需要考虑真正顾客的需求。例如产品为了赶工期要求暂不对数据进行加密，技术人员应该指出风险而不是默默接受，站在真正顾客的角度考虑问题，而不是提出任务的人。 工程师和机器打交道，架构师和人打交道，因此要学会Sell自己的观点。如何推销自己的观点： 建立起Value proposition 用数据说话，尽早建立起监控进行反馈 找一个合适的时间提出方案（例如前一个框架被验证是失败的时候） 沟通 在讨价还价的过程中先索取得比需要的更多（以在协商中留有退路） 挑战并验证假设。是否用户真的不能忍？是否这个库就是比那个库糟糕？ 做一个好合作、会说话、但并非好说话的工程师。 许下承诺和践行承诺才能得到尊重。这就要求我们考虑预算和时间的限制，尽我们所能让系统变得高效。 不断地问需求的提出方，需求能不能加上「在任何场合下，总会有……」的概括。由于需求方会比较谨慎地回答这种问题，因此他会不断地修饰和思考需求。反复地问，这样我们就会得到一个非常简洁、核心的「系统本质」。这个本质才是真正的我们需要关注的。 技术选型 影响技术选型的决策因素中，需求比经历重要。不要为了简历好看而用新技术。找那些容易和其他模块配合的框架。 决定技术选型之前，不妨真正去试一试。找两个工程师，花时间调研一下优点和缺点，总结对比，最后选哪个在很多情况下都是很显然的。这可能是在浪费时间吗？有可能。但总比匆匆找到一个非最优解马上开工最后发现不得不绕回来要好。 记录进行设计的依据。记录做了哪些决策，为什么做了这个决策，以及为什么不用其他选择。 为任何的技术决策负起责任。很多正确的设计最后却以失败告终。要避免失败，至少做到以下几点：技术决策应该传达给所有相关的人士。 选好趁手的工具，不要轻易切换。 工程师的自身成长 理解硬件。起码知道系统的状态和什么相关，各种硬件的性能级别，有线上报警的时候能根据报警项定位问题。 软件行业的一个大问题是工程师需要解决远超自己当前理解的问题。因此学习与沟通是关键的能力 架构师首先是个工程师。假如我做了设计，那起码我应该有能力亲自实现。","categories":[],"tags":[{"name":"Book Review, Thoughts","slug":"Book-Review-Thoughts","permalink":"http://fwz.github.io/tags/Book-Review-Thoughts/"}]},{"title":"Unit Test 101","slug":"Unit-Test-101","date":"2015-05-09T08:08:20.000Z","updated":"2019-05-02T17:20:41.788Z","comments":true,"path":"2015/05/09/Unit-Test-101/","link":"","permalink":"http://fwz.github.io/2015/05/09/Unit-Test-101/","excerpt":"The importance of writing unit tests have been heavily discussed. However, reality is cruel. Many developers do not write UT, and even larger amount of developers write funny tests just to pass the code coverage bar, and some developers with ambition are not quite sure about how to write good unit tests. Most good UT are written by top developers. I thought I am that kind of ambitious developer, so I spend two weekends to start to learn it. Most cases listed below are originated from or revised from the book “Practical Unit Testing with Testng and Mockito” by Tomek Kaczanowski. A really good book worth reading. ConceptsBefore we start, let’s visit the concepts of SUT and DOC.","text":"The importance of writing unit tests have been heavily discussed. However, reality is cruel. Many developers do not write UT, and even larger amount of developers write funny tests just to pass the code coverage bar, and some developers with ambition are not quite sure about how to write good unit tests. Most good UT are written by top developers. I thought I am that kind of ambitious developer, so I spend two weekends to start to learn it. Most cases listed below are originated from or revised from the book “Practical Unit Testing with Testng and Mockito” by Tomek Kaczanowski. A really good book worth reading. ConceptsBefore we start, let’s visit the concepts of SUT and DOC. SUT &amp; DOCSUT, or System Under Test, are understood as the part of the system being tested. Depending on the type of test, SUT may be of very different granularity – from a single class to a whole application. DOC, or Depended On Component, is any entity that is required by an SUT to fulfill its duties. For example, recently I am working with Spring, and I am implementing a Filter class, which take in a ServletRequest object, a ServletResponse object and a FilterChain object. Then when writing unit test, SUT is the Filter class I am working on. DOC are consist of ServletRequest and a ServletResponse, and a FilterChain. Test TypeThere’s many types of tests(and names) in software development, but the most important tests are: unit test which help to ensure high-quality code integration test which verify that different modules are cooperating effectively end to end test which put the system through its paces in ways that reflect the standpoint of users Simple. Let’s start to discuss UT (Unit Test). Unit TestStructureThe structure of writing a UT is: Arrange (Setup environment / foundation of the test) Act (run the test) Assert (compare expect result with actual result) Frequency of running UTUT are supposed to be run repeatedly and frequently. Even someone is not using TDD, after we have implemented any feature, all the UT under this project should be run. Bad smellsA test is not a unit test if: It talks to the database It communicates across the network It touches the file system It can’t run at the same time as any of your other unit tests It might fail even the code is correct, or it might pass even the code is not correct. The above points make writing UT not quite straight forward. To make our UT clean, some knowledge such as Mocking is needed. Also we need to master are some skills to utilize the power of our test framework. Framework and toolsBase on some investigation, I choose TestNG and Mockito and finally spot this book. This is just personal preferences. JUnit provide mostly the similar function with TestNG. Something we desperately need should be available for a mature framework. Let’s start coding to demonstrate what we have listed above. Here is a Money class, with two properties, amount and currency. CompatabilityMoney.java123456789101112131415161718192021222324252627282930313233public class Money &#123; private final int amount; private final String currency; public Money(int amount, String currency) &#123; if (amount &lt; 0) &#123; throw new IllegalArgumentException(\"illegal negative amount: [\" + amount + \"]\"); &#125; if (currency == null || currency.isEmpty()) &#123; throw new IllegalArgumentException(\"illegal currency: [\" + currency + \"], it can not be null or empty\"); &#125; this.amount = amount; this.currency = currency; &#125; public int getAmount() &#123; return amount; &#125; public String getCurrency() &#123; return currency; &#125; public boolean equals(Object o) &#123; if (o instanceof Money) &#123; Money money = (Money) o; return money.getCurrency().equals(getCurrency()) &amp;&amp; getAmount() == money.getAmount(); &#125; return false; &#125;&#125; Now let’s write a very simple test to test whether the constructor work as expect. Here our test are with TestNG and JUnit style (with support in TestNG). MoneyTest.java123456789101112131415161718import org.testng.annotations.Test;import static org.testng.Assert.assertEquals;import org.testng.AssertJUnit;@Testpublic class MoneyTest &#123; public void constructorShouldSetAmountAndCurrency() &#123; Money money = new Money(10, \"USD\"); // arrange // a TestNG Way assertEquals(money.getAmount(), 10); // act and assert assertEquals(money.getCurrency(), \"USD\"); // a JUnit Way AssertJUnit.assertEquals(\"USD\", money.getCurrency()); AssertJUnit.assertEquals(10, money.getAmount()); &#125;&#125; Parameter TestsNow the first important trick. For many cases, one set of input/output are not sufficient when writing UT.NotSoGoodTest.java1234567891011121314151617public void Test&#123; TestObject to1 = new TestObject(); InputParam input1 = new InputParam(); OutputParam output1 = new Output(); assertEquals(to1.calc(input1), output1); TestObject to2 = new TestObject(); InputParam input2 = new InputParam(); OutputParam output2 = new Output(); assertEquals(to2.calc(input2), output2);&#125;If we don’t want to write code like this, then Parameter Test is our friend. Use a DataProvider annotation to mark a method as supplying data for a test method. It return a 2D array, contains a list of Object[], each Object will be passed as input to the test function. MoneyTest.java12345678910111213141516171819202122232425import org.testng.annotations.DataProvider;import org.testng.annotations.Test;import static org.testng.Assert.assertEquals;@Testpublic class MoneyTest &#123; // Parameter tests @DataProvider private static final Object[][] getMoney()&#123; return new Object[][] &#123; &#123;10, \"USD\"&#125;, &#123;20, \"EUR\"&#125;, &#123;30, \"CNY\"&#125; &#125;; &#125; @Test(dataProvider = \"getMoney\") public void constructorShouldSetAmountAndCurrency( int amount, String currency) &#123; Money money = new Money(amount, currency); assertEquals(money.getAmount(), amount); assertEquals(money.getCurrency(), currency); &#125;&#125; See? A DataProvider notation help you define a list of objects. When linked with the dataprovider function, it will automatically pass the object to the test function as parameters. Previous code generate 3 tests. There are some advance usage of Parameter Tests: DataProvider could live in another Class so they could be reused (But nee modification to test function, use @Test(dataProvider = &quot;getMoney&quot;, dataProviderClass = DataProviders.class) annotation). DataProvider could do lazy initialization, so when you are going to generate many test cases (say 10000+), they don’t need to be initialized before the test, incase any cases fail in the middle and it waste a lot of resources. (need further implement an iterator based on DataProvider class) dataProviderIterator.java12345678@DataProvider(name=&quot;colors&quot;) public Iterator&lt;Object[]&gt; getColors()&#123; Set&lt;Object[]&gt; result=new HashSet&lt;Object[]&gt;(); result.add(new Object[]&#123;&quot;black&quot;&#125;); result.add(new Object[]&#123;&quot;silver&quot;&#125;); result.add(new Object[]&#123;&quot;gray&quot;&#125;); return result.iterator();&#125; Testing ExceptionsHere is a simple case about how to write UT to test the exception. Some developer might actually include 4 stuffs in a single test (just to improve coverage): start the SUT pass an invalid value catch the exceptions handle it However, TestNG provide an much cleaner way to test it. expectedExceptions notation are to help. (Why bother to handle it? Because if we don’t handle the exception, then this test will fail…) 1234567891011121314151617181920212223242526public class MoneyIAETest &#123; private final static int VALID_AMOUNT = 5; private final static String VALID_CURRENCY = &quot;USD&quot;; @DataProvider private static final Object[][] getInvalidAmount()&#123; return new Integer[][] &#123; &#123;-12387&#125;, &#123;-5&#125;, &#123;-1&#125; &#125;; &#125; @Test(dataProvider = &quot;getInvalidAmount&quot;, expectedExceptions = IllegalArgumentException.class) public void shouldThrowIAEForInvalidAmount(int invalidAmount) &#123; Money money = new Money(invalidAmount, VALID_CURRENCY); &#125; @DataProvider private static final Object[][] getInvalidCurrency()&#123; return new String[][] &#123; &#123;null&#125;, &#123;&quot;&quot;&#125; &#125;; &#125; @Test(dataProvider = &quot;getInvalidCurrency&quot;, expectedExceptions = IllegalArgumentException.class) public void shouldThrowIAEForInvalidCurrency(String invalidCurrency) &#123; Money money = new Money(VALID_AMOUNT, invalidCurrency);&#125; &#125; Testing Concurrent CodesTesting concurrency, to some extent is nightmare. If not correctly implemented, the quality of test might become a problem of a UT. Let’s introduce two more attributes which is helpful to test concurrent codes. threadPoolSize, which sets the number of threads that are to execute a test method invocationCount, which sets the total number of test method executions. There is no need for us to implement threads ourselves. 123456789public class SystemIdGenerator implements IdGenerator &#123; private static Long nextId = System.currentTimeMillis(); // is it thread safe? public Long nextId() &#123; return nextId++; &#125;&#125; SystemIdGeneratorTest.java123456789101112131415161718192021public class SystemIdGeneratorTest &#123; private IdGenerator idGen = new SystemIdGenerator(); @Test public void idsShouldBeUnique() &#123; Long idA = idGen.nextId(); Long idB = idGen.nextId(); assertNotEquals(idA, idB, \"idA \" + idA + \" idB \" + idB); &#125; @Test public class JVMUniqueIdGeneratorParallelTest &#123; private IdGenerator idGen = new SystemIdGenerator(); private Set&lt;Long&gt; ids = new HashSet&lt;Long&gt;(10000); @Test(threadPoolSize = 997, invocationCount = 10000) public void idsShouldBeUnique() &#123; assertTrue(ids.add(idGen.nextId())); &#125; &#125;&#125; I get 307 fail cases in my laptop for the above code. It failed because the unary operator is not atomic and there might be two thread reading the same value of id. In addition to what we have done, you can also use the timeOut and invocationTimeOut attributes of @Test annotation. Their role is to break the test execution and fail the test if it takes too long (e.g. if your code has caused a deadlock or entered some infinite loop). Collection Testing Unitils Hamcrest FEST Fluent Assertion UnitilsUnitils could help customize the equal definition SetEqualityTest.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import org.testng.annotations.BeforeMethod;import org.testng.annotations.Test;import org.unitils.reflectionassert.ReflectionComparatorMode;import java.util.LinkedHashSet;import java.util.Set;import static org.unitils.reflectionassert.ReflectionAssert.assertReflectionEquals;public class SetEqualityTest &#123; // same setA and setB created as in the previous TestNG example Set&lt;Integer&gt; setA; Set&lt;Integer&gt; setB; @BeforeMethod public void setUp() &#123; setA = new LinkedHashSet&lt;Integer&gt;(); setB = new LinkedHashSet&lt;Integer&gt;(); setA.add(1); setA.add(2); setB.add(2); setB.add(1); &#125; @Test public void twoSetsAreEqualsIfTheyHaveSameContentAndSameOrder() &#123; // assertReflectionEquals(setA, setB); // This will failed with following error // --- Found following differences --- /* [0]: expected: 1, actual: 2 [1]: expected: 2, actual: 1 --- Difference detail tree --- expected: [1, 2] actual: [2, 1] */ &#125; @Test public void twoSetsAreEqualsIfTheyHaveSameContentAndAnyOrder() &#123; assertReflectionEquals(setA, setB, ReflectionComparatorMode.LENIENT_ORDER); &#125;&#125; Fest Fluent AssertionFEST Fluent Assertions, which is a part of FEST library, offers many assertions which can simplify collections testing. It also provides a fluent interface, which allows for the chaining together of assertions. FestCollectionTest.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import org.fest.assertions.api.MapAssert;import org.testng.annotations.BeforeMethod;import org.testng.annotations.Test;import java.util.HashMap;import java.util.LinkedHashMap;import java.util.LinkedHashSet;import java.util.Set;import static org.fest.assertions.api.Assertions.assertThat;import static org.fest.assertions.api.Assertions.entry;/** * Created by wenzhong on 5/9/15. */public class FestCollectionTest &#123; // same setA and setB created as in the previous TestNG example Set&lt;Integer&gt; setA; Set&lt;Integer&gt; setB; @BeforeMethod public void setUp() &#123; setA = new LinkedHashSet&lt;Integer&gt;(); setB = new LinkedHashSet&lt;Integer&gt;(); setA.add(1); setA.add(2); setB.add(2); setB.add(1); &#125; @Test public void collectionsUtilityMethods() &#123; assertThat(setA) .isNotEmpty() .hasSize(2) .doesNotHaveDuplicates(); assertThat(setA).containsOnly(1, 2); &#125; @Test public void mapUtilityMethods() &#123; HashMap&lt;String, Integer&gt; map = new LinkedHashMap&lt;String, Integer&gt;(); map.put(\"a\", 2); map.put(\"b\", 3); assertThat(map) .isNotNull() .isNotEmpty() .contains(entry(\"a\", 2), entry(\"b\", 3)) .doesNotContainKey(\"c\"); &#125;&#125; FEST provides a fluent interface, which allows for the chaining together of assertions. In this case verification of emptiness, size and the absence of duplicates all gets fitted into a single line of code, but the code is still readable. Dependency between testsNow think about a test which want to make sure that user account management is correct. It have 2 functions: adding and deleting user accounts. In traditional UT scope, test are independent so the test code might looks like testAddition and testDeletion. Note that the addition actually run for 2 times, which actually break the “DRY” rules. However, testNG use another attribute in @Test notation to solve this problem. This time, an explicit dependency is established between the tests. TestWithDependencyTest.java12345678910111213141516171819202122232425262728import org.testng.annotations.Test;/** * Created by wenzhong on 5/9/15. */public class TestWithDependencyTest &#123; @Test public void testAddition() &#123; // adds user X to the system // verifies it exists, by issuing SQL query against the database &#125; @Test public void testDeletion() &#123; // adds user Y to the system // verify that it exists (so later we know that it was actually removed) // removes user Y // makes sure it does not exist, by issuing SQL query against the database &#125; @Test(dependsOnMethods = \"testAddition\") public void testDeletion2() &#123; // removes user X // makes sure it does not exist, by issuing SQL query against the database &#125;&#125; However, it’s not always a good thing to have dependency. Think of a test suite with dozens of test, adding a new test will be a nightmare because you might need to find a correct spot to place it into the dependency tree. An alternative is try to initialize the state before each test. And use the dependency in integration test and E2E test. Private Method TestingShould we test private method? Of course. But from the best practice we should test it via public method test. However, when facing legacy code, we might face a dilemma: we need test to make sure it work as expect, but writing test need refactor on code, without test we don’t know whether our refactor is correct. So when facing legacy code, we compromise on private method testing. Built-in support in JavaLet’s examine some native support from Java. 123456789101112131415161718192021222324252627282930313233343536373839404142public class ClassWithPrivateMethod &#123; private boolean privateMethod(Long param) &#123; return true; &#125;&#125;import org.testng.annotations.Test;import java.lang.reflect.InvocationTargetException;import java.lang.reflect.Method;import static org.testng.Assert.*;public class ClassWithPrivateMethodTest &#123; @Test public void testingPrivateMethodWithReflection() throws NoSuchMethodException, InvocationTargetException, IllegalAccessException &#123; // Note: this is an ugly implementation ClassWithPrivateMethod sut = new ClassWithPrivateMethod(); Class[] parameterTypes = new Class[1]; parameterTypes[0] = java.lang.Long.class; Method m = sut.getClass() .getDeclaredMethod(&quot;privateMethod&quot;, parameterTypes); // make it accesible outside of class m.setAccessible(true); Object[] parameters = new Object[1]; parameters[0] = 5569L; // actually invoke Boolean result = (Boolean) m.invoke(sut, parameters); assertTrue(result); &#125;&#125; Using PowerMockPowerMock give us a cleaner approach to work on testing private methods. The WhiteBox class provide various utilities for accessing internals of a class. Basically it’s a simplified reflection utility intended for tests.12345678910111213141516import org.powermock.reflect.Whitebox;public class ClassWithPrivateMethodTest &#123; @Test public void testingPrivateMethodWithReflectionByPowerMock() throws Exception, IllegalAccessException &#123; ClassWithPrivateMethod sut = new ClassWithPrivateMethod(); Boolean result = Whitebox .invokeMethod(sut, &quot;privateMethod&quot;, 302483L); assertTrue(result); &#125;&#125; Testing non dependency injection codeThink about the following production code (which is less test-able), how can we mock the MyCollaborator class? 123456public class MySut &#123; public void myMethod() &#123; MyCollaborator collaborator = new MyCollaborator(); // some behaviour worth testing here which uses collaborator &#125;&#125; PS. the correct way with dependency injection should be123456public class MySut &#123; public void myMethod(MyCollaborator collaborator) &#123; // some behaviour worth testing here which uses collaborator &#125;&#125; PowerMock to Rescue1234567891011121314151617181920212223242526272829import org.powermock.api.mockito.PowerMockito;import org.powermock.core.classloader.annotations.PrepareForTest;import org.testng.IObjectFactory;import org.testng.annotations.ObjectFactory;import org.testng.annotations.Test;import static org.powermock.api.mockito.PowerMockito.mock;@PrepareForTest(NotDOCInjectedSUT.class)public class NonInjectedDOCTest&#123; @ObjectFactory public IObjectFactory getObjectFactory() &#123; return new org.powermock.modules.testng.PowerMockObjectFactory(); &#125; @Test public void testMyMethod() throws Exception &#123; NotDOCInjectedSUT sut = new NotDOCInjectedSUT(); MyCollaborator collaborator = mock(MyCollaborator.class); // the whenNew function applies to // normal test using Mockito&apos;s syntax // e.g. Mockito.when(collaborator.someMethod()).thenReturn(...) PowerMockito.whenNew(MyCollaborator.class) .withNoArguments().thenReturn(collaborator); &#125;&#125; Here some new attribute and method are introduced: the @PrepareForTest annotation informs PowerMock that the NotDOCInjectedSUT class will create a new instance of some other class. In general, this is how PowerMock learns, about which classes it should perform some bytecode manipulation. In order to use PowerMock with TestNG, we need to make PowerMock responsible for the creation of all of the test instances. So we use the @ObjectFactory notation to assign PowerMockObjectFactory as the Factory class to generate test instances. The test double is created as usual - with the static mock() method of Mockito. whenNew() is the place magic happens: whenever a new object of the MyCollaborator class gets created, our test double object (collaborator) will be used instead. Two of PowerMock’s methods - whenNew() and withNoArguments() - are used to control the execution of a no-arguments constructor of the MyCollaborator class. Note that static methods could also be mocked as the new operator. ArgumentCaptorBut if the collaborator class have some input parameters for its constructor? Here we have a not quite good class – PIM (Personal Information Manager?), and related class Calendar and Meeting. 123456789101112131415161718192021222324252627282930313233public interface Calendar &#123; public void addEvent(Event event);&#125;public class Meeting implements Event &#123; private final Date startDate; private final Date endDate; public Meeting(Date startDate, Date endDate) &#123; this.startDate = new Date(startDate.getTime()); this.endDate = new Date(endDate.getTime()); &#125; public Date getStartDate() &#123; return startDate; &#125; public Date getEndDate() &#123; return endDate; &#125;&#125;public class PIM &#123; private final static int MILLIS_IN_MINUTE = 60 * 1000; private Calendar calendar; public PIM(Calendar calendar) &#123; this.calendar = calendar; &#125; public void addMeeting(Date startDate, int durationInMinutes) &#123; Date endDate = new Date(startDate.getTime() + MILLIS_IN_MINUTE * durationInMinutes); Meeting meeting = new Meeting(startDate, endDate); calendar.addEvent(meeting); &#125;&#125; As we could see that the meeting inside the addMeeting method is hard to mock. However, Mockito provide a ArgumentCaptor function, which we could use to get information from the type Meeting. 1234567891011121314151617181920212223242526272829303132333435363738394041424344import org.mockito.ArgumentCaptor;import org.testng.annotations.Test;import java.util.Date;import static org.mockito.Mockito.mock;import static org.mockito.Mockito.verify;import static org.testng.Assert.*;/** * Created by wenzhong on 5/10/15. */public class PIMTest &#123; private static final int ONE_HOUR = 60; private static final Date START_DATE = new Date(); private static final int MILLIS_IN_MINUTE = 1000 * 60; private static final Date END_DATE = new Date(START_DATE.getTime() + ONE_HOUR * MILLIS_IN_MINUTE); @Test public void shouldAddNewEventToCalendar() &#123; Calendar calendar = mock(Calendar.class); PIM pim = new PIM(calendar); // An object of the ArgumentCaptor class is created, // which will gather information on arguments of the type Meeting. ArgumentCaptor&lt;Meeting&gt; argument = ArgumentCaptor.forClass(Meeting.class); pim.addMeeting(START_DATE, ONE_HOUR); // The addEvent() method’s having been called is verified, // and Mockito is instructed to capture arguments of this method call. verify(calendar).addEvent(argument.capture()); Meeting meeting = argument.getValue(); assertEquals(meeting.getStartDate(), START_DATE); assertEquals(meeting.getEndDate(), END_DATE); &#125;&#125;In the above code, the actual argument to the `addEvent()` method is extracted from the `ArgumentCaptor` object. Writing Testable CodeCode that wasn’t designed to be testable is not testable. Rely on Dependency InjectionSo every dependency could be mocked. Never hide a TUF within TUCTUF, or Test UnFriendly Feature, include following examples: Database access FileSystem access Network access Affect of side effect access lengthy / inscrutable computations static variable usage TUC, or Test Unfriendly Constructor, include following examples: Final methods Final classes Static methods Private methods Static InitializationExpressions Constructors ObjectInitialization Blocks New-Expressions Handling existing issues subclass and override 1234567891011public class EventResponder&#123; void showNotification(String notificationMessage) &#123; JOptionPane.showMessageDialog(null, notificationMessage); &#125; public void respond() &#123; ... showNotification(message); &#125;&#125; testRespond12345public class TestingEventResponder extends EventResponder&#123; void showNotification(String notificationMessage) &#123; &#125;&#125; Let’s think about when we could not use this “subclass and override” way to test? Final method (we could not override) Final class (so we could not inherit) Static method (we could not override) Private method (we could not override) Constructor (they have to run and we have no way to override it) Strictly speaking, final classes and final methods are only a problem in testing when they hide a TUF, but it’s nice to have a carefully considered reason for using them rather than just using them by default. References &amp; Further Reading http://martinfowler.com/bliki/FluentInterface.html Testable Java by Michael Feathers Next Generation Java Testing: TestNG and Advanced Concepts By Cédric Beust, Hani Suleiman Working Effectively With Legacy Code by Michael Feathers Refactoring: Improving the Design of Existing Code by Martin Fowler, Kent Beck, John Brant, William Opdyke, Don Roberts Growing Object-Oriented Software, Guided by Tests by Steve Freeman , Nat Pryce Java Concurrency in Practice Clean Code: A Handbook of Agile Software Craftsmanship By Robert C. Martin","categories":[{"name":"Engineering","slug":"Engineering","permalink":"http://fwz.github.io/categories/Engineering/"},{"name":"Engineering Excellence","slug":"Engineering/Engineering-Excellence","permalink":"http://fwz.github.io/categories/Engineering/Engineering-Excellence/"}],"tags":[{"name":"Unit Test","slug":"Unit-Test","permalink":"http://fwz.github.io/tags/Unit-Test/"}]},{"title":"Linear Algebra in Room Escape","slug":"Linear-Algebra-in-Room-Escape","date":"2015-04-12T04:26:54.000Z","updated":"2019-05-02T17:20:41.766Z","comments":true,"path":"2015/04/12/Linear-Algebra-in-Room-Escape/","link":"","permalink":"http://fwz.github.io/2015/04/12/Linear-Algebra-in-Room-Escape/","excerpt":"Last weekend I went to play a reality room escape game with some friends. It’s a lot of fun and we finally escape on time! The only thing make it less perfect is that we skip a “very hard” puzzle according to the staff in the room. We spend 1O minutes on it and we could not found an effective way to solve it. The game is consisted of a board with 5 rows * 5 columns = 25 lights. Each light is either on or off. Player could switch any light on/off, but switching any light will also switch it’s neighbour on up/down/left/right position at the same time. The goal of this game is for a given status, try to switch some of the lights to make all the lights on. You could also refer to this graph for the “switch logic”. To win the game, For example, if the initial status looks like the following board (O mean an enlighted light and X mean an off light), then we could switch 2 lights on (2,1) and (4,3) to make all the light on. But is there an systematic way to get a solution?","text":"Last weekend I went to play a reality room escape game with some friends. It’s a lot of fun and we finally escape on time! The only thing make it less perfect is that we skip a “very hard” puzzle according to the staff in the room. We spend 1O minutes on it and we could not found an effective way to solve it. The game is consisted of a board with 5 rows * 5 columns = 25 lights. Each light is either on or off. Player could switch any light on/off, but switching any light will also switch it’s neighbour on up/down/left/right position at the same time. The goal of this game is for a given status, try to switch some of the lights to make all the lights on. You could also refer to this graph for the “switch logic”. To win the game, For example, if the initial status looks like the following board (O mean an enlighted light and X mean an off light), then we could switch 2 lights on (2,1) and (4,3) to make all the light on. But is there an systematic way to get a solution? x O O O O O O O O O O O O O O x x O O O O O O O O O O O O O x O x O O switch (2,1) =&gt; O O x O O switch (4,3) =&gt; O O O O O O x x x O O x x x O O O O O O O O x O O O O x O O O O O O O But not all cases are so straight forward. For example: x O O O O O O O O O O O O O O x O O O O O O O O O O O O O O x O O O O OR O O O O O OR O O O O O O O O O O O O x O O O O O O O O O O O O O O x O O X O O O X Some Helpful DeductionBefore we further illustrate, there are 2 useful tips deduced from the game rule: we don’t need to switch a light for more than 1 time. the sequence of switching does not matter. Solution 1: Brutal EnumerationWe enumerate all possible combination of switches based on the current status. And see whether there is possible solution. Base on the above deduction, this solution have a time complexity of $O(2^{mn})$, where m * n is the total number of lights.Calculation will be effective if m,n &lt; 5. But for a square grid with m = n = 6, it’s already quite a long time for a laptop. Solution 2: Linear Algebra WayThe following linear algebra approach is a more systematic way to solve it. In the following illustration, I will use a 3*3 grid for demonstration, after we have understand it, we could extend it to size with any size. First of all, the Lights status are represented with matrix L. Here 1 means the lights are on, 0 for off.\\[ L = \\begin{vmatrix}0 &amp; 1 &amp; 0 \\\\1 &amp; 1 &amp; 0 \\\\0 &amp; 1 &amp; 1\\end{vmatrix} \\] To make all the lights on, we should toggle some lights to generate effects of\\[ \\overline{L} = \\begin{vmatrix}1 &amp; 0 &amp; 1 \\\\0 &amp; 0 &amp; 1 \\\\1 &amp; 0 &amp; 0\\end{vmatrix} \\] start from 0 matrix. The action of the switch placed at (i,j) can be interpreted as the matrix ${A}_{ij}$ , where $A_{ij}$ is the matrix in which the only entries equal to 1 are those placed at (i,j) and in the adjacent positions; there are essentially three types of matrices ${A}_{ij}$, for different types of position (corner, edge, internal): $${A}_{ij}= \\begin{cases} \\begin{vmatrix} 1 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 0 \\end{vmatrix}& \\text{if i,j refer to top-left corner light}\\\\ \\begin{vmatrix} 1 & 1 & 1 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{vmatrix}& \\text{if i,j refer to top-middle light}\\\\ \\begin{vmatrix} 0 & 1 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 1 & 0 \\end{vmatrix}& \\text{if i,j refer to internal light}\\\\ \\text{...}\\\\ \\end{cases}$$ Every winning combination of moves can be expressed mathematically in the form: $$\\sum_{i,j} {x_{ij} } { {A}_{ij} } = \\overline{L}$$ each coefficient $x_{ij}$ represents the number of times that switch (i,j) has to be pressed. According to our previous deduction, it could be only 1 or 0. And if we flatten the ${A}_{ij}$ into a vector, then we get the following equation: $$\\begin{vmatrix} 1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\ 1 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 1 & 0 & 0 & 1 & 0 & 0 & 0 \\\\ 1 & 0 & 0 & 1 & 1 & 0 & 1 & 0 & 0 \\\\ 0 & 1 & 0 & 1 & 1 & 1 & 0 & 1 & 0 \\\\ 0 & 0 & 1 & 0 & 1 & 1 & 0 & 0 & 1 \\\\ 0 & 0 & 0 & 1 & 0 & 0 & 1 & 1 & 0 \\\\ 0 & 0 & 0 & 0 & 1 & 0 & 1 & 1 & 1 \\\\ 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 1 \\end{vmatrix} \\begin{vmatrix} {x_{11}} \\\\ {x_{12}} \\\\ {x_{13}} \\\\ {x_{21}} \\\\ {x_{22}} \\\\ {x_{23}} \\\\ {x_{31}} \\\\ {x_{32}} \\\\ {x_{33}} \\\\ \\end{vmatrix} = \\begin{vmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{vmatrix}$$ Pretty good, But how could we solve this equation? It’s not a traditional linear equation, it’s based on an (mod 2) operation or what we call “XOR”. But the basic idea is the same, we just redefine the operation like “add”, “multiply” and then compute the equation. There is a lot of solution given in different languages, let’s take a look at a python version provided by pmneila. I add some comment in the code for a (hopefully) easier understanding. lightout_solver.pylightsout.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169# coding: utf-8\"\"\"The following code based onhttps://github.com/pmneila/Lights-Out/blob/master/lightsout.py\"\"\"from operator import addfrom itertools import chain, combinationsimport numpy as npfrom scipy import ndimage# First, we define operation in Galois Field (https://en.wikipedia.org/wiki/Finite_field)# Here what we need is an Z/2Z (A mod 2 Galois Fields)class GF2(object): \"\"\"Galois field GF(2).\"\"\" def __init__(self, a=0): self.value = int(a) &amp; 1 def __add__(self, rhs): return GF2(self.value + GF2(rhs).value) def __mul__(self, rhs): return GF2(self.value * GF2(rhs).value) def __sub__(self, rhs): return GF2(self.value - GF2(rhs).value) def __div__(self, rhs): return GF2(self.value / GF2(rhs).value) def __repr__(self): return str(self.value) def __eq__(self, rhs): if isinstance(rhs, GF2): return self.value == rhs.value return self.value == rhs def __le__(self, rhs): if isinstance(rhs, GF2): return self.value &lt;= rhs.value return self.value &lt;= rhs def __lt__(self, rhs): if isinstance(rhs, GF2): return self.value &lt; rhs.value return self.value &lt; rhs def __int__(self): return self.value def __long__(self): return self.value# Encapsulate operation for vectorization computationGF2array = np.vectorize(GF2)def gjel(A): \"\"\"Gauss-Jordan elimination.\"\"\" nulldim = 0 for i in xrange(len(A)): pivot = A[i:,i].argmax() + i if A[pivot,i] == 0: nulldim = len(A) - i break row = A[pivot] / A[pivot,i] A[pivot] = A[i] A[i] = row for j in xrange(len(A)): if j == i: continue A[j] -= row*A[j,i] return A, nulldimdef GF2inv(A): \"\"\"Inversion(逆) and eigenvectors(特征向量) of the null-space of a GF2 matrix.\"\"\" n = len(A) assert n == A.shape[1], \"Matrix must be square\" A = np.hstack([A, np.eye(n)]) B, nulldim = gjel(GF2array(A)) inverse = np.int_(B[-n:, -n:]) E = B[:n, :n] null_vectors = [] if nulldim &gt; 0: null_vectors = E[:, -nulldim:] null_vectors[-nulldim:, :] = GF2array(np.eye(nulldim)) null_vectors = np.int_(null_vectors.T) print \"inverse of matrix:\" print inverse print \"eigenvectors of matrix:\" print null_vectors return inverse, null_vectorsdef powerset(iterable): \"powerset([1,2,3]) --&gt; () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\" s = list(iterable) return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))def lightsoutbase(n): # base of the lights out problem of size (n, n) a = np.eye(n*n) a = np.reshape(a, (n*n, n, n)) # construct the A_&#123;ij&#125; Matrix a = np.array(map(ndimage.binary_dilation, a)) lo_base = np.reshape(a, (n*n, n*n)) return lo_baseclass LightsOut(object): \"\"\"Lights-Out solver.\"\"\" def __init__(self, size=5): self.n = size self.base = lightsoutbase(self.n) self.invbase, self.null_vectors = GF2inv(self.base) def solve(self, b): b = np.asarray(b) assert b.shape[0] == b.shape[1] == self.n, \"incompatible shape\" if not self.issolvable(b): raise ValueError, \"The given setup is not solvable\" # Find the base solution. first = np.dot(self.invbase, b.ravel()) &amp; 1 # Given a solution, we can find more valid solutions # adding any combination of the null vectors. # Find the solution with the minimum number of 1's. solutions = [(first + reduce(add, nvs, 0))&amp;1 for nvs in powerset(self.null_vectors)] final = min(solutions, key=lambda x: x.sum()) return np.reshape(final, (self.n,self.n)) def issolvable(self, b): \"\"\"Determine if the given configuration is solvable. A configuration is solvable if it is orthogonal to the null vectors of the base. \"\"\" b = np.asarray(b) assert b.shape[0] == b.shape[1] == self.n, \"incompatible shape\" b = b.ravel() p = map(lambda x: np.dot(x,b)&amp;1, self.null_vectors) return not any(p)if __name__ == \"__main__\": b = np.array([[1,0,1], [0,0,1], [1,0,0]]) print \"\\nlights status:\" print b lo = LightsOut(3) bsol = lo.solve(b) print \"\\nThe solution of\" print b print \"is\" print bsol run it and it give output like:12345678910111213141516171819202122232425262728$ python lightsout_solver.pylights status:[[1 0 1] [0 0 1] [1 0 0]]inverse of matrix:[[1 0 1 0 0 1 1 1 0] [0 0 0 0 1 0 1 1 1] [1 0 1 1 0 0 0 1 1] [0 0 1 0 1 1 0 0 1] [0 1 0 1 1 1 0 1 0] [1 0 0 1 1 0 1 0 0] [1 1 0 0 0 1 1 0 1] [1 1 1 0 1 0 0 0 0] [0 1 1 1 0 0 1 0 1]]eigenvectors of matrix:[]The solution of[[1 0 1] [0 0 1] [1 0 0]]is[[0 1 0] [0 1 0] [1 0 0]] To solve this equation to get those coefficent $x_{ij} = 1$ , we get a solution $[x_{12}, x_{22}, x_{31}]$. X O X O X O O O O O O O O O X switch (1,2) =&gt; O X X switch (2,2) =&gt; X O O switch (3,1) =&gt; O O O X O O X O O X X O O O O Solution 3: Light ChasingThe above solution is effective, but it’s also crazy to ask someone to solve a equation with 25 parameters in a Room Escape Game. The following approach is easy to follow and to get the solution. The Principle of this solution is “normalize different boards into several solvable boards, and solved them with known strategies”. Here is how it proceed: rows are manipulated one at a time starting with the top row. All the lights are turned on in the row by toggling the adjacent lights in the next row. apply the same method on 2-4 row. The last row is solved separately, depending on its active lights. Corresponding lights (see table below) in the top row are toggled and the initial algorithm is run again, resulting in a solution. Bottom row switch Top row XOOOX XXOOO OXOXO XOOXO XXXOO OXOOO OOXXX OOOXO XOXXO OOOOX OXXOX XOOOO XXOXX OOXOO This approach always lead to an solution (if there is any) for a 5 * 5 Grid. This might be the ideal solution when trapped in the game :).","categories":[{"name":"Game","slug":"Game","permalink":"http://fwz.github.io/categories/Game/"},{"name":"Room Escape","slug":"Game/Room-Escape","permalink":"http://fwz.github.io/categories/Game/Room-Escape/"}],"tags":[{"name":"Linear Algebra","slug":"Linear-Algebra","permalink":"http://fwz.github.io/tags/Linear-Algebra/"}]},{"title":"Evolution of Metric System Architecture","slug":"Evolution-of-Metric-System-Architecture","date":"2015-03-26T06:31:12.000Z","updated":"2019-05-02T17:20:41.751Z","comments":true,"path":"2015/03/26/Evolution-of-Metric-System-Architecture/","link":"","permalink":"http://fwz.github.io/2015/03/26/Evolution-of-Metric-System-Architecture/","excerpt":"PrefaceIn the past 2 years, I spent about 70% of my working time to build, to break, and to fix data products. This article is a brief retrospect of my understanding on building the whole systems, as well as what kind of tools could be plugged as components. Goal of a data SystemWe use data to understand reality and improve our product. This is the primary goal of a data/metric system. A good data system answers question, a better data system identifies root causes, and an even better data system help improve the whole system directly. Use casesIn Yahoo!, the data platform I am working on mainly support a Personalization System (Recommendation system). During the iteration of the recom system, we follow and forecast what would be the actual use cases for the team to understand or to improve the Recom system. The major use cases for our system includes: Understand system performance with reports from different key metrics Detect / identify metric abnormal / data pipeline failure Collect user feedback data to improve system online in short cycle Make it easy for PM/Dev/Scientist to play with data For different stage, we focus on different aspect and use different tools / techniques to solve problems. Let me illustrate.","text":"PrefaceIn the past 2 years, I spent about 70% of my working time to build, to break, and to fix data products. This article is a brief retrospect of my understanding on building the whole systems, as well as what kind of tools could be plugged as components. Goal of a data SystemWe use data to understand reality and improve our product. This is the primary goal of a data/metric system. A good data system answers question, a better data system identifies root causes, and an even better data system help improve the whole system directly. Use casesIn Yahoo!, the data platform I am working on mainly support a Personalization System (Recommendation system). During the iteration of the recom system, we follow and forecast what would be the actual use cases for the team to understand or to improve the Recom system. The major use cases for our system includes: Understand system performance with reports from different key metrics Detect / identify metric abnormal / data pipeline failure Collect user feedback data to improve system online in short cycle Make it easy for PM/Dev/Scientist to play with data For different stage, we focus on different aspect and use different tools / techniques to solve problems. Let me illustrate. Stage 1: System ValidationIn this Stage, both the recom system and the metric system are in prototype status. As data team, our top priority is to use data to identify whether the recom system work as expect, which means we care about our services more than our actual user at this stage. So we apply a “scraper”, mocking thousands of different queries to visit our backend system. Then extract the instrumentation we are interested in, and compare with our design using different user profile to generate And the above graph show a scraper pattern. We build a scraper to send multiple mock requests Analysis the statistical result from response of mock requests Write the statistical result to a local MySQL. The front end of data product call MySQL directly to get data and render Stage 2: Report System Performance Metrics Now we start to care more about user, we want to know how user in different segments interact with our business product. Thus we collect a user behaviour log (which is not produced from our side), and compute metrics like Retention Rate and CTR(click through rate) to measure user’s engagement. Before starting to expand our system, let’s review our use cases: “Understand system performance with reports from different key metrics”. Two estimation should be considered: volume of data &amp; latency of report. For volume of data, we should be aware of the “dimension explosion” effect. For example, we might have a user location segment/dimension when reporting our DAU metrics, if the location are split by nations, maybe 50 times of volume are needed, if the location are split by city, then the number will be much scary. This is only 1 dimension. Think about combination of different dimensions. The segment might based on age of the user, location of the user, login status of the user, the A/B test id, etc. the combination of dimensions will soon explode to a number you might not imagine before. So here we have to come up a solution of scalability. While volume of data might vary, latency of report should be the same. Instant response is our goal. A low latency should be a must have features. When interpret them into a design goal, they should be: Highly scalable Low latency on read queries StorageFirst of all, using Hadoop is natural, User behaviour logs (TB / daily) are aggregated on HDFS Hadoop is still a very good playground for such data manipulation. Considering the data volume / latency factor, We select HBase in our data storage layer. Scalable when data volume increase Friendly integration with HDFS and other Hadoop projects Good (enough) latency on range query No relation query use case in predictable future Some alternative might have their shortcomings, such as Hive could not provide instant response, and it take many maintenance effort to scale MySQL and keep data in sync. After stack selection done, we should figure out a general schema design. In most report system, schema cover “metric id/name”, “dimensions”, “values”. Since HBase is a rowkey based KV database, so this is also about designing the construction of rowkey, to make it represent “metric_id/name” and “dimensions”, and be backward compatible. And this is our rowkey schema: 1hash|metric_name^dim1_val^dim2_val^...|timestamp This schema provide following features: Load Balance. Hash is used as a load balanced technique by HBase, generated from md5 of “metric_name” + “dimensions”. This could guarantee that for the same combination of metrics and dimensions, they could fall into the same region server indicated by the hash value. Scanning operation is also efficient, since rows are sorted by row key. So same metrics with same dimensions are clustered and sorted by the timestamp. Given a time range then we could define a start row and end row to scan the rows between. Backward compatibility. Note that not all metrics share the same dimension. A shared configuration is used to store rowkey definition for different metrics. Dimensions are added sequentially in rowkey. For example, for metric M with dimension P, Q, the key looks like “M^p^q”. when adding a dimension S, the key looks like “M^p^q^s”. However, sometimes we want to ignore dimension S in the report. At this time, trailing empty dimension will be omitted in rowkey. We will still get “M^p^q”, which is backward compatible with the rowkey without the new dimension S. Data PipelineMost data pipelines are actually taking ETL operation against data. In our cases, ETL against logs mainly output aggregations number on limited fields. A more reasonable and natural choice on Hadoop is Pig instead of Hive. See more from Alan Gates’ summary. With the support of UDF (which could be written in Python/Ruby/JS), constructing an ETL data pipeline is effective. SchedulerAnother tools we need is job scheduler. If we want to generate regular daily report, the most straight-forward way is to start a cronjob to run the report generation pipeline regularly. But in real world, a pipeline might have external dependencies. In our case, we have to wait till the user log is available then we could start. Sure we can write a loop in the crontab to wait, or trigger this job with a reasonable delay? But how long should we wait? How could I start my pipeline once the dependency is ready? what if we have multiple dependencies? Besides, the pipeline topology should be taken cared. A pipeline in our case cover at least 2 steps: metrics generation and persistance. Each one is a different job. We should also trigger persistance job or fail the pipeline once the generation job succeed / failed. Apache Oozie came out and it save us tons of efforts. On many cases, we use it as a data-trigger, when all dependency data is ready, trigger a series of jobs. Data Product ServingTo secure our backend data and as a more regularized way to manage data, we implement a Serving layer using Tomcat. Since most operation happened in data products are READ operation, we mainly focus on RPS of Serving. For a internal report system, this layer could be very light weighted. FrontendWe made our FE more user-friendly by leveraging Bootstrap (For page Layout/CSS), Highchart (For the charting module). Node.js is used to communicate with Serving. Stage 3: Expand EcosystemAs the number of reporting metrics increase, some other problems / requirements emerged. We integrate more component to solve real world problems such as monitoring, metric self-service, online machine learning. Abnormal DetectionAs number of pipelines increases, probability of a broken pipeline also increases. First of all, data dependencies matters. Here data dependencies is actually a topology dependency, which mean pipeline A need pipeline B’s output as input. thus A depends on B. When pipeline B breaks or delays, pipeline A will also get blocked. Thus we get metrics delayed. For time sensitive metrics, detecting such issues become more important. Secondly, we’d like to detect abnormal value in metrics. For example, if Ads CTR become 0 after a release, we definitely want to get some notifications immediately instead of knowing revenue of the past week are blank one week later. So a detection system is implemented. For metric delay issue, we label each pipeline with expected running time, then detection system system build data dependencies graph according to configs and source code of data pipeline. Then we scan the intermediate data on HDFS to see whether each pipeline have produce output. In this way, we have enough information to get status of each pipelines. If delay occurs, we will be able to know why. To detect metric abnormally, we also define our metric detectors, which use configurable parameters to control detect algorithm. With a series of metrics data as input, could apply numerous algorithm to detect metric abnormal. One surprise finding is that, using a fixed threshold is one of our best friend if quality of source data could not be guaranteed. (To be illustrated) Ad-hoc QueryNow we have more and more data, people wish we could help answer their questions using our existing data, QUICKLY. With our previous architecture, such requirement / enquiry often require us to build and launch a new data pipeline (even not regularly), because the enquiry could not be computed from our existing metrics. It is not cost-effective for following reasons: 1. it take human resources to finish such a task. 2. communication cost is un-imaginable big especially for people in remote office. To make good use of data and reduce engineering efforts, we decided to build a data-warehouse: Store atomic data for both metric computation and ad-hoc query. Since the enquiry are mostly relational (Such as “what are the top 100 publishers (get most clicks) among mid-age user?”), we use Hive(https://hive.apache.org/) to build our data warehouse. With Support of HCatalog(http://hortonworks.com/hadoop/hcatalog/), Pig and Other MapReduce could easily access / operate data in Hive. Also an user interface is necessary for a data warehouse, currently we are using our own UI, and will switch to Hue(http://gethue.com/) in near future. Logging &amp; Model FeedbackNow also think about the essence of our system, data. As we could see, we collect only the user feedback data from the UI layer. For a recommendation system, logging runtime intermediate results will be helpful to build ML model when combined with the user feedback data. But it’s not clever to return these results to front-end (User UI) because it will increase latency so the data should be logged at server side and send to somewhere (in our case Hadoop). Also the Server has it’s own duty, so this logging pipeline should be as light-weight as possible. We finally choose Flume as our logging framework. For each server, a Flume client is added and it help we send data to HDFS via memory &amp; sockets. No Disk I/O is needed. Once we have both logs from server and user, we are able to join them together and get labeled training data, thus we could run ML training pipelines to generate new model. After necessary validation, we could then upload the model for recommendation server. Also, a handy way to check error log is helpful for debugging issues. We deploy a Splunk instance to collect system log, and provide an real-time search UI to the team member. So engineer could easily check what types of error happened for each machine with a unified way instead of logging in to each machine remotely. Lesson LearnedSimplify the MetricsComplicated metrics make itself hard to understand, hard to compare, and error-prone in the computation stage. Also, try to limited the number of metrics to make important decision, we could seldom make a decision when some number is encouraging us to move forward when some saying NO if there is too many metrics for optimization goal. Manage Metrics Life CyclesWhen a metrics is no more used, retire it. It takes resource to maintain a metric/pipelines. It’s much easier to retire a metrics than to deprecate existing code: just stop the pipeline. With version control (or even better continuous integration) support, we could restart it very soon in with a simple click. One way to identify which metric should be retired is “Don’t ask”, trust numbers. People are afraid of losing existing property. So when ask “May I retire these metrics”, we get “please don’t” for most cases. In our team, we setup a service to collect the logs to identify how many times the metrics have been requested by user, determine a threshold to filter some candidates, stop the pipeline with or without enquiry. Do it monthly. Define Project Goal Clearly Who will be our major user? Is it an internal tool? Or is it for business partner or real user? How long this projects suppose to support? How long the data suppose to support?The answer greatly impact our future decision of design and resources allocation. Understand and Clean data Equip data expert so won’t get lost when facing data quality problem Manage dimension explosion. We might get more data rather than what we expect because of some special cases. Try to remove/lower the impact given by low quality data. For example, filter out low counted aggregation record. Finally, always ask questions. When something is weird, ask. When you think something is interesting, ask. When we ask more, we get much more.","categories":[{"name":"Engineering","slug":"Engineering","permalink":"http://fwz.github.io/categories/Engineering/"},{"name":"Big Data","slug":"Engineering/Big-Data","permalink":"http://fwz.github.io/categories/Engineering/Big-Data/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://fwz.github.io/tags/Hive/"},{"name":"Pig","slug":"Pig","permalink":"http://fwz.github.io/tags/Pig/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://fwz.github.io/tags/Hadoop/"},{"name":"HBase","slug":"HBase","permalink":"http://fwz.github.io/tags/HBase/"},{"name":"Oozie","slug":"Oozie","permalink":"http://fwz.github.io/tags/Oozie/"},{"name":"Flume","slug":"Flume","permalink":"http://fwz.github.io/tags/Flume/"},{"name":"Splunk","slug":"Splunk","permalink":"http://fwz.github.io/tags/Splunk/"}]},{"title":"Resistance:Avalon 体验","slug":"ResistanceAvalon体验","date":"2015-02-22T12:19:52.000Z","updated":"2019-05-02T17:20:41.782Z","comments":true,"path":"2015/02/22/ResistanceAvalon体验/","link":"","permalink":"http://fwz.github.io/2015/02/22/ResistanceAvalon体验/","excerpt":"过年约了几个兄弟姐妹去玩桌游，大师Neo带了几个桌游过来，玩了一回合菲力猫，人到齐了以后玩了两局，，有赢有输，超级好玩。 总结一下玩了几盘以后的经验： 最大信息量的是投票阶段。需要记住当前的派票情况，每个人的表决是否和之前阐述理由的时候观点一致，以进行下一步的推理。多通过玩家的举动而非言语来判断玩家的身份。 推理身份的时候需要前提，假如在当前前提下，无法合理推出当前发生的事实（投票和任务结果），表明预设的玩家身份前提很可能错误。 除非特殊情况，能参与任务却投反对的很有可能是正义方，因为他觉得有邪恶方人物参与任务。但也不排除邪恶方觉得肯定无法通过投票，通过投反对进行掩饰将分配拖入下一轮。","text":"过年约了几个兄弟姐妹去玩桌游，大师Neo带了几个桌游过来，玩了一回合菲力猫，人到齐了以后玩了两局，，有赢有输，超级好玩。 总结一下玩了几盘以后的经验： 最大信息量的是投票阶段。需要记住当前的派票情况，每个人的表决是否和之前阐述理由的时候观点一致，以进行下一步的推理。多通过玩家的举动而非言语来判断玩家的身份。 推理身份的时候需要前提，假如在当前前提下，无法合理推出当前发生的事实（投票和任务结果），表明预设的玩家身份前提很可能错误。 除非特殊情况，能参与任务却投反对的很有可能是正义方，因为他觉得有邪恶方人物参与任务。但也不排除邪恶方觉得肯定无法通过投票，通过投反对进行掩饰将分配拖入下一轮。 正义方：推理出来谁是正义一方（或者梅林）就要坚定的跳出来保他。保对了除了更有可能拿到好人的票以外，还能保护梅林（因为梅林和邪恶方都知道场上的方分布，能混淆邪恶方）不过一旦推错了，反贼就知道你不是梅林，没法挡刀。不过再厉害一点，梅林可以通过错保邪恶角色来掩饰自己的身份。忠臣是最考验推理的角色，很好玩。 邪恶方：一定要声称自己是好人，然后不要考虑自己要怎么拿票，一定要站在从正义方的角度进行推理，注意语气和用词。过程中总会有人跳出来质疑，从质疑的声音中找到梅林。当然投票的时候该怎么投怎么投。邪恶方考验的是伪装，隐藏和陷害。 梅林：伪装。等到有一两个人开始集火邪恶方势力的时候，再开始加入。同时，想办法找到帕西维尔。 附录：简单版规则游戏分为两方：正义方和邪恶方。一次游戏流程内有5个任务，每个任务必然有一方胜利。 游戏胜利条件：正义方内有特殊角色梅林（merlyn）和忠臣，正方赢得3个任务，而且反方无法识别出梅林就可以胜利。邪恶方赢得3个任务后立即胜利，或者在正方赢得3个任务后，正确识别出梅林也可以胜利。 任务胜利条件：每个任务中，所有玩家轮流担任“领袖”，领袖将任务参与权利（每个任务需要的参与人随着游戏进行不断改变）进行初始分配，然后大家进行讨论，然后领袖将任务参与权利进行最后分配，所有玩家对这次分配进行表决，若没有过半数以上同意分配方案，则此玩家担任领袖失败，由下一玩家担任领袖重新开始此轮任务（但任务并没有失败）。若半数以上同意分配方案则开始进行任务。每个任务中，假如没有人破坏任务，则任务成功，假如有邪恶势力混入并投出破坏票，则任务失败（人数增多以后有特殊规则，允许某次任务混入一个邪恶势力）。 七人局特殊人物：帕西维尔：正义方，知道哪两个人是梅林和莫甘娜，但不知道谁是谁。七人局会多一个邪恶方，所以需要帕西维尔给梅林挡刀，梅林也要找出帕西维尔。莫甘娜：邪恶方，主要作用是混淆帕西维尔。","categories":[{"name":"Game","slug":"Game","permalink":"http://fwz.github.io/categories/Game/"},{"name":"Board Game","slug":"Game/Board-Game","permalink":"http://fwz.github.io/categories/Game/Board-Game/"}],"tags":[{"name":"Avalon","slug":"Avalon","permalink":"http://fwz.github.io/tags/Avalon/"}]},{"title":"Git: under the basics","slug":"Git-under-the-basics","date":"2014-11-01T12:04:28.000Z","updated":"2019-05-02T17:20:41.760Z","comments":true,"path":"2014/11/01/Git-under-the-basics/","link":"","permalink":"http://fwz.github.io/2014/11/01/Git-under-the-basics/","excerpt":"My boss told me that my goal in this quarter is to working on Continuous Integration for our current product, and all of a sudden I think there’s a lot of gap between the goal and my current skill. The first thing came into my mind is that: “Ohhh, I am still not quite familiar with Git”. After a short period of panic, I sit down to learn about git. And here’s my note. If you think you could learn git with manual after you learn how to branch, commit and merge, then you might probably be dispointted. Git is very flexible but it do something in a more novel way, so certain understanding of it’s internal is necessary for mastering it, and would be helpful when you look for help in manual. For example, I hear about so many terms such as “HEAD”, “Index”, “Ref”, “Staging Area”, but I could not tell exactly what is that, and I don’t even know how git works. After some diving, I wrapped something very basic in this post.","text":"My boss told me that my goal in this quarter is to working on Continuous Integration for our current product, and all of a sudden I think there’s a lot of gap between the goal and my current skill. The first thing came into my mind is that: “Ohhh, I am still not quite familiar with Git”. After a short period of panic, I sit down to learn about git. And here’s my note. If you think you could learn git with manual after you learn how to branch, commit and merge, then you might probably be dispointted. Git is very flexible but it do something in a more novel way, so certain understanding of it’s internal is necessary for mastering it, and would be helpful when you look for help in manual. For example, I hear about so many terms such as “HEAD”, “Index”, “Ref”, “Staging Area”, but I could not tell exactly what is that, and I don’t even know how git works. After some diving, I wrapped something very basic in this post. TermsHEADThe first one is “HEAD”. It could be described as any of following: The symbolic name for commit you’re working on top of. Always points to the most recent commit of the checkouted branch. Is the parent of your next commit. When you commit, the status of current branch is altered and this change is visible through HEAD. To Navigate from HEAD, we could use ^ notation and ~ notation HEAD^ -&gt; the commit before HEAD HEAD~{Number} -&gt; the {Number}th commit before HEAD If we navigate to a git repo and cat the .git/HEAD file, the content is telling us that we need to look at the file refs/heads/master in the .git directory to find out where HEAD points. 1234$ cat .git/HEADref: refs/heads/master$ cat .git/refs/heads/master9c65d51b2c8f405debdf9b100505f814981e8940 And the .git/refs/heads/{CURRENT_BRANCH} points to the last commit of this branch. After we switch the branch, we could see that the HEAD is now pointing to another branch. 1234$ git checkout hotfixSwitched to branch &apos;hotfix&apos;$ cat .git/HEADref: refs/heads/hotfix Most git commands which make changes to the working tree will start by changing HEAD. The second term is index. Index Index – where you place files you want committed to the git repository. Alias Cache Directory cache Staging area Staged files After we git add &lt;file&gt; a file, it’s in the index. See the light blue box in the following workflow chart. Ref Ref is not mysterial, it is just a reference (pointer) to a commit/tag/branch. 123456$ git show-refbeb8ee5d9a968842a8ec3a5a689b2e993ef02e40 refs/heads/master15eca7bbc33f148cd0072cdc0ff10011951bb98a refs/remotes/origin/master15eca7bbc33f148cd0072cdc0ff10011951bb98a refs/remotes/origin/HEAD3518ffa48c41a7a1a1a670975e501b8eeae259ea refs/stash7beec4dfaa7b0af8b8a8c4120ad782327049404f refs/tags/ci-trunk-0.1 We could use git cat-file -p to get the content of a pretty-printed reference.1234567$ git cat-file -p refs/heads/mastertree 5a20d43b086432fbee9775a1a1f042523733b807parent 29b9e11cf64b8a1901341be6a20899ec3bd306caauthor wenzhong &lt;wenzhong@example.com&gt; 1413274718 +0800committer wenzhong &lt;wenzhong@example.com&gt; 1413274718 +0800remove p13nsingal checking pipeline in bundle How Git store history?OK, It’s time to take a look at how git work. The secret lies on the “.git” directory, and the “objects” sub directory store all objects and history of this repository. name Usage .git/HEAD file, Point to current branch .git/index file, store staging area info .git/refs directory, store pointers points to commit .git/objects directory, store all data For a newly initialized repo, the objects directory looks like this1234$ tree .git/objects.git/objects├─ info└─ pack After I do a simple commit, the objects directory are now with 3 new objects added. 12345678910111213$ echo &quot;test object v1&quot; &gt; README.md$ git add README.md$ git commit -m &quot;initial commit&quot;$ tree .git/objects.git/objects├─ ab│ └ b08c95ed3c6e5623f0e5b49bcdff0cbac74d4a├─ c0│ └ baa8366339e7e0d2e8a1f4d2a6b70e38ce9164├─ ef│ └ 0f5c785b315ad24cbd5997b67090fc71b7c5ce├─ info└─ pack So, what is this? There’s mainly 3 types of objects in git – “Commit”, “Tree” and “Blob”. A BLOB is a file under a version A TREE is a directory, including blobs and sub-tree (sub-dir) under this dir. A COMMIT will point to the repository tree it based on, and also contain commit info (author,message) cat-file is our friend. Let’s examine the object with it. The “-t” parameters will tell us the type of this objects. 12345678$ git cat-file -t c0baa8366339e7e0d2e8a1f4d2a6b70e38ce9164commit$ git cat-file -p c0baa8366339e7e0d2e8a1f4d2a6b70e38ce9164tree ef0f5c785b315ad24cbd5997b67090fc71b7c5ceauthor wenzhong &lt;example@gmail.com&gt; 1413733136 +0800committer wenzhong &lt;example@gmail.com&gt; 1413733136 +0800initial commit It’s a commit and contain a tree object “ef0f….”. 1234$ git cat-file -t ef0f5c785b315ad24cbd5997b67090fc71b7c5cetree$ git cat-file -p ef0f5c785b315ad24cbd5997b67090fc71b7c5ce100644 blob abb08c95ed3c6e5623f0e5b49bcdff0cbac74d4a README.md The tree object now contain a blob object. 1234$ git cat-file -t abb08c95ed3c6e5623f0e5b49bcdff0cbac74d4ablob$ git cat-file -p abb08c95ed3c6e5623f0e5b49bcdff0cbac74d4atest object v1 Then, how about multiple commits? how each commit know which commit it based on?There would be a parent pointer pointing to the last commit in each commit object. Now we should have a basic understanding about how git store our history in the .git/objects. SHA1 digest In Git, objects are named / located via its SHA1-digest. SHA1 will generate a 160 bit Byte array Object abb08c95ed3c6e5623f0e5b49bcdff0cbac74d4a will be sent to the “ab” directory 123.git/objects├─ ab│ └ b08c95ed3c6e5623f0e5b49bcdff0cbac74d4a What about SHA1 collision really really really damn unlikely– Linus But if it happens no new object is created. commit will ends up pointing to old object. could be noticed in git pull or git clone or something might relavant to a tree diff Fix it by adding minor comment Here’s an example to give you an idea of what it would take to get a SHA-1 collision. If all 6.5 billion humans on Earth were programming, and every second, each one was producing code that was the equivalent of the entire Linux kernel history (1 million Git objects) and pushing it into one enormous Git repository, it would take 5 years until that repository contained enough objects to have a 50% probability of a single SHA-1 object collision. A higher probability exists that every member of your programming team will be attacked and killed by wolves in unrelated incidents on the same night. BranchingBranch is cheap Questions: What does “branching is cheap” mean in Git? Answer: Switching branch in Git is simply moving a lightweight movable pointer to one of existing commits. Create a branch When run git branch {name_of_branch}, a few things happen: A reference is created to the local branch at: .git/refs/heads/{name_of_branch}. point to the commit of current HEAD points to. Switching branch is moving HEAD MergeFrom the branch you currently on, use git merge $FROM_BRANCH to merge changes from $FROM_BRANCH. 1234567891011$ git branch master$ git merge try_branch$ git log --graph --pretty='%h %s'* 3729056 merge the try_branch branch|\\| * 802e6ea first commit on try_branch* | 4292dd0 add example of pretty print in git log* | 986eda3 add useful git log options usage|/* 9b7dcc9 add some further change* 6e2494b init commit Another useful option to figure out what state your branches are in is to filter output from git branch -v to branches that you have or have not yet merged into the branch you’re currently on. The useful –merged and –no-merged options have been available in Git since version 1.5.6 for this purpose. To see which branches are already merged into the branch you’re on, you can run git branch --merged 123$ git branch --merged* master try_branch Because I have merged “try_branch” branch, so I see it now. How about this? 123$ git checkout -b hotfix$ git branch --no-merged$ No branch is un-merged? Why? The reason is that we just create a branch and no commit on it, so both the HEAD pointer of master branch and hotfix branch are pointing to the same commit 3729056. 1234$ git branch -v hotfix 3729056 merge the try_branch branch* master 3729056 merge the try_branch branch try_branch 802e6ea first commit on try_branch now do something on hotfix branch. And rerun the git branch --no-merged 1234567$ git commit -a -m \"apply a hotfix\"[hotfix f7963a5] apply a hotfix 1 file changed, 61 insertions(+)$ git co masterSwitched to branch 'master'$ git branch --no-mergedhotfix Good, now merge the new commit.1234567891011121314151617181920212223$ git merge hotfixAuto-merging README.mdCONFLICT (content): Merge conflict in README.mdAutomatic merge failed; fix conflicts and then commit the result.$ vim README.md$ git add README.md$ git commit -m \"merge hotfix\"[master cc87eec] merge hotfix$ git log --graph --pretty='%h %s'* cc87eec merge hotfix|\\| * 75828cc apply another hotfix| * f7963a5 apply a hotfix* | ec00aea apply change on master|/* 3729056 merge the try_branch branch|\\| * 802e6ea first commit on try_branch* | 4292dd0 add example of pretty print in git log* | 986eda3 add useful git log options usage|/* 9b7dcc9 add some further change* 6e2494b init commit Remote BranchesIt’s important to remember when you’re doing above that these branches are completely local. When you’re branching and merging, everything is being done only in your Git repository — no server communication is happening. This used to cause a few headache. Let’s add a remote repo (in this case, bitbucket.org. Of course we could switch it to github). `git remote add origin ssh://git@bitbucket.org/wenzhong/git_learning.git` And push all refs to this origin after creating an empty project on bitbucket. 1234567891011121314$ git push -u origin --allWarning: Permanently added the RSA host key for IP address '131.103.20.168' to the list of known hosts.Counting objects: 30, done.Delta compression using up to 8 threads.Compressing objects: 100% (20/20), done.Writing objects: 100% (30/30), 4.44 KiB, done.Total 30 (delta 9), reused 0 (delta 0)To ssh://git@bitbucket.org/wenzhong/git_learning.git * [new branch] hotfix -&gt; hotfix * [new branch] master -&gt; master * [new branch] try_branch -&gt; try_branchBranch hotfix set up to track remote branch hotfix from origin.Branch master set up to track remote branch master from origin.Branch try_branch set up to track remote branch try_branch from origin. The -u parameters here is telling git that, for every branch that is up to date or successfully pushed, add upstream (tracking) reference, so they could be used by argument-less git-pull command And let’s pretend there’re other collaborators who clone this code and push some update. I check out this code to another location (how about call my first local repo “repo1” and this new local repo “repo2”?)in my laptop. And add some changes. 123456$ git commit -a -m &quot;some changes&quot;[master 93965e6] some changes 1 file changed, 48 insertions(+), 47 deletions(-)$ git push originTo git@bitbucket.org:wenzhong/git_learning.git cc87eec..93965e6 master -&gt; master Now, there’re 2 different states. 1 state from remote repo (bitbucket.org) and it has been updated by repo2. 1 state from repo1, which is identical with the initial state on remote repo before repo2 push his changes. repo1 know nothing about repo2, now do some change on “repo1” and try to push my work to the remote repo. Synchronize workNow I run git fetch origin from repo1.This command looks up which server origin is (in this case, it’s bitbucket.org), fetches any data from it that I don’t yet have, and updates my local database, moving my origin/master pointer to its new, more up-to-date position. At this time, there’re still two branch for repo1 – origin/master, local/master. They are not the same. origin/master include changes from repo2. local/mastera include changes from repo1, they are not pushed yet. That means we get a reference to origin’s master branch locally. But now I want to share my work, push it up to the remote. my local branches aren’t automatically synchronized to the remotes I write to – I have to explicitly push the branch. Now use git push origin master. Note that master is the branch name of my local branch. You can also use git push origin master:new_master to create a new_master branch on remote “origin”. Next time, when repo2 fetches from server, they will get a references to where the server’s version of new_master is under the remote branch origin/new_master. $ git push origin master:new_master ... To ssh://git@bitbucket.org/wenzhong/git_learning.git * [new branch] master -&gt; new_master Now, repo2 can fetch this new branch by git fetch origin $ git fetch origin From bitbucket.org:wenzhong/git_learning * [new branch] new_master -&gt; origin/new_master It’s important to note that when you do a fetch that brings down new remote branches, you don’t automatically have local, editable copies of them. In other words, in this case, you don’t have a new new_master branch — you only have an origin/new_master pointer that you can’t modify. $ git checkout -b new_master origin/new_master Branch new_master set up to track remote branch new_master from origin. Switched to a new branch &apos;new_master&apos; So far so good. But wait, what happen if I push the repo1/master to origin/master? I assume there would be conflicts. 12345678$ git push originTo ssh://git@bitbucket.org/wenzhong/git_learning.git ! [rejected] master -&gt; master (non-fast-forward)error: failed to push some refs to 'ssh://git@bitbucket.org/wenzhong/git_learning.git'hint: Updates were rejected because the tip of your current branch is behindhint: its remote counterpart. Merge the remote changes (e.g. 'git pull')hint: before pushing again.hint: See the 'Note about fast-forwards' in 'git push --help' for details. Follow it’s hint and run git pull origin master Clean your change before Merging12345678$ git pull origin master...From ssh://bitbucket.org/wenzhong/git_learning * branch master -&gt; FETCH_HEADUpdating cc87eec..93965e6error: Your local changes to the following files would be overwritten by merge: README.mdPlease, commit your changes or stash them before you can merge. Git require that our repo should be clean before merging remote changes, so local repo will not corrupted. See man git-mergeWarning: Running git merge with uncommitted changes is discouraged: while possible, it leaves you in a state that is hard to back out of in the case of a conflict. So, let’s commit our current change (or do a git stash) and try to merge changes from origin/master brought by repo2.123456789$ git commit -m \"add syncing remote &amp; local repo\"[master 205e4ce] add syncing remote &amp; local repo 1 file changed, 43 insertions(+)$ git pull origin masterrom ssh://bitbucket.org/wenzhong/git_learning * branch master -&gt; FETCH_HEADAuto-merging README.mdCONFLICT (content): Merge conflict in README.mdAutomatic merge failed; fix conflicts and then commit the result. That’s expected, remember that in repo2, we move the tip section to the bottom. (Yes, if you check out commit 986eda3, tips are on top of this README file. and repo2 put it to the bottom at commit 93965e6). So resolve conflicts. and git commit -a -m &quot;merge changes from repo2 and apply my fix&quot; 1234$ git push origin master...ssh://git@bitbucket.org/wenzhong/git_learning.git93965e6..586c16f master -&gt; master That’s it.To summarize, in repo1, we: Fetch change from origin/master (latest updated by repo2) try to automatically merge our local change (but could not) merge it locally push to origin/master Deleting Remote BranchesNow I think the branch “new_branch” have finished its duty, and I want to delete it from the remote server. I will use git push origin :new_branch. A way to remember this command is by recalling the git push [remotename] [local- branch]:[remotebranch] syntax that we went over a bit earlier. If you leave off the [localbranch] portion, then you’re basically saying, “Take nothing on my side and make it be [remotebranch].” 123$ git push origin :new_masterTo ssh://git@bitbucket.org/wenzhong/git_learning.git - [deleted] new_master RebasingIn Git, there are two main ways to integrate changes from one branch into another: the merge and the rebase. With the rebase command, we can take all the changes that were committed on one branch and replay them on another one. It works by: going to the common ancestor of the two branches (the one you’re on and the one you’re rebasing onto), getting the diff introduced by each commit of the branch you’re on, saving those diffs to temporary files, resetting the current branch to the same commit as the branch you are rebasing onto, and finally applying each change in turn. Cherry-pickA more fancy way to merge changes is the git cherry-pick. git cherry-pick - Apply the changes introduced by some existing commits A typical use case is that you could pick some commits from a dev branch to master branch (not all of them). Another use case I could think of is that when tracking a bug, you might add debug info, commit and trigger CI to reproduce problem, and apply fix commit. using cherry-pick then you could only apply don’t have to remove the debug code in your apply fix commit. Tagging Branches are easy to move around and often refer to different commits as work is completed on them. Branches are easily mutated, often temporary, and always changing. If that’s the case, you may be wondering if there’s a way to permanently mark historical points in your project’s history. For things like major releases and big merges git-tag - Create, list, delete or verify a tag object signed with GPG by modifying tag reference in .git/refs/tags/ Describe how far way from you and the tag? you could use git describe to Show the most recent tag that is reachable from a commit output of git describe will be &lt;tag&gt;_&lt;numCommits&gt;_g&lt;hash&gt;, Where tag is the closest ancestor tag in history, numCommits is how many commits away that tag is, and is the hash of the commit being described. StashThink about working on a new feature modifying files in the working directory and/or index and you find out you need to fix a bug on a different branch. You can’t just switch / create a different branch because it will lose all your work. git stash Saves your working directory and index to a safe place Using git stash pop to restores your working directory and index to the most recent commit Of course, you could commit your current change, move HEAD to HEAD^, then create a branch. But sometime your current change is not complete as a commit. You don’t want dirty commit added to your repo. git stash give you a clearer way to do this Hooks Some action pre/post each “git action” could be taken by git hooks e.g. pre-commit script would be called before commit, here we run a simple ‘run_test.py’ to run test before actually commit something 123456789101112131415#!/usr/bin/env bashif git diff-index --quiet HEAD --; then #no changed between index and working copy; just run tests bin/run_tests.py RESULT=$?else #Test the version that's about to be committed #stashing all unindexed changes git stash -q --keep-index bin/run_tests.py RESULT=$? git stash pop -pfi[ $RESULT -ne 0 ] &amp;&amp; exit 1exit 0 Tips and TricksInspect commitsThere are many options can be used in git log. Some are extremely useful: git log –graph git log –stat git log –since=”2013-10-01” –before=”2014-03-01” –author=fwz 12345git log --pretty=\"%h - %ad - %an - %s\"986eda3 - Sat Mar 1 20:15:12 2014 +0800 - fwz - add useful git log options usage9b7dcc9 - Sat Mar 1 20:01:06 2014 +0800 - fwz - add some further change6e2494b - Sat Mar 1 19:44:56 2014 +0800 - fwz - init commit git show – reports the changes introduced by the most recent commit: Auto completion Git comes with a nice auto-completion script for Bash User. Get the latest git-completion.sh from Github Put it in your HOME directory and Put source ~/git-completion.sh to source it when you login. This also works with options, which is probably more useful.12$ git log --s&lt;tab&gt;--shortstat --since= --src-prefix= --stat --summary if you use zsh, you can get more surprises. For instance, if you’re running a git log command and can’t remember one of the options, you can start typing it and press Tab to see what matches: That’s a pretty nice trick and may save you some time and documentation reading.use D3 as an example. AliasesGit doesn’t infer your command if you type it in partially. If you don’t want to type the entire text of each of the Git commands, you can easily set up an alias for each command using git config. Note: the global settings git config global is under ~/.gitconfig. Here’s my simple aliases. 123456789101112131415 [user] email = wenzhong.work@gmail.com name = fwz [core] editor = /usr/local/bin/vim whitespace = trailing-space,space-before-tab [alias] ci = commit co = checkout st = status br = branch unstage = reset HEAD -- last = log -1 HEAD [color]* ui = true Diffgit diff can be used to list differences between working tree and index, or between index and commit, or between working tree and commit working tree : your working directory. index file(stage): Files in the git index are files(after git add) that git would commit to the git repository if you used the git commit command. This is a brigde between working tree and commit commit: the last stage. after commit, all changes will checked in git repo. 123git diff : show the differences between working tree and index filegit diff --cached : show the differences between index file and commitgit diff HEAD: show the didferences between working tree and commit(HEAD means the latest commit) Diff Cont.git diff usually list lots of changes to stdout. If you want to use less as the default pager, below is one solution. For current project 1git config core.pager &apos;less -r&apos; For all projects 1git config --global core.pager &apos;less -r&apos; CloneIf you just want to clone one branch from github, what you have to do: 12git remote add -t $BRANCH -f origin $REMOTE_REPOgit checkout $BRANCH If you jsut want to clone specific commits(say latest commit), what you have to do: 1git clone --depth=1 $REMOTE_REPO List deleted/Add filesIf you want to know when and which commit delete a file, 1git log --diff-filter=D --summary If you want to know when and which commit add a file, 1git log --diff-filter=A --summary search string from all versions in git reposIf you want to get a piece of code, variable, function, file in the repo, but you can not find it.Maybe it has been deleted for long time. How can I know when and who delete it? 12345git rev-list --all|( while read revision; do git grep -F &apos;Your search string&apos; $revision done) Useful Materials Learn Git Branching Atlassian’s Git Tutorial Introduction to Git with Scott Chacon of GitHub","categories":[{"name":"Engineering","slug":"Engineering","permalink":"http://fwz.github.io/categories/Engineering/"},{"name":"Version Control","slug":"Engineering/Version-Control","permalink":"http://fwz.github.io/categories/Engineering/Version-Control/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://fwz.github.io/tags/Git/"}]},{"title":"简约至上：交互式设计四策略 读后感","slug":"【读书笔记】简约至上：交互式设计四策略","date":"2014-09-12T05:30:09.000Z","updated":"2019-05-02T17:20:41.815Z","comments":true,"path":"2014/09/12/【读书笔记】简约至上：交互式设计四策略/","link":"","permalink":"http://fwz.github.io/2014/09/12/【读书笔记】简约至上：交互式设计四策略/","excerpt":"","text":"在重构一个遗留前端系统的时候，我觉得需要有一些指导原则来引领我做设计。正好看到了这本书，摘录一些有益的观点。 普适观点 人们喜欢简单、值得信赖、适应性强的产品 考虑大多数用户的体验，让他们觉得产品井然有序，轻松自在。他们正在掌控着一切。 改变会产生影响，需要有办法（最好是公式）来衡量究竟是正面影响大，还是负面影响大 描述你的设计 使产品的设计符合用户使用产品的环境，要意识到影响用户体验的因素极多 简单的用户体验是初学者、新手的体验，或者是压力之下的主流用户的体验 想要实现简单的体验，需要将目标定得极端，这样能保持产品迭代朝着正确方向前进。例如将目标定位“瞬间响应”而不是“快速响应”，这样我们能在开发新功能时时刻提醒自己。 实现简化的4个策略：删除 去掉不必要的功能，直到不能再减 组织 按照有意义的标准将他们划分成不同的组 隐藏 隐藏不是最重要的功能，避免分散用户注意力 转移 将复杂性转移到其它地方。例如遥控器保留具备最基本功能的按钮，而将其它控制放到电视屏幕的菜单上 对待客户需求 不要简单地因为客户要求就增加功能，应该对用户的要求做逆向工程——搞清楚用户到底遇到的问题是什么，然后仔细斟酌这个问题是不是应该由软件来解决。 增加功能不一定能让用户体验更简单，反而经常导致更多的迷惑。 Simplicity is the ultimate sophistication – Leonardo Da Vinci","categories":[{"name":"Read & Learn","slug":"Read-Learn","permalink":"http://fwz.github.io/categories/Read-Learn/"},{"name":"Book Review","slug":"Read-Learn/Book-Review","permalink":"http://fwz.github.io/categories/Read-Learn/Book-Review/"}],"tags":[{"name":"Reading","slug":"Reading","permalink":"http://fwz.github.io/tags/Reading/"}]},{"title":"Sync two Git remote repositories","slug":"syncing-two-remote-repository","date":"2014-08-28T16:03:20.000Z","updated":"2016-09-05T03:39:54.000Z","comments":true,"path":"2014/08/29/syncing-two-remote-repository/","link":"","permalink":"http://fwz.github.io/2014/08/29/syncing-two-remote-repository/","excerpt":"In Yahoo we use Gerrit as our code review tool. Engineers commit code changes to Gerrit for review. After code has been reviewed by peers, Gerrit help push to Github. However sometimes bad thing happens. For example, if a committer forget to setup Gerrit env and the code is committed and pushed to Github directly (because he has this permission to do so), he will notice this soon because future changes from Gerrit might break. Then he try to submit a review for the missing commit to Gerrit with an “git commit –amend” to generate a change-id (which is used by Gerrit). Because “–amend” generate different commit-id, so after the review is passed, even the content is the same, the commit-id in two remote repo (Gerrit and Github) is different, which leads to future reviews are still not able to pushed to Github from Gerrit and sometimes new review could not be submitted. So how could we fix it? It would be a good idea to push changes from Github to Gerrit to get them in sync. Here are two options.","text":"In Yahoo we use Gerrit as our code review tool. Engineers commit code changes to Gerrit for review. After code has been reviewed by peers, Gerrit help push to Github. However sometimes bad thing happens. For example, if a committer forget to setup Gerrit env and the code is committed and pushed to Github directly (because he has this permission to do so), he will notice this soon because future changes from Gerrit might break. Then he try to submit a review for the missing commit to Gerrit with an “git commit –amend” to generate a change-id (which is used by Gerrit). Because “–amend” generate different commit-id, so after the review is passed, even the content is the same, the commit-id in two remote repo (Gerrit and Github) is different, which leads to future reviews are still not able to pushed to Github from Gerrit and sometimes new review could not be submitted. So how could we fix it? It would be a good idea to push changes from Github to Gerrit to get them in sync. Here are two options. first approach12git pull origin mastergit push gerrit master --force Usually, the command refuses to update a remote ref that is not an ancestor of the local ref used to overwrite it. This flag disables the check. So latest version from “origin” now could be pushed to “gerrit”. By this way, some commit will be lost, but quite straight forward. second approach123git reset --hard gerrit/mastergit merge origin/mastergit push gerrit master In this approach, we first make local repo get synced with “gerrit/master”, then merge changes with “origin/master”. Because we merge changes with “origin/master” so the commit-id of this merge are based on the one on “origin”, and both “gerrit” and “origin” are supposed to accept this commit. After we push this merged changes to “gerrit”, “gerrit” is able to push it to “origin”. Then two repo get synced. By this way, no commit will be lost.","categories":[{"name":"Engineering","slug":"Engineering","permalink":"http://fwz.github.io/categories/Engineering/"},{"name":"Version Control","slug":"Engineering/Version-Control","permalink":"http://fwz.github.io/categories/Engineering/Version-Control/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://fwz.github.io/tags/Git/"}]},{"title":"Recommended workflows in Alfred","slug":"Recommended-workflows-in-Alfred","date":"2014-07-13T15:06:52.000Z","updated":"2019-05-02T17:20:41.777Z","comments":true,"path":"2014/07/13/Recommended-workflows-in-Alfred/","link":"","permalink":"http://fwz.github.io/2014/07/13/Recommended-workflows-in-Alfred/","excerpt":"Finally I purchase Alfred for workflows. A Workflow is a combination of actions, and the killer feature in Alfred powerpack. In Alfred, we can import existing workflows or create our own workflows – to run a series of actions, which will dramatcially improve productivity. My workflow listsBefore writing this post, I spent about half an hour on Alfred Workflows to go through all existing workflows based on the AlfredWorkflow repo by hzlzh and select following workflows as enhancement of Alfred. There is another workflow repo by @zenorocha. And here’s my lists.","text":"Finally I purchase Alfred for workflows. A Workflow is a combination of actions, and the killer feature in Alfred powerpack. In Alfred, we can import existing workflows or create our own workflows – to run a series of actions, which will dramatcially improve productivity. My workflow listsBefore writing this post, I spent about half an hour on Alfred Workflows to go through all existing workflows based on the AlfredWorkflow repo by hzlzh and select following workflows as enhancement of Alfred. There is another workflow repo by @zenorocha. And here’s my lists. Workflows usage Add-task-to-Things Use “task” to add task into Things. Adium Use “im {User}” to search online User in Adium AlfredTweet-2 Use “tweet” to send tweets Baidu-Map Use “bmap” to locate Baidu_Search Use “bd” to search on baidu Dash Use “dash” to search libaray/methods in Dash Dianping Use “dianping {merchants}” to search merchants Douban Use “book / movie / music” to search items on douban.com Drop-in-Pocket Use “pocket {URL}” to save webpage into pocket Eject Use “eject” to eject all ejectable devices Evernote Use “en {item}” to search item in evernote. Use “ennew” to create new note in evernote. Kill+Process Use “kill {process name}” to kill a process Maven Use “mvn” to find packges in maven NPM-Search Use “npm” to search node modules Node.js-docs Use “nodejs” to locate node libraris Open-Airdrop Use “airdrop” to activate Airdrop pm2.5-alfred Use “pm2.5 to search pm2.5 index”. (But seems using a restricted API ) stackoverflow Use “st” to search stuffs in stack overflow zhihu Use “zh” to search quesiton or people in zhihu, Use “zhdaily” to search in zhihu Daily 有道翻译 Use “yd {terms}” to search translation Install workflowsdouble click the workflows downloaded. Some other tricks on Alfred Use it as a calculator. e.g. 3*2 advance calculator could be run started with =. e.g. “=log(3)” or “=sin(1)” Use “&gt; {command}” to run command in terminal. You could also setup which terminal to use {Terminal/iTerms} I will update how many time I have been saved using Alfred later :)","categories":[{"name":"Engineering","slug":"Engineering","permalink":"http://fwz.github.io/categories/Engineering/"},{"name":"Productivity","slug":"Engineering/Productivity","permalink":"http://fwz.github.io/categories/Engineering/Productivity/"}],"tags":[{"name":"MAC","slug":"MAC","permalink":"http://fwz.github.io/tags/MAC/"},{"name":"Alfred","slug":"Alfred","permalink":"http://fwz.github.io/tags/Alfred/"}]},{"title":"Apache Pig in Practice 1","slug":"Apache-Pig-in-Practice-1","date":"2014-07-03T17:27:43.000Z","updated":"2019-05-02T17:20:41.741Z","comments":true,"path":"2014/07/04/Apache-Pig-in-Practice-1/","link":"","permalink":"http://fwz.github.io/2014/07/04/Apache-Pig-in-Practice-1/","excerpt":"I write many pig script in the past few months and have explored some tricks with my buddies. hopes it could help someone. Let’s focus on some interesting topics in this first article and get prepared for the later Pig rush. IDE &amp; EnvironmentVimI use Vim to write most script language and those are my favourite plugins to write Pig: Pig Syntax Highlight. Latest update on Jun 2014, Pig 0.12 supported. You complete me. Best auto-complete plugins ever. If you don’t use a MAC, Supertab is also a reasonable choice. Tabularize Align and keep cleaness of the Pig codelet. Most common usage is :Tab/AS to align FOREACH ... GENERATE clause. To improve debug efficiency, I like to run pig with short cut. Here are my simple approach: add the following in .vimrc for quick run with F51234567891011121314map &lt;F5&gt; :call Compile_Run()&lt;CR&gt;function Compile_Run() if &amp;filetype==&quot;coffee&quot; :w !coffee % 2&gt;&amp;1 elseif &amp;filetype==&quot;cpp&quot; :w !g++ -g -o %&lt; %; ./%&lt; elseif &amp;filetype==&quot;python&quot; :w !python % elseif &amp;filetype==&quot;pig&quot; :w !./run_pig.sh % revise run_pig.sh as you like. General idea is reduce redundant work and typo.","text":"I write many pig script in the past few months and have explored some tricks with my buddies. hopes it could help someone. Let’s focus on some interesting topics in this first article and get prepared for the later Pig rush. IDE &amp; EnvironmentVimI use Vim to write most script language and those are my favourite plugins to write Pig: Pig Syntax Highlight. Latest update on Jun 2014, Pig 0.12 supported. You complete me. Best auto-complete plugins ever. If you don’t use a MAC, Supertab is also a reasonable choice. Tabularize Align and keep cleaness of the Pig codelet. Most common usage is :Tab/AS to align FOREACH ... GENERATE clause. To improve debug efficiency, I like to run pig with short cut. Here are my simple approach: add the following in .vimrc for quick run with F51234567891011121314map &lt;F5&gt; :call Compile_Run()&lt;CR&gt;function Compile_Run() if &amp;filetype==&quot;coffee&quot; :w !coffee % 2&gt;&amp;1 elseif &amp;filetype==&quot;cpp&quot; :w !g++ -g -o %&lt; %; ./%&lt; elseif &amp;filetype==&quot;python&quot; :w !python % elseif &amp;filetype==&quot;pig&quot; :w !./run_pig.sh % revise run_pig.sh as you like. General idea is reduce redundant work and typo. Basic form would be:1pig -x local $1 or with default local debug settings1234intput=./input.txtoutput=./outputrm -rf $&#123;output?&#125;pig -x local -Dinput=$&#123;input&#125; -Doutput=$&#123;output&#125; $1 Modulize your Pig code using MarcoUnder standing MarcoWe have mix feelings with Marco, still I love it better. Marco could help organize and reuse your code. Marco in Pig is quite like Marco in C – they do substitution. Think about you want to do the same series of operation with 3 dataset… Try refactor it into a marco, you will absolutely thank your mercy later. Understanding the $ sign is important when using Marco. $ decorate those variable to be replaced number1.txt1234512345 filter.marco123DEFINE filter_small_number (events, threshold) RETURNS filtered_events &#123; $filtered_events = FILTER $events BY a &gt; $threshold;&#125;; 1234567IMPORT &apos;filter.marco&apos;;events = LOAD &apos;./data/number1.txt&apos; AS (a:int);big_number = filter_small_number(events, 2);DUMP big_number; 123(3)(4)(5) Easy. However, you’d better not change input with in a Marco. they are just substitution, every change in input variables are global 12345678IMPORT &apos;filter.marco&apos;;events = LOAD &apos;./data/number1.txt&apos; AS (a:int);big_number = filter_small_number(events, 2);DUMP big_number;DUMP events; 1234DEFINE filter_small_number (events, threshold) RETURNS filtered_events &#123; $filtered_events = FILTER $events BY a &gt; $threshold; $events = FILTER $events BY a == 4;&#125;; 1234567big_number(3)(4)(5)events(4) You can also return multiple data set in Marco123456789101112DEFINE split_events (events, threshold) RETURNS big, small &#123; $big = FILTER $events BY a &gt;= $threshold; $small = FILTER $events BY a &lt; $threshold;&#125;;events = LOAD &apos;./data/number1.txt&apos; AS (a:int);big_num, small_num = split_events(events, 3);DUMP big_num;DUMP small_num; 1234567big(3)(4)(5)small(1)(2) What’s not so coolOne reason we love Marco less is that after marco is plugined into Pig then error message become a little difficult to read and resolve root cause, because line number would be reflecting the reassembled Pig scripts. However, it’s still a great tool and a must have skill to use Pig. INPUT and OUTPUTINPUT You can almost load everything, HDFS / Avro / Protobuf / Hive / Elastic Search / MongoDB. OUTPUT Of course, there are Storage Function in Pair for above persistency / serialization tools. MultiStorage could help you store data hierarchily, which mean you could partition result when storing, absolutly must-know features. Third party Pig library piggybank DataFu from LinkedIn ElephantBird from twitter Hcatalog UDFOnce you know you could use Python/Ruby/JS to write UDF, I suppose nobody will try to use JAVA for common cases.Python UDF Unit testPigUnitWrite UT to be a good man. Of course, Pig could and should be unit-tested. The PigUnit backbone are supported in Java. However docs are limited and you might run into many troubles. Unit test a python UDFwhen using native unittest packages to test the python script，outputSchema will complains. One way is to add Pig support in Python script, the other one is to disable the outputSchema notation. Here we should the second tricks, put this codelet at the top of the UDF. 1234567if __name__ != &apos;__lib__&apos;: def outputSchema(dont_care): def wrapper(func): def inner(*args, **kwargs): return func(*args, **kwargs) return inner return wrapper This block is intended to test the UDF with the outputSchema notation. The __name__ will be marked as ‘lib’ when script is call by Pig. So it will not take effect when the script is running as Pig UDF. ReferencesComparing Pig Latin and SQL for Constructing Data Processing Pipelines By Alan Gates, Pig Architect in Yahoo.Programming Pig also by Alan Gates.Pig Design Pattern","categories":[{"name":"Engineering","slug":"Engineering","permalink":"http://fwz.github.io/categories/Engineering/"},{"name":"Big Data","slug":"Engineering/Big-Data","permalink":"http://fwz.github.io/categories/Engineering/Big-Data/"}],"tags":[{"name":"Pig","slug":"Pig","permalink":"http://fwz.github.io/tags/Pig/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://fwz.github.io/tags/Hadoop/"}]},{"title":"如何准备技术演讲","slug":"如何准备技术演讲","date":"2014-06-11T15:58:03.000Z","updated":"2019-05-02T17:40:28.049Z","comments":true,"path":"2014/06/11/如何准备技术演讲/","link":"","permalink":"http://fwz.github.io/2014/06/11/如何准备技术演讲/","excerpt":"难得在知乎上认真回答问题，也摘到这里来。原问题：如何做一个优秀的技术分享？ 做一次成功的presentation性价比是很高的，请把握好每一次。 内容的准备 多讲目标听众不知道／感兴趣的内容。听众投入最宝贵的时间来听分享，Speaker的一个起码责任是保证听众在这段时间的收获。 设计一个有创意的开头。提升自信良方。好的开头会影响观众对speaker的第一印象，也使听众更容易包容后来可能出现的失误。 设计一个强而有力的结尾。想一句推而广之有哲理的话，再来强而有力的”Thank you!”，告诉大家是时候鼓掌了。结尾推而广之的例子可见 一个关于数学归纳法的悖论问题：到底是第N天有N个红眼睛自杀，还是什么都不会发生？ 的最高票答案补充，我恨不得点3个赞。 避免常见错误或者明显缺陷。可以参考http://www.slideshare.net/heypig/caffeinenicotine-ppt 蔡老师就如何组织内容说得很好。 平日的大量积累。没有切身体验的死记硬背和摸爬滚打后的娓娓道来，听众的收获和感觉是大不一样的。在接受一次分享邀请之前，最好先掂量一下自己对这个话题的理解。 幻灯片(Slides): 有时间的话尝试用新软件去设计幻灯片。我用过的有impress.js, Prezi, Slidify。Prezi上手更容易一些，但是收费。Impress.js用HTML5实现了Prezi的大部分功能。Slidify比较适合数据分析师使用。花点时间稍加运用，绝对会让人耳目一新。 所有元素风格一致。包括但不限于字体，色调，标点，对齐方式，动画出现退出的方式。 用无衬线字体。中文字体里面比较保险的是微软雅黑或者苹果丽黑。宋体只会让人质疑你的品味。英文字体没什么研究，可以根据自己审美选择了，当然也有保险的Helvetica，或者Impact。","text":"难得在知乎上认真回答问题，也摘到这里来。原问题：如何做一个优秀的技术分享？ 做一次成功的presentation性价比是很高的，请把握好每一次。 内容的准备 多讲目标听众不知道／感兴趣的内容。听众投入最宝贵的时间来听分享，Speaker的一个起码责任是保证听众在这段时间的收获。 设计一个有创意的开头。提升自信良方。好的开头会影响观众对speaker的第一印象，也使听众更容易包容后来可能出现的失误。 设计一个强而有力的结尾。想一句推而广之有哲理的话，再来强而有力的”Thank you!”，告诉大家是时候鼓掌了。结尾推而广之的例子可见 一个关于数学归纳法的悖论问题：到底是第N天有N个红眼睛自杀，还是什么都不会发生？ 的最高票答案补充，我恨不得点3个赞。 避免常见错误或者明显缺陷。可以参考http://www.slideshare.net/heypig/caffeinenicotine-ppt 蔡老师就如何组织内容说得很好。 平日的大量积累。没有切身体验的死记硬背和摸爬滚打后的娓娓道来，听众的收获和感觉是大不一样的。在接受一次分享邀请之前，最好先掂量一下自己对这个话题的理解。 幻灯片(Slides): 有时间的话尝试用新软件去设计幻灯片。我用过的有impress.js, Prezi, Slidify。Prezi上手更容易一些，但是收费。Impress.js用HTML5实现了Prezi的大部分功能。Slidify比较适合数据分析师使用。花点时间稍加运用，绝对会让人耳目一新。 所有元素风格一致。包括但不限于字体，色调，标点，对齐方式，动画出现退出的方式。 用无衬线字体。中文字体里面比较保险的是微软雅黑或者苹果丽黑。宋体只会让人质疑你的品味。英文字体没什么研究，可以根据自己审美选择了，当然也有保险的Helvetica，或者Impact。 一图胜千文。比较推荐利用信息图/数据可视化作品。虽然信息图已经烂大街了，但还是人民群众喜闻乐见的元素。平时没有刻意搜罗的话，可以直接Google搜”Visualization (topic)” 或者”Infographics (topic)”，一般都能从图研所啊flowingdata啊visual-athestics之类的网站翻出来一些。ps－最好用英文搜。 排练 排练到吐。保证内容烂熟于心，各种笑话和梗会被适时、自然放出。 录制排练视频。镜头下所有的弱点都会被暴露，这个阶段超级打击自信。要改进的时候多看看TED talk。 请有经验的朋友对幻灯片内容和表达给意见。他们能反映听众的真实感受。 善用软件的排练功能。PPT的Present view能看到下一屏的内容，可以让你更好地连接上下文。 计时。保证对时间的分配有清晰的掌握。避免前面的slides讲得细致入微，后面压根讲不完。 表达 永远站着做演讲。听众大多是坐着的，站起来有利于建立自信，营造一种对场面的无形控制力。 不要跳页。跳页会让人感到准备不足，或者你并不熟悉此页上的内容。即使时间有限，也要用一句话概括。 事先说明你是否乐意被观众打断。被一根筋的观众缠上是很麻烦的事情，我见过有speaker直接讲着讲着就开吵的。 适当自嘲。有点幽默感大家都会很喜欢的。 细节 控制时间。假如不只有你一个人演讲的话，时间太长可能会影响其它主讲的时间。 事先检查好设备。有没有激光笔，用MAC的话主办方有没有配转接头，甚至预估去会场的时间等等。这样做的目的是保证实际演讲环境与排练环境一致，保证排练的意义。 适当与观众交流。问问题送礼物是激起活跃度的好花招。（建议给些高雅的礼物，例如书） 避免说一句话的时候头重脚轻。据我观察很多人一句话说到后面就越说越小声，是没有自信的表现。 注意仪态。例如过程中尽量避免经常搓手，托眼镜等小动作。有一次我讲完感觉相当好，最后一看回放视频，瓦擦驼背啊……沮丧了一天。 多参加一些演讲活动。Toastmaster是个不错的选择，还能交到很多朋友。 欢迎邀请我去演讲或听演讲。","categories":[{"name":"Career","slug":"Career","permalink":"http://fwz.github.io/categories/Career/"},{"name":"Public Speaking","slug":"Career/Public-Speaking","permalink":"http://fwz.github.io/categories/Career/Public-Speaking/"}],"tags":[{"name":"Public Speaking","slug":"Public-Speaking","permalink":"http://fwz.github.io/tags/Public-Speaking/"},{"name":"Zhihu","slug":"Zhihu","permalink":"http://fwz.github.io/tags/Zhihu/"}]},{"title":"Agile Data Science 译者序","slug":"agile_data_science_preface","date":"2014-05-31T15:36:19.000Z","updated":"2019-05-02T17:46:05.102Z","comments":true,"path":"2014/05/31/agile_data_science_preface/","link":"","permalink":"http://fwz.github.io/2014/05/31/agile_data_science_preface/","excerpt":"大数据时代到来。或者至少可以说，它在概念层面上到来了。然而读者朋友，有没有想过数据的核心价值是什么？ 是信息量。数据携带的信息量赋予了人们进一步洞察这个复杂世界的可能性。数据是现实世界的快照。通过分析数据，我们可以对世界有更深入，更准确的理解。若我们能越快、越好地理解这个世界，就越有可能在行动中占得先机。 因此各行各业，都开始讲究数据驱动决策。为了更好地利用数据带来的价值，我们开始构建数据分析应用程序，通过搜集、清洗、聚合、存储、分析、学习数据，挖掘出数据的内在价值，以指导行动。然而构建数据分析应用程序是一项艰苦的工程——构建过程中，用户需求在变化，系统负载在增大，数据质量经常难以保证，改动不断甚至困难重重。究竟如何用有限的资源构建出一条真正带来价值的数据流水线呢？ 现在你拿在手里的，就是作者给出的答案–一份帮助读者高效构建数据分析产品，以更快更好地洞察这个复杂世界的实践指南。","text":"大数据时代到来。或者至少可以说，它在概念层面上到来了。然而读者朋友，有没有想过数据的核心价值是什么？ 是信息量。数据携带的信息量赋予了人们进一步洞察这个复杂世界的可能性。数据是现实世界的快照。通过分析数据，我们可以对世界有更深入，更准确的理解。若我们能越快、越好地理解这个世界，就越有可能在行动中占得先机。 因此各行各业，都开始讲究数据驱动决策。为了更好地利用数据带来的价值，我们开始构建数据分析应用程序，通过搜集、清洗、聚合、存储、分析、学习数据，挖掘出数据的内在价值，以指导行动。然而构建数据分析应用程序是一项艰苦的工程——构建过程中，用户需求在变化，系统负载在增大，数据质量经常难以保证，改动不断甚至困难重重。究竟如何用有限的资源构建出一条真正带来价值的数据流水线呢？ 现在你拿在手里的，就是作者给出的答案–一份帮助读者高效构建数据分析产品，以更快更好地洞察这个复杂世界的实践指南。作者结合自身数据产品构建经验和大量的例子，介绍如何用现代化的工具和平台，如Pig，MongoDB，Python，Elastic Search，D3.js，AWS等等，构建一个完整、可扩展的数据分析应用。读过本书，读者朋友就大概可以知道在构建数据分析应用时可能遇到的问题，遇到某些问题时可以有哪些选择，设计中有哪些陷阱与反模式，以及如何利用开源项目组合出简洁优雅的解决方案。 本书涉及到构建数据分析产品的方方面面，因此： 架构师，可以参考书中介绍的技术，改进系统设计和新增特性； 数据科学家，可以掌握更多的数据操作工具来处理和展示数据； 项目经理，可以认识到该如何构建团队、分解任务，如何使项目开发流程变得更加敏捷，如何向客户和团队设立合理的预期； 有志于向全栈方向发展的工程师或者学生，也能够进一步开阔眼界，了解自己熟悉的领域之外的生态系统。 2013年秋天我与本书不期而遇，刚好对书中内容有所涉猎，一时技痒，争取到了与阿里的晓风老师一起翻译的机会。过程中晓风老师悉心指导，初稿完成后又字斟句酌地对译文进行了极为细致的审校和润色，让我受益匪浅。还有张春雨编辑在译作过程中为我们铺桥搭路，之后不断给予我们专业的修改意见，在此一并致以衷心的感谢。 当然，由于译者本身的阅历和水平所限，本书的翻译难免存在疏漏和错误，还请读者朋友不吝指正。 最后，感谢你对本书的兴趣，相信你一定会有所收获。 文中2014年北京 搁笔之际又想起了一个程序员自嘲的段子——“我不生产代码，我只是Github的搬运工”。大致的意思是现今的程序员，仅仅是利用Github上的开源代码，就可以完成很多任务。读罢本书，感受更深。找到正确的方法和合适的工具，将生产效率提升十倍甚至百倍，并非天方夜谭。","categories":[{"name":"Publications","slug":"Publications","permalink":"http://fwz.github.io/categories/Publications/"}],"tags":[{"name":"data science","slug":"data-science","permalink":"http://fwz.github.io/tags/data-science/"},{"name":"translate","slug":"translate","permalink":"http://fwz.github.io/tags/translate/"}]},{"title":"Learn d3 the Hard Way (3)-- Force","slug":"learn-d3-the-hard-way-(3)---force","date":"2013-09-20T13:05:05.000Z","updated":"2019-05-02T17:41:39.134Z","comments":true,"path":"2013/09/20/learn-d3-the-hard-way-(3)---force/","link":"","permalink":"http://fwz.github.io/2013/09/20/learn-d3-the-hard-way-(3)---force/","excerpt":"This is a very useful sketch for node placement. Rather than the original force only move nodes, this sketch also place label in it. Origin from Mortiz’s Force-based label placement. The basic idea is to have labels orbit around their target node at a fixed distance, but repeal each other, so that they don’t overlap, and orient themselves to the outside of clusters. To support that, labels on the right of their target node are left-aligned, and labels on the left of their target node are right-aligned; in between, we interpolate. In this example, one force layout governs the node placement, and the second one the label placement, but of course, the node placement could be computed by any other algorithm.","text":"This is a very useful sketch for node placement. Rather than the original force only move nodes, this sketch also place label in it. Origin from Mortiz’s Force-based label placement. The basic idea is to have labels orbit around their target node at a fixed distance, but repeal each other, so that they don’t overlap, and orient themselves to the outside of clusters. To support that, labels on the right of their target node are left-aligned, and labels on the left of their target node are right-aligned; in between, we interpolate. In this example, one force layout governs the node placement, and the second one the label placement, but of course, the node placement could be computed by any other algorithm. Sketch Force based label placement #svg_Force_based_label_placement{ margin : 5px auto; display : block; var w = 700, h = 450; var labelDistance = 0; var vis = d3.select(\"#div_Force_based_label_placement\").append(\"svg:svg\").attr(\"width\", w).attr(\"height\", h).attr(\"id\", \"svg_Force_based_label_placement\"); var nodes = []; var labelAnchors = []; var labelAnchorLinks = []; var links = []; for(var i = 0; i < 60; i++) { var node = { label : \"node \" + i }; nodes.push(node); labelAnchors.push({ node : node }); labelAnchors.push({ node : node }); }; for(var i = 0; i < nodes.length; i++) { for(var j = 0; j < i; j++) { if(Math.random() > .95) links.push({ source : i, target : j, weight : Math.random() }); } labelAnchorLinks.push({ source : i * 2, target : i * 2 + 1, weight : 1 }); }; var force = d3.layout.force().size([w, h]).nodes(nodes).links(links).gravity(1.5).linkDistance(20).charge(-1500).linkStrength(function(x) { return x.weight * 5 }); force.start(); var force2 = d3.layout.force().nodes(labelAnchors).links(labelAnchorLinks).gravity(0).linkDistance(0).linkStrength(8).charge(-100).size([w, h]); force2.start(); var link = vis.selectAll(\"line.link\").data(links).enter().append(\"svg:line\").attr(\"class\", \"link\").style(\"stroke\", \"#CCC\"); var color = d3.scale.category10(); var node = vis.selectAll(\"g.node\").data(force.nodes()).enter().append(\"svg:g\").attr(\"class\", \"node\"); node.append(\"svg:circle\").attr(\"r\", 5).style(\"fill\", function(d, i) { return color(i % 5); }).style(\"stroke\", \"#FFF\").style(\"stroke-width\", 3); node.call(force.drag); var anchorLink = vis.selectAll(\"line.anchorLink\").data(labelAnchorLinks)//.enter().append(\"svg:line\").attr(\"class\", \"anchorLink\").style(\"stroke\", \"#999\"); var anchorNode = vis.selectAll(\"g.anchorNode\").data(force2.nodes()).enter().append(\"svg:g\").attr(\"class\", \"anchorNode\"); anchorNode.append(\"svg:circle\").attr(\"r\", 0).style(\"fill\", \"#FFF\"); anchorNode.append(\"svg:text\").text(function(d, i) { return i % 2 == 0 ? \"\" : d.node.label }).style(\"fill\", \"#555\").style(\"font-family\", \"Arial\").style(\"font-size\", 12); var updateLink = function() { this.attr(\"x1\", function(d) { return d.source.x; }).attr(\"y1\", function(d) { return d.source.y; }).attr(\"x2\", function(d) { return d.target.x; }).attr(\"y2\", function(d) { return d.target.y; }); } var updateNode = function() { this.attr(\"transform\", function(d) { return \"translate(\" + d.x + \",\" + d.y + \")\"; }); } force.on(\"tick\", function() { force2.start(); node.call(updateNode); anchorNode.each(function(d, i) { if(i % 2 == 0) { d.x = d.node.x; d.y = d.node.y; } else { var b = this.childNodes[1].getBBox(); var diffX = d.x - d.node.x; var diffY = d.y - d.node.y; var dist = Math.sqrt(diffX * diffX + diffY * diffY); var shiftX = b.width * (diffX - dist) / (dist * 2); shiftX = Math.max(-b.width, Math.min(0, shiftX)); var shiftY = 5; this.childNodes[1].setAttribute(\"transform\", \"translate(\" + shiftX + \",\" + shiftY + \")\"); } }); anchorNode.call(updateNode); link.call(updateLink); anchorLink.call(updateLink); }); Codes12345678910111213141516171819202122232425262728293031323334353637383940var w = 700, h = 450;var labelDistance = 0;var vis = d3.select(&quot;#div_Force_based_label_placement&quot;).append(&quot;svg:svg&quot;).attr(&quot;width&quot;, w).attr(&quot;height&quot;, h).attr(&quot;id&quot;, &quot;svg_Force_based_label_placement&quot;);var nodes = [];var labelAnchors = [];var labelAnchorLinks = [];var links = [];for(var i = 0; i &lt; 60; i++) &#123; var node = &#123; label : &quot;node &quot; + i &#125;; nodes.push(node); labelAnchors.push(&#123; node : node &#125;); labelAnchors.push(&#123; node : node &#125;);&#125;;for(var i = 0; i &lt; nodes.length; i++) &#123; for(var j = 0; j &lt; i; j++) &#123; if(Math.random() &gt; .95) links.push(&#123; source : i, target : j, weight : Math.random() &#125;); &#125; labelAnchorLinks.push(&#123; source : i * 2, target : i * 2 + 1, weight : 1 &#125;);&#125;; Just like the introduction said, two force is used in this sketch. labelAnchorLinks is used to maintain the links between every node and it’s label. 12345678910111213141516var force = d3.layout.force().size([w, h]).nodes(nodes).links(links).gravity(1.5).linkDistance(20).charge(-1500).linkStrength(function(x) &#123; return x.weight * 5&#125;);force.start();var force2 = d3.layout.force().nodes(labelAnchors).links(labelAnchorLinks).gravity(0).linkDistance(0).linkStrength(8).charge(-100).size([w, h]);force2.start();var link = vis.selectAll(&quot;line.link&quot;).data(links).enter().append(&quot;svg:line&quot;).attr(&quot;class&quot;, &quot;link&quot;).style(&quot;stroke&quot;, &quot;#CCC&quot;);var color = d3.scale.category10();var node = vis.selectAll(&quot;g.node&quot;).data(force.nodes()).enter().append(&quot;svg:g&quot;).attr(&quot;class&quot;, &quot;node&quot;);node.append(&quot;svg:circle&quot;).attr(&quot;r&quot;, 5).style(&quot;fill&quot;, function(d, i) &#123; return color(i % 5); &#125;).style(&quot;stroke&quot;, &quot;#FFF&quot;).style(&quot;stroke-width&quot;, 3);node.call(force.drag); Then run both forces and draw them by adding svn elements.node.call(force.drag) allow interactive dragging. So you can drag one node in the force layout and see how the whole layout react to the drag. Note: the layout would be resumed when mousemove event is triggered, meaning that the friction will be reset, simulation will be reheated. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263var anchorLink = vis.selectAll(&quot;line.anchorLink&quot;).data(labelAnchorLinks)//.enter().append(&quot;svg:line&quot;).attr(&quot;class&quot;, &quot;anchorLink&quot;).style(&quot;stroke&quot;, &quot;#999&quot;);var anchorNode = vis.selectAll(&quot;g.anchorNode&quot;).data(force2.nodes()).enter().append(&quot;svg:g&quot;).attr(&quot;class&quot;, &quot;anchorNode&quot;);anchorNode.append(&quot;svg:circle&quot;).attr(&quot;r&quot;, 0).style(&quot;fill&quot;, &quot;#FFF&quot;);anchorNode.append(&quot;svg:text&quot;).text(function(d, i) &#123; return i % 2 == 0 ? &quot;&quot; : d.node.label&#125;).style(&quot;fill&quot;, &quot;#555&quot;).style(&quot;font-family&quot;, &quot;Arial&quot;).style(&quot;font-size&quot;, 12);var updateLink = function() &#123; this.attr(&quot;x1&quot;, function(d) &#123; return d.source.x; &#125;).attr(&quot;y1&quot;, function(d) &#123; return d.source.y; &#125;).attr(&quot;x2&quot;, function(d) &#123; return d.target.x; &#125;).attr(&quot;y2&quot;, function(d) &#123; return d.target.y; &#125;);&#125;var updateNode = function() &#123; this.attr(&quot;transform&quot;, function(d) &#123; return &quot;translate(&quot; + d.x + &quot;,&quot; + d.y + &quot;)&quot;; &#125;);&#125;force.on(&quot;tick&quot;, function() &#123; force2.start(); node.call(updateNode); anchorNode.each(function(d, i) &#123; if(i % 2 == 0) &#123; d.x = d.node.x; d.y = d.node.y; &#125; else &#123; var b = this.childNodes[1].getBBox(); var diffX = d.x - d.node.x; var diffY = d.y - d.node.y; var dist = Math.sqrt(diffX * diffX + diffY * diffY); var shiftX = b.width * (diffX - dist) / (dist * 2); shiftX = Math.max(-b.width, Math.min(0, shiftX)); var shiftY = 5; this.childNodes[1].setAttribute(&quot;transform&quot;, &quot;translate(&quot; + shiftX + &quot;,&quot; + shiftY + &quot;)&quot;); &#125; &#125;); anchorNode.call(updateNode); link.call(updateLink); anchorLink.call(updateLink);&#125;);","categories":[{"name":"Engineering","slug":"Engineering","permalink":"http://fwz.github.io/categories/Engineering/"},{"name":"Visualization","slug":"Engineering/Visualization","permalink":"http://fwz.github.io/categories/Engineering/Visualization/"}],"tags":[{"name":"Visualization","slug":"Visualization","permalink":"http://fwz.github.io/tags/Visualization/"},{"name":"D3","slug":"D3","permalink":"http://fwz.github.io/tags/D3/"}]},{"title":"Learn d3 the Hard Way (2)--yet another collision detection","slug":"learn-d3-the-hard-way-(2)--yet-another-collision-detection","date":"2013-09-16T05:37:37.000Z","updated":"2019-05-02T17:41:34.046Z","comments":true,"path":"2013/09/16/learn-d3-the-hard-way-(2)--yet-another-collision-detection/","link":"","permalink":"http://fwz.github.io/2013/09/16/learn-d3-the-hard-way-(2)--yet-another-collision-detection/","excerpt":"Another collision detection work. Very elegant. Here’s a little bit background about this sketch. As we can see, this sketch is composed by non-touching circles in a canvas. This algorithm generate a random sample by generating K candidates, and choose the best one and add it to the system. The definition of best is that it’s the farthest away from the previous sample. By this way, the emergance of new sample would be more natural, compared with the uniform random sampling. Some people ask, “is this useful? I can’t see how I can use it into my work.” They are probably wrong, an English artist use exactly the same effect in his work Silent Buddha. It’s a really impressive work. Now take a look from the inside.","text":"Another collision detection work. Very elegant. Here’s a little bit background about this sketch. As we can see, this sketch is composed by non-touching circles in a canvas. This algorithm generate a random sample by generating K candidates, and choose the best one and add it to the system. The definition of best is that it’s the farthest away from the previous sample. By this way, the emergance of new sample would be more natural, compared with the uniform random sampling. Some people ask, “is this useful? I can’t see how I can use it into my work.” They are probably wrong, an English artist use exactly the same effect in his work Silent Buddha. It’s a really impressive work. Now take a look from the inside. Sketch circle { fill: #000; stroke: #fff; stroke-width: 1.5px; } #body_yet_another_collsion { width: 600px; height : 300px; } #svg_yac{ margin : 5px auto; display : block; } var maxRadius = 32, // maximum radius of circle padding = 1, // padding between circles; also minimum radius margin = {top: -maxRadius, right: -maxRadius, bottom: -maxRadius, left: -maxRadius}, width = 600 - margin.left - margin.right, height = 300 - margin.top - margin.bottom; var k = 1 // initial number of candidates to consider per circle m = 10, // initial number of circles to add per frame n = 1000, // remaining number of circles to add newCircle = bestCircleGenerator(maxRadius, padding); var svg = d3.select(\"#body_yet_another_collision\").append(\"svg\") .attr(\"id\", \"svg_yac\") .attr(\"width\", width) .attr(\"height\", height) .append(\"g\"); d3.timer(function() { for (var i = 0; i < m && --n >= 0; ++i) { var circle = newCircle(k); svg.append(\"circle\") .attr(\"cx\", circle[0]) .attr(\"cy\", circle[1]) .attr(\"r\", 0) .transition() .attr(\"r\", circle[2]); // As we add more circles, generate more candidates per circle. // Since this takes more effort, gradually reduce circles per frame. if (k < 500) k *= 1.01, m *= .998; } return !n; }); function bestCircleGenerator(maxRadius, padding) { var quadtree = d3.geom.quadtree().extent([[0, 0], [width, height]])([]), searchRadius = maxRadius * 2, maxRadius2 = maxRadius * maxRadius; return function(k) { var bestX, bestY, bestDistance = 0; for (var i = 0; i < k || bestDistance < padding; ++i) { var x = Math.random() * width, y = Math.random() * height, rx1 = x - searchRadius, rx2 = x + searchRadius, ry1 = y - searchRadius, ry2 = y + searchRadius, minDistance = maxRadius; // minimum distance for this candidate quadtree.visit(function(quad, x1, y1, x2, y2) { if (p = quad.point) { var p, dx = x - p[0], dy = y - p[1], d2 = dx * dx + dy * dy, r2 = p[2] * p[2]; if (d2 < r2) return minDistance = 0, true; // within a circle var d = Math.sqrt(d2) - p[2]; if (d < minDistance) minDistance = d; } return !minDistance || x1 > rx2 || x2 < rx1 || y1 > ry2 || y2 < ry1; // or outside search radius }); if (minDistance > bestDistance) bestX = x, bestY = y, bestDistance = minDistance; } var best = [bestX, bestY, bestDistance - padding]; quadtree.add(best); return best; }; } If you read the previous post about collision detection, you can safely read from setup timer section. If not, and you do not have much experience about d3, you can spend maybe 5 - 10 mins on that post to have a basic sense about the following things. Initialization1234567891011121314151617181920212223242526272829303132333435&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;meta charset=&quot;utf-8&quot;&gt;&lt;style&gt;circle &#123; fill: #000; stroke: #fff; stroke-width: 1.5px;&#125;&lt;/style&gt;&lt;div id=&quot;body_yet_another_collision&quot;&gt; &lt;/div&gt;&lt;script src=&quot;/js/d3.min.js&quot;&gt;&lt;/script&gt;&lt;script&gt;var maxRadius = 32, // maximum radius of circle padding = 1, // padding between circles; also minimum radius margin = &#123;top: -maxRadius, right: -maxRadius, bottom: -maxRadius, left: -maxRadius&#125;, width = 600 - margin.left - margin.right, height = 300 - margin.top - margin.bottom;var k = 1 // initial number of candidates to consider per circle m = 10, // initial number of circles to add per frame n = 2000, // remaining number of circles to add newCircle = bestCircleGenerator(maxRadius, padding);var svg = d3.select(&quot;#body_yet_another_collision&quot;).append(&quot;svg&quot;) .attr(&quot;id&quot;, &quot;svg_yac&quot;) .attr(&quot;width&quot;, width) .attr(&quot;height&quot;, height) .append(&quot;g&quot;) .attr(&quot;transform&quot;, &quot;translate(&quot; + margin.left + &quot;,&quot; + margin.top + &quot;)&quot;); Setup timer123456789101112131415161718d3.timer(function() &#123; for (var i = 0; i &lt; m &amp;&amp; --n &gt;= 0; ++i) &#123; var circle = newCircle(k); svg.append(&quot;circle&quot;) .attr(&quot;cx&quot;, circle[0]) .attr(&quot;cy&quot;, circle[1]) .attr(&quot;r&quot;, 0) .transition() .attr(&quot;r&quot;, circle[2]); // As we add more circles, generate more candidates per circle. // Since this takes more effort, gradually reduce circles per frame. if (k &lt; 500) k *= 1.01, m *= .998; &#125; return !n;&#125;); The timer function is not like the tick function. tick will be trigger forever, but timer will stop. timer receive a function, and then invoking that function until it return true. There are also a new method we have not seen before transition. it animate change smoothly over time rather than applying instantaneously.The r attribute in a circle will be transited from 0 to a given value circle[2]. So here’s what it has done in this fucntion. create a new circle. this circle is selected from k candidates. add it to the svg container do step 1&amp;2 m time for each timer if have add enough circle, then stop the timer. Circle GeneratorAs you might already noticed, the secret lies in the newCircle(k) method. newCircle = bestCircleGenerator(maxRadius, padding); What? Why we need to pass a k?All right, take a close look of the bestCircleGenerator. bestCircleGenerator return an anonymous function, which receive an integer k as number of candidates. And look at the anonymous function, we can see that the function do mainly 3 things. create k candidates and select the best one. add it to the quadtree return the coordinate and the radius Then in the timer, we can add this candidate to svg and present it. 12345678910111213141516171819202122232425262728293031323334353637383940function bestCircleGenerator(maxRadius, padding) &#123; var quadtree = d3.geom.quadtree().extent([[0, 0], [width, height]])([]), searchRadius = maxRadius * 2, maxRadius2 = maxRadius * maxRadius; return function(k) &#123; var bestX, bestY, bestDistance = 0; for (var i = 0; i &lt; k || bestDistance &lt; padding; ++i) &#123; var x = Math.random() * width, y = Math.random() * height, rx1 = x - searchRadius, rx2 = x + searchRadius, ry1 = y - searchRadius, ry2 = y + searchRadius, minDistance = maxRadius; // minimum distance for this candidate quadtree.visit(function(quad, x1, y1, x2, y2) &#123; if (p = quad.point) &#123; var p, dx = x - p[0], dy = y - p[1], d2 = dx * dx + dy * dy, r2 = p[2] * p[2]; if (d2 &lt; r2) return minDistance = 0, true; // within a circle var d = Math.sqrt(d2) - p[2]; if (d &lt; minDistance) minDistance = d; &#125; return !minDistance || x1 &gt; rx2 || x2 &lt; rx1 || y1 &gt; ry2 || y2 &lt; ry1; // or outside search radius &#125;); if (minDistance &gt; bestDistance) bestX = x, bestY = y, bestDistance = minDistance; &#125; var best = [bestX, bestY, bestDistance - padding]; quadtree.add(best); return best; &#125;;&#125; When firstly bestCircleGenerator is called, an empty Quadtree is generated, which will cover the whole svg container via the extent method. And it also setup an searching range. Again, it need some effort to understand the quadtree.visit. Here, it will visit nodes in this quadtree.But because quadtree have some feature that we can reduce the depth of visit, so we need to tell the quadtree whether we want to continue our visit to deeper node. the anonymous fucntion here, return a boolean value indicate whether we need to continue our visit. If function return true, then the children of current node won’t be visited. So what have been done inside this function is: get the position and radius of current node calculate the whether current node is apart from candidate node If not, return true and stop visiting this node If so, update the minimum distance. if the distance is zero and current node is outside of the search radius, also return true and stop visiting this node. check whether the new distance is the “best” distance If so, update the “best” distance And finally, if a new circle is founded, add it to the quadtree, return the position and radius. So in the d3.timer() method, it can be drawn on the svg container. That’s all for this simple and pretty sketch.","categories":[{"name":"Engineering","slug":"Engineering","permalink":"http://fwz.github.io/categories/Engineering/"},{"name":"Visualization","slug":"Engineering/Visualization","permalink":"http://fwz.github.io/categories/Engineering/Visualization/"}],"tags":[{"name":"Visualization","slug":"Visualization","permalink":"http://fwz.github.io/tags/Visualization/"},{"name":"D3","slug":"D3","permalink":"http://fwz.github.io/tags/D3/"}]},{"title":"Learn d3 the Hard Way (1) -- collision detection","slug":"learn-d3-the-hard-way-(1)----collision-detection","date":"2013-09-15T15:36:19.000Z","updated":"2019-05-02T17:41:27.190Z","comments":true,"path":"2013/09/15/learn-d3-the-hard-way-(1)----collision-detection/","link":"","permalink":"http://fwz.github.io/2013/09/15/learn-d3-the-hard-way-(1)----collision-detection/","excerpt":"Finally, make up my mind to learn d3 with focus. I decide to learn the example on d3’s gallery line-by-line. This should not be a hard way as the title of this post because: Most example written by author. the API doc is perfect. Yes, perfect. You can choose the your favourite effect to dive in, never lose interest. Javascript BasicIf you do not have any javascript experience, try to take some. As I always recommend, the javascript course in Codeschool. If you have time, take the Try jQuery course also. Also, we might want to try write some js but I hate write in a file and refresh the browser to see whether I make it correctly. If it’s the case to you, then you can try JSFiddle. you can type js and run to see output all in browser. What’s more, JSFiddle provide most popular js library (such as D3 and Processing.js) so you don’t need to care about this when you are prototyping. OK, we are ready to go! Collision","text":"Finally, make up my mind to learn d3 with focus. I decide to learn the example on d3’s gallery line-by-line. This should not be a hard way as the title of this post because: Most example written by author. the API doc is perfect. Yes, perfect. You can choose the your favourite effect to dive in, never lose interest. Javascript BasicIf you do not have any javascript experience, try to take some. As I always recommend, the javascript course in Codeschool. If you have time, take the Try jQuery course also. Also, we might want to try write some js but I hate write in a file and refresh the browser to see whether I make it correctly. If it’s the case to you, then you can try JSFiddle. you can type js and run to see output all in browser. What’s more, JSFiddle provide most popular js library (such as D3 and Processing.js) so you don’t need to care about this when you are prototyping. OK, we are ready to go! Collision Sketch circle { stroke: #000; stroke-opacity: .5; } var w = 600, h = 400; var nodes = d3.range(200).map(function() { return {radius: Math.random() * 12 + 4}; }), color = d3.scale.category10(); var force = d3.layout.force() .gravity(0.05) .charge(function(d, i) { return i ? 0 : -2000; }) .nodes(nodes) .size([w, h]); var root = nodes[0]; root.radius = 0; root.fixed = true; force.start(); var svg = d3.select(\"#body\").append(\"svg:svg\") .attr(\"width\", w) .attr(\"height\", h); svg.selectAll(\"circle\") .data(nodes.slice(1)) .enter().append(\"svg:circle\") .attr(\"r\", function(d) { return d.radius - 2; }) .style(\"fill\", function(d, i) { return color(i % 5); }); force.on(\"tick\", function(e) { var q = d3.geom.quadtree(nodes), i = 0, n = nodes.length; while (++i < n) { q.visit(collide(nodes[i])); } svg.selectAll(\"circle\") .attr(\"cx\", function(d) { return d.x; }) .attr(\"cy\", function(d) { return d.y; }); }); svg.on(\"mousemove\", function() { // var p1 = d3.svg.mouse(this); var p1 = d3.mouse(this); root.px = p1[0]; root.py = p1[1]; force.resume(); }); function collide(node) { var r = node.radius + 16, nx1 = node.x - r, nx2 = node.x + r, ny1 = node.y - r, ny2 = node.y + r; return function(quad, x1, y1, x2, y2) { if (quad.point && (quad.point !== node)) { var x = node.x - quad.point.x, y = node.y - quad.point.y, l = Math.sqrt(x * x + y * y), r = node.radius + quad.point.radius; if (l < r) { l = (l - r) / l * .5; node.x -= x *= l; node.y -= y *= l; quad.point.x += x; quad.point.y += y; } } return x1 > nx2 || x2 < nx1 || y1 > ny2 || y2 < ny1; }; } CodeLayout and Create data12var w = 600, h = 400; Define the width and height of the layout, much similar to Processing. 12var nodes = d3.range(200).map(function() &#123; return &#123;radius: Math.random() * 12 + 4&#125;; &#125;), color = d3.scale.category10(); range() is just like it in python, so d3.range(200) would create an array with 200 members.array.map() would receive a function, which here is an anonymous function, will return a dict, which only have one key “radius” in it (which indicate the size of the node), and the value is a randomized number. d3.scale.category10() is going to create a color list. So we can think of the above code do some initialization for the viz. Setup the force system123456var force = d3.layout.force() .gravity(0.05) .charge(function(d, i) &#123; return i ? 0 : -2000; &#125;) .nodes(nodes) .size([w, h]); d3.layout.force() is a physical system, which we can manipulate object inside it by defining physical principle. gravity() may be misleading, but what this method do is to setup a constrain to ensure that no nodes will escape from the system. charge() I don’t know what’s that, at first I suppose it’s liek LinkStrength, but not. So the previous code setup an new force layout. 123456var root = nodes[0];root.radius = 0;root.fixed = true;force.start(); Here, select a fix node and start to run the layout (This node is used for user interaction, which controlled by user mouse, we will see it later). After the layout run, then the system manipulate the object itself. Create an SVG container to present the graph with data123456789var svg = d3.select(&quot;#body&quot;).append(&quot;svg:svg&quot;) .attr(&quot;width&quot;, w) .attr(&quot;height&quot;, h);svg.selectAll(&quot;circle&quot;) .data(nodes.slice(1)) .enter().append(&quot;svg:circle&quot;) .attr(&quot;r&quot;, function(d) &#123; return d.radius - 2; &#125;) .style(&quot;fill&quot;, function(d, i) &#123; return color(i % 3); &#125;); now we start to modify the DOM. Just like jQuery, we select the &lt;body&gt; node and append an &lt;svg&gt; node in it, and assign height and width attributes. And then, it use the “circle” selector to select all element whose class == “circle”, which here is an empty selection. Then the selection.data() method “Joins the specified array of data with the current selection.” nodes.slice(1) will remove the first node from nodes (which is selected as root) the enter() function is the one worth understand when learning d3 and its philosophy. It returns the entering selection: placeholder nodes for each data element for which no corresponding existing DOM element was found in the current selection. In our case, all data is new and they do not in the DOM yet, so we create placeholder nodes for all nodes except the root node. Note that the enter operator merely returns a reference to the entering selection, and it is up to you to add the new nodes. So the append() create a “circle” element in the SVG namespace. Then for each element, we assign it a “r” attribute via a anonymous function, indicating the radius of this node. Remember, d3 use d to indicate the datum. So here 1function(d) &#123; return d.radius - 2; &#125; will assign current circle an r attribute with the radius vaule of current datum. Finally we add some color to this circle by applying attribute using style(). Collision detection12345678910111213force.on(&quot;tick&quot;, function(e) &#123; var q = d3.geom.quadtree(nodes), i = 0, n = nodes.length; while (++i &lt; n) &#123; q.visit(collide(nodes[i])); &#125; svg.selectAll(&quot;circle&quot;) .attr(&quot;cx&quot;, function(d) &#123; return d.x; &#125;) .attr(&quot;cy&quot;, function(d) &#123; return d.y; &#125;);&#125;); Here is an event handler, the function inside it will be triggered when tick event is happened. Firstly, it create a quadtree object, using the coordinate of the nodes. Then the quadtree call the visit() to check whether it have collision. We will check the collide method later, but from the code suggests, it change the (x,y) coordinate so we need to update the cx and cy attribute accordingly. 1234567svg.on(&quot;mousemove&quot;, function() &#123; // var p1 = d3.svg.mouse(this); this line of code will report error var p1 = d3.mouse(this); root.px = p1[0]; root.py = p1[1]; force.resume();&#125;); This code transit the mouse position as the fixed root node. when the mouse moves, the layout redefine the root position and recalculate the layout. Note: if you are using d3 3+, then you might encounter an error thatvar p1 = d3.svg.mouse(this); is reporting error. It is caused by the d3 upgrade (non forward competiable). Check the latest API spec and use d3.mouse(this) instead. 1234567891011121314151617181920212223242526function collide(node) &#123; var r = node.radius + 16, nx1 = node.x - r, nx2 = node.x + r, ny1 = node.y - r, ny2 = node.y + r; return function(quad, x1, y1, x2, y2) &#123; if (quad.point &amp;&amp; (quad.point !== node)) &#123; var x = node.x - quad.point.x, y = node.y - quad.point.y, l = Math.sqrt(x * x + y * y), r = node.radius + quad.point.radius; if (l &lt; r) &#123; l = (l - r) / l * .5; node.x -= x *= l; node.y -= y *= l; quad.point.x += x; quad.point.y += y; &#125; &#125; return x1 &gt; nx2 || x2 &lt; nx1 || y1 &gt; ny2 || y2 &lt; ny1; &#125;;&#125; The previous code calculate a square bounding-box (nx1, nx2, ny1, ny2), which the node can be put inside of it. This function (quad, x1, x2, y1, y2) is required by the quad.visit() function. The function now create compare whether bounding boxes of two nodes overlap by a simple collision detection algorithm. 1234return x1 &gt; nx2 || x2 &lt; nx1 || y1 &gt; ny2 || y2 &lt; ny1; The bounding-box collision detection algorithm. 1234567if (l &lt; r) &#123; l = (l - r) / l * .5; node.x -= x *= l; node.y -= y *= l; quad.point.x += x; quad.point.y += y;&#125; if two nodes are overlaped, adjust them. That’s All!To summarize, this viz use the force layout to move nodes, following some physical principle. Then it build a quadtree at every “tick” event of the layout, and detect collision. When collision is detected, then it will adjust the node immediately. What’s Next?I am going to study this block – Mitchell’s Best-Candidate. Also using the Quadtree to build a fasinating effect with limited code.","categories":[{"name":"Engineering","slug":"Engineering","permalink":"http://fwz.github.io/categories/Engineering/"},{"name":"Visualization","slug":"Engineering/Visualization","permalink":"http://fwz.github.io/categories/Engineering/Visualization/"}],"tags":[{"name":"Visualization","slug":"Visualization","permalink":"http://fwz.github.io/tags/Visualization/"},{"name":"D3","slug":"D3","permalink":"http://fwz.github.io/tags/D3/"}]},{"title":"Visualization Learning Resources","slug":"visualization-resources","date":"2013-09-15T03:24:04.000Z","updated":"2016-09-05T03:39:54.000Z","comments":true,"path":"2013/09/15/visualization-resources/","link":"","permalink":"http://fwz.github.io/2013/09/15/visualization-resources/","excerpt":"Many friends ask me to recommend a list for learning Visualization techniques. As I promised, it’s out now! Best places to learnd3.js“D3” is short for data driven document. With d3, you can manipulate DOM with changing data and user input easily. But if you are not familiar with javascript, the learning curve would be a little steep. Highly recommend examples from Mike, author of d3 And there is a crazy blog post collect 1,000 d3 example. OpenProcessing And Processing.jsProcessing user like to show off their work on OpenProcessing! Because it’s open, you can learn other’s algorithm and find out techniques to create nice effect but using the ordinary function.","text":"Many friends ask me to recommend a list for learning Visualization techniques. As I promised, it’s out now! Best places to learnd3.js“D3” is short for data driven document. With d3, you can manipulate DOM with changing data and user input easily. But if you are not familiar with javascript, the learning curve would be a little steep. Highly recommend examples from Mike, author of d3 And there is a crazy blog post collect 1,000 d3 example. OpenProcessing And Processing.jsProcessing user like to show off their work on OpenProcessing! Because it’s open, you can learn other’s algorithm and find out techniques to create nice effect but using the ordinary function. FlowingDataFlowingData explores how designers, statisticians, and computer scientists are using data to understand ourselves better — mainly through data visualization. I learn a lot from FlowingData. Goto the tutorial and you can learn enough to astonish your colleague and boss! GallerywtfvizWhat the fuck viz? People submit poor visualization there. Visual Complexity Infosthetics BehanceNot always visualization, but full of creatives! Other techniques Code School’s Try jQuery course Code School’s Try CoffeeScript course Books Beautiful Visualization Visualize this Data Points A huge amount of books on Processing.org You know more?Let me know so I can add to this post. Any help would be welcomed!","categories":[{"name":"Engineering","slug":"Engineering","permalink":"http://fwz.github.io/categories/Engineering/"},{"name":"Visualization","slug":"Engineering/Visualization","permalink":"http://fwz.github.io/categories/Engineering/Visualization/"}],"tags":[{"name":"Visualization","slug":"Visualization","permalink":"http://fwz.github.io/tags/Visualization/"}]},{"title":"【桌游】《大逆转》策略分析","slug":"【桌游】《大逆转》策略分析","date":"2013-05-12T03:42:20.000Z","updated":"2019-05-02T17:20:41.811Z","comments":true,"path":"2013/05/12/【桌游】《大逆转》策略分析/","link":"","permalink":"http://fwz.github.io/2013/05/12/【桌游】《大逆转》策略分析/","excerpt":"《大逆转》策略分析 很久前看到过这个桌游的宣传教程，觉得很有意思，于是买回来了和GF玩，玩了大概10局，发现这个游戏的变化真的挺大的。几局下来诞生了三回合速推的一箭穿心打法，从头压制到尾的压迫式打法，另外还有两次激动人心的逆转翻盘节奏。决定写一下心得。 桌游视频教程官方教程 手牌的运营由于手牌一直只能保持4张，所以要留和弃哪些牌，要根据游戏的发展来定。先说说功能牌。 功能牌 禁手：保留优先级很高的手牌，因为功能牌的作用很大，不可思议的逆转经常是通过一张交换牌开始引起的连锁反应。禁手能够打乱对手的部署。","text":"《大逆转》策略分析 很久前看到过这个桌游的宣传教程，觉得很有意思，于是买回来了和GF玩，玩了大概10局，发现这个游戏的变化真的挺大的。几局下来诞生了三回合速推的一箭穿心打法，从头压制到尾的压迫式打法，另外还有两次激动人心的逆转翻盘节奏。决定写一下心得。 桌游视频教程官方教程 手牌的运营由于手牌一直只能保持4张，所以要留和弃哪些牌，要根据游戏的发展来定。先说说功能牌。 功能牌 禁手：保留优先级很高的手牌，因为功能牌的作用很大，不可思议的逆转经常是通过一张交换牌开始引起的连锁反应。禁手能够打乱对手的部署。 炸弹：前期优先级较低的牌，对手还没来得及攻进来。后期快进入对峙阶段的时候就相当有用了，这时空位很少，炸掉自己的一张牌，重新发起进攻，经常可以攻入 对手的腹地，然后对手的格子也快满了，通过数字卡牌很难进行有效反击，这时再想办法进入对峙，便宜很大。 超级加：典型后期牌，配合0来进行进攻。 交换：这张牌应该是整个游戏里面最逆天的一张牌了。棋盘内使用交换一次，就等于有了两张热牌，假如两张热牌都能够发动逆转的话，给对手的压力就会很大了。保留禁手的原因很大程度就是为了避免这种情况的发生。 闪电：初期有多余的数字牌的时候一定要多多使用，否则容易被对手压制。 然后说说数字牌 数字牌里面，本质上1，2，3，4是相同的，只是有时超级加能够通过1-1-3，1-2-2，这种情况启动而已。 0这张牌，我个人不太喜欢。因为很难通过这张牌发动逆转，只能通过超级加。所以0是稳定局势用的牌，主要用在对峙之前压制对方变化布局上的变化，以及对方强势攻击时稳定局面。但0这张牌副作用太大，游戏中鼓励的链式变化也会减少。 棋盘的运营初期 扩张扩张扩张! 放置的原则是使棋盘上有尽可能多的逆转发动空格，我自己的做法主要是保证安全的情况下想办法布一个菱形阵营。争取对方放下的每一张牌我都有可能在自己的回合构成逆转。 中期 巩固自己布局的健壮度。这样可以避免被对手炸断或者逆转枢纽后，前线所有的卡牌由于缺乏支撑而被移出棋盘。 进入对峙 不要轻易放下最后一张牌进入对峙。对峙中的先手优势我认为是挺大的，所以优势一方要确立较大的优势后再进入对峙。 通常这个时候优势方本身的棋盘会有空格，所以可以决定什么时候进入对峙，而劣势方就没有空间放牌，所以很有可能几个回合下来双方都是在不断弃牌。这个时候劣势的一方应该根据布局攒一些炸弹或者交换牌谋求反击，当然禁手还是最重要的。很多时候，优势方发动一个交换再一个超级加，最后填完自己剩下的唯一一个空格进入对峙。这时棋盘上劣势方的生存空间就很小了。 对峙期间 最简单的思路，进入对峙阶段以后，肯定会有某方是比较优势的，主要说说劣势方。* 首先劣势方要确保自己手里的牌是有可能发动逆转的，比如你棋盘上只有0-1-3，那就趁早GG吧，肯定是要挂的。 所以顺着这个思路，你要抢夺/保留劣势方最重要的一张牌，我称之为pivot。 例如有一局我只剩下4张牌，分别是0-2-2-3，那么很明显，我的pivot就是3。一定要保住，否则肯定玩完。 多多设计那些会出现连锁局面的阵型。由于交换两张牌位置的变化实在太多，每次只逆转对方一张牌是远远不够的。要多想想各种可能的组合，","categories":[{"name":"Game","slug":"Game","permalink":"http://fwz.github.io/categories/Game/"},{"name":"Board Game","slug":"Game/Board-Game","permalink":"http://fwz.github.io/categories/Game/Board-Game/"}],"tags":[]},{"title":"时间管理--致系统管理员 读后感","slug":"【读书笔记】时间管理-致系统管理员","date":"2012-12-31T10:38:01.000Z","updated":"2018-09-09T07:48:23.000Z","comments":true,"path":"2012/12/31/【读书笔记】时间管理-致系统管理员/","link":"","permalink":"http://fwz.github.io/2012/12/31/【读书笔记】时间管理-致系统管理员/","excerpt":"我所理解的时间管理可以分为两部分： 规划时间（决定哪段时间做什么） 利用时间（高效地利用当前时间段） 使用日程表和Todo笔记（规划） 每天早上，花10分钟计划自己的一天 晚上离家之前，管理未完成的任务 将非工作的内容也纳入笔记进行管理 假如经常有突然插入的干扰任务，就为他们预分配一些时间 没做完的，一定要移入明天的Todo 大脑只做运算，不做存储。（利用） 充分利用大脑 把需要记住的东西从大脑中挪走，用别的自己信任的系统记下来（Evernote） 选择合适的多任务（规划） 充分利用多任务 要意识到哪些任务是多任务，哪些不是 属于多任务的好工作是：要马上做，但可以等的任务。（下载超大文件，备份，编译等等） 切换任务的时候，花一点时间记录你做到哪里了。这样返回到这个任务比较容易。","text":"我所理解的时间管理可以分为两部分： 规划时间（决定哪段时间做什么） 利用时间（高效地利用当前时间段） 使用日程表和Todo笔记（规划） 每天早上，花10分钟计划自己的一天 晚上离家之前，管理未完成的任务 将非工作的内容也纳入笔记进行管理 假如经常有突然插入的干扰任务，就为他们预分配一些时间 没做完的，一定要移入明天的Todo 大脑只做运算，不做存储。（利用） 充分利用大脑 把需要记住的东西从大脑中挪走，用别的自己信任的系统记下来（Evernote） 选择合适的多任务（规划） 充分利用多任务 要意识到哪些任务是多任务，哪些不是 属于多任务的好工作是：要马上做，但可以等的任务。（下载超大文件，备份，编译等等） 切换任务的时候，花一点时间记录你做到哪里了。这样返回到这个任务比较容易。 规划自己的Routine 设计自己的Routine，让其他人也知道。 形成习惯。（规划+利用） 设定工作优先级（规划） 优先级 设定短期/长期目标，也设定优先级 假如手头的工作都是重要度最高的，就按工作表的次序做起。（与其苦恼该做什么，还不如简化决策，马上开始做） 做事的目标是追求效果。按照客户预期来安排优先级（从短到长）会较好。 如何选择项目的优先级：简单的？有趣的？保险的？根据旧的项目而有明确后续目标的？都不是，应该是对机构的目标有最大正面影响的项目。做重大且有正面影响的大项目 管理上司（规划） 管理上司 确定上司知道我的职业生涯目标，告诉他你的梦想和目标，不要害怕。这样上司会给予feedback，会告诉你要走到这个位置，需要完成哪些成就和经历。 适当地往上委派（upwards delegation) 了解并协助上司完成目标（私下问他们，目标是什么，我可以如何协助他们达成这个目标。如果我知道了，就可以将这些目标记在心里，用更有效的方式对团队做出贡献） 保持专注。（规划+利用） 利用好时间 找到自己最能维持专注的时间段（规划+利用） 办公室里最安静的一个小时是第一个小时 Email 管理（利用）： 管理email 对每封email，只读一次 要回复但无法立即回复，放入当天todo 把别人道谢或称赞的mail放到某个文件夹，用于鼓励自己以及绩效回顾 传递正能量的箴言 迟早要做，晚做不如早做 信任流程 （关于收拾工作环境）当你犹豫的时候，扔掉它 远离游戏和IM 如果是每天都要做的事情，早点完成 良好的应对客户的话术 还有什么要补充的吗 我记下了你的重点了吗 其它忠告 改变东西之前，备份…… 永远要测试自己的工作 你必须经常高估工作时间，这样能比较保险 如果要问，答案永远是“是”的问题： 现在是保存文件的好时机吗？ 我应该带着笔记吗？ 我应该把它加入todo list吗？ 我应该现在就做这些小任务或琐碎的事情吗？ 各种工具： life balance leave 会议组织/主持者的技巧： 总是提前24小时寄出提醒邮件 列出完整的日期、时间、时区 包含指向说明文件的URL 包含议程表 调试音频视频设备，就再提早5分钟或更多 总是准时开始（把最重要的项目摆在最前面，人们会讨厌你，但以后会准时）","categories":[{"name":"Read & Learn","slug":"Read-Learn","permalink":"http://fwz.github.io/categories/Read-Learn/"},{"name":"Book Review","slug":"Read-Learn/Book-Review","permalink":"http://fwz.github.io/categories/Read-Learn/Book-Review/"}],"tags":[{"name":"Time Management","slug":"Time-Management","permalink":"http://fwz.github.io/tags/Time-Management/"}]},{"title":"数据可视化分享——Processing漫游(3)","slug":"数据可视化分享-processing漫游3","date":"2012-11-21T10:02:39.000Z","updated":"2019-05-02T17:35:54.579Z","comments":true,"path":"2012/11/21/数据可视化分享-processing漫游3/","link":"","permalink":"http://fwz.github.io/2012/11/21/数据可视化分享-processing漫游3/","excerpt":"今天突然想写一个小游戏，按捺不住就做了个Processing的版本。 游戏规则：两个或以上相连的同色棋子，点击后可以消去，增加消去棋子个数平方的分数。尽可能获取最高分数。 各路大神来贴分吧!","text":"今天突然想写一个小游戏，按捺不住就做了个Processing的版本。 游戏规则：两个或以上相连的同色棋子，点击后可以消去，增加消去棋子个数平方的分数。尽可能获取最高分数。 各路大神来贴分吧! Sketch // Stone board config int row = 10; int column = 12; int grid_size = 50; int margin = 4; int draw_mode = 1; int color_scheme = 0; // board status Stone[][] stones; // stone info int [] bar_height = new int[column]; // current height of each column boolean[][] toclear; // whether this grid is mark as same color from trigger point. int display_score; int score; int clear_num; // Color and Image color[][] clrs = new color[][] { { #F60018, #FF9C00, #0E53A7, #25D500, #FFFFFF}, //red, orange, blue, green,background, { #6666CC, #6633CC, #990033, #CC3333, #FFFFFF}, //trash,..., }; // Four direction to search int [][] dir = new int[][] { {0, 1}, {0, -1}, {1, 0}, {-1, 0} }; void setup() { size(column*grid_size, row*grid_size); reset(); } void reset() { stones = new Stone[column][row]; //so we can use stone[i][j] for co-ordinate(i,j) toclear = new boolean[column][row]; score = 0; display_score = 0; ellipseMode(CORNER); for (int i=0; i 0) { display_score += 1; } text(str(display_score), width/2, height/2); for (int i=0; i= 0 && yn < row && stones[xn][yn].enable && stones[x][y].clr_index == stones[xn][yn].clr_index) { return true; } } return false; } void merge_column(int empty_column) { for (int i = empty_column; i < column-1; i++ ) { bar_height[i] = bar_height[i+1]; for (int j = 0; j < bar_height[i]; j++) { stones[i][j].y = stones[i+1][j].y; stones[i][j].clr_index = stones[i+1][j].clr_index; stones[i][j].enable = stones[i+1][j].enable; stones[i+1][j].enable = false; } } } void find_neighbor(int x, int y, color clr) { for (int i = 0; i < 4; i++) { int xn = x + dir[i][0]; int yn = y + dir[i][1]; if (xn >= 0 && xn < column && yn >= 0 && yn < row && stones[xn][yn].enable && toclear[xn][yn] == false && stones[xn][yn].clr_index == clr) { clear_num++; toclear[xn][yn] = true; stones[xn][yn].enable = false; find_neighbor(xn, yn, clr); } } } void keyPressed() { if (key == 'r') { reset(); } if (key == 's') { //draw_mode += 1; //draw_mode %= 2; } if (key == 'c') { color_scheme += 1; color_scheme %= clrs.length; } } class Stone { public int x; public int y; public int clr_index; public boolean enable = true; private float grid_size; public Stone(int x, int y, int clr_index, int grid_size) { this.x = x; this.y = y; this.clr_index = clr_index; this.grid_size = grid_size; } public Stone(Stone s) { this.x = s.x; this.y = s.y; this.clr_index = s.clr_index; this.grid_size = s.grid_size; this.enable = s.enable; } public void disable () { enable = false; } public void draw(int draw_mode, int color_scheme) { if (enable) { noStroke(); fill(clrs[color_scheme][clr_index], 200); ellipse(x * grid_size, (row - y - 1) * grid_size, grid_size - margin, grid_size - margin); } } public int calc_down_step () { int down_step = 0; // for stone not being clear // use toclear matrix to calculate how many step go get down; for (int i = 0 ; i < y; i++) { if (toclear[x][i]) { down_step++; } } return down_step; } }","categories":[{"name":"Engineering","slug":"Engineering","permalink":"http://fwz.github.io/categories/Engineering/"},{"name":"Visualization","slug":"Engineering/Visualization","permalink":"http://fwz.github.io/categories/Engineering/Visualization/"}],"tags":[{"name":"Processing","slug":"Processing","permalink":"http://fwz.github.io/tags/Processing/"}]},{"title":"数据可视化分享——Processing漫游(2)","slug":"数据可视化分享-processing漫游2","date":"2012-11-11T10:02:39.000Z","updated":"2019-05-02T17:35:57.884Z","comments":true,"path":"2012/11/11/数据可视化分享-processing漫游2/","link":"","permalink":"http://fwz.github.io/2012/11/11/数据可视化分享-processing漫游2/","excerpt":"今天看到一个非常性感的作品，按捺不住就做了个Processing的版本。所以抱歉了，这个漫游系列硬生生地把它插入。继续阅读查看效果吧，按S保存截图，R清空屏幕重新绘图。","text":"今天看到一个非常性感的作品，按捺不住就做了个Processing的版本。所以抱歉了，这个漫游系列硬生生地把它插入。继续阅读查看效果吧，按S保存截图，R清空屏幕重新绘图。 这个例子来自Patrick Gunderson，看到这个例子的时候，我非常好奇，这究竟是用什么算法实现的？旁边有图形帝说可以用热学传导模型加偏微分方程模拟，我瞬间就给跪了。后来找到了作者的介绍，发现算法简单得让人发指。不敢独美，与大家分享。 Sketch // This is a naive implement of ablaze.js in processing // Thanks Patrick Gunderson's great work // Wenzhong @ Beijing, Oct 2012 // wenzhong.work@gmail.com // build config int PARTICLE_NUM = 60; int MIN_DIST = 70; float MIN_SPEED = 0.25; float MAX_SPEED = 0.75; float RING_SIZE = 150; boolean line_connect = true; // show config boolean use_color_map = false; boolean use_color_gradient = false; float clr; float NOISE_SCALE = 0.02; float frame_cnt = 0; PImage clrmap; Particle [] ps; class Particle { public float x, y, rot_angle; private float vx, vy; private int step; Particle(float x, float y, float v, float v_angle) { this.x = x; this.y = y; this.vx = v*cos(v_angle); this.vy = v*sin(v_angle); generate_new_curve(); } public void next() { turn(rot_angle); x += vx; y += vy; step--; if (step > 16) & 0xFF, (c >> 8) & 0xFF, c & 0xFF, 32 * (1 - dis / MIN_DIST)); stroke((c >> 16) & 0xFF, (c >> 8) & 0xFF, c & 0xFF, 32 * (1 - dis / MIN_DIST)); } buildConnect(ps[i], ps[j], dis); } } ps[i].next(); } } void keyPressed() { switch(key) { case 'g': changeColorGradient(); break; case 't': changeConnectionBuild(); break; case 'r': reset(); break; case 's': saveFrame(); break; case 'c': changeColor(); break; } } void buildConnect(Particle p1, Particle p2, float dis) { if (line_connect) { line(p1.x, p1.y, p2.x, p2.y); } else { float cx = (p1.x + p2.x)/2; float cy = (p1.y + p2.y)/2; noFill(); ellipse(cx, cy, dis/2, dis/2); } } void changeConnectionBuild() { line_connect = ! line_connect; } void changeColor() { use_color_map = ! use_color_map; } void load_color_map() { clrmap = loadImage(\"chocolate.png\"); blurImage(clrmap); } color set_color_by_map(Particle p) { if (clrmap.width > 0) { if (abs(p.x) >= width/2 - 1 || abs(p.y) >= height/2 - 1) return 0; float x = map(p.x, -width/2, width/2, 0, clrmap.width); float y = map(p.y, -height/2, height/2, 0, clrmap.height); if ((int)((y) * clrmap.width) + (int) (x-1) >= clrmap.width * clrmap.height) return 0; color c = clrmap.pixels[(int)((y) * clrmap.width) + (int) (x-1)]; return c; } return 0; } // shamlessly taken from the processing.org's, for blurring image void blurImage(PImage img) { float v = 1.0 / 25.0; float[][] kernel = { { v, v, v, v, v } , { v, v, v, v, v } , { v, v, v, v, v } , { v, v, v, v, v } , { v, v, v, v, v } }; // Loop through every pixel in the image for (int y = 2; y < img.height-2; y++) { // Skip top and bottom edges for (int x = 2; x < img.width-2; x++) { // Skip left and right edges float sum_r = 0, sum_g = 0, sum_b = 0; // Kernel sum for this pixel for (int ky = -2; ky","categories":[{"name":"Engineering","slug":"Engineering","permalink":"http://fwz.github.io/categories/Engineering/"},{"name":"Visualization","slug":"Engineering/Visualization","permalink":"http://fwz.github.io/categories/Engineering/Visualization/"}],"tags":[{"name":"Processing","slug":"Processing","permalink":"http://fwz.github.io/tags/Processing/"}]},{"title":"处女面试小结","slug":"处女面试小结","date":"2012-11-08T17:13:02.000Z","updated":"2016-09-05T03:39:54.000Z","comments":true,"path":"2012/11/09/处女面试小结/","link":"","permalink":"http://fwz.github.io/2012/11/09/处女面试小结/","excerpt":"感谢老大给机会我去体验当面试官，虽然没给我多少心理准备，虽然我当时还在training…… 看了看时间，每人40分钟。趁面试开始前翻了一下feedback的Check list，看看有哪些需要考察的。总归是三大项，专业技能，研究领域，个人性格。于是根据这几项分配时间。目前觉得如下分布比较合理。 开始阶段自我介绍。建议让讲点简历上没有的东西，可以看出应变的能力，能传达出一种淡定干练的感觉就非常好了。1分钟简单的交谈，问一些开放性问题。平时会看什么书？上什么网站啊，等等。（我今天在这样问的时候，HR姐姐过来在我耳边说，我们一二面主要考察代码能力。囧。好吧我争取干到以后能随便问开放性问题）1分钟","text":"感谢老大给机会我去体验当面试官，虽然没给我多少心理准备，虽然我当时还在training…… 看了看时间，每人40分钟。趁面试开始前翻了一下feedback的Check list，看看有哪些需要考察的。总归是三大项，专业技能，研究领域，个人性格。于是根据这几项分配时间。目前觉得如下分布比较合理。 开始阶段自我介绍。建议让讲点简历上没有的东西，可以看出应变的能力，能传达出一种淡定干练的感觉就非常好了。1分钟简单的交谈，问一些开放性问题。平时会看什么书？上什么网站啊，等等。（我今天在这样问的时候，HR姐姐过来在我耳边说，我们一二面主要考察代码能力。囧。好吧我争取干到以后能随便问开放性问题）1分钟 正式面试Linux / shell / Editor usage （看candidate的简历选择，不要超过2个），完成日常工作中常用的系统管理和自动化的操作等等，知道就知道，不知道就问别的。无需展开。2分钟OS / Database （根据简历选，同上，2 分钟） 编程题 / 特定领域知识 （二选一）10 分钟编程题代码实现尽量不要超过30行，15行左右为佳。就不要让写CLRS上的算法啥的了，没意义。主要考察一下简单编程功底，包括 指针的使用 动态分配内存的释放 边界条件的处理 编程风格今天我把这个demo展示出来，提供了某些画图函数，让写了个遍历二叉树 ，应该不难吧…… 特定领域知识（假如你对这个领域有一定了解的话，不妨试试这个）例如，熟悉ML/NLP的，就可以问问如何实现垃圾评论/邮件过滤；熟悉MapReduce的，就设计一个大数据运算用例。可结合系统设计同时询问。在面试者提出系统设计的时候，需要不断找到系统的盲点，让面试者不断完善。这对面试官的要求就比较高了。 算法题15-20 分钟：5-10分钟讨论算法，10分钟写（伪）代码不要难到ACM水平才能解出。提示、帮助面试者得到思路和结果。 防AK最后预留一条数学题避免题目被AK。 结束阶段让candidate问问题， 3-5分钟。传达公司最大的善意以及自己对面试者的建议。 交流阶段不要忽视后续各个面试官之间的交流。1、交流面过的内容（避免知识性的问题重复出现）2、简单交流优缺点（用于后续简单验证）3、经验尚浅就不妨请教一下资深面试官的相关经验 : ) 总结 提前熟悉一下简历还是很重要的，在坐到面试者前面时才拿到简历会很被动（今天我就是这样）。 40分钟时间略少，问不了多少code，能写完两道code的应该都是比较优秀的了。由于考察内容不多，导致最后给HR姐姐反馈的时候不是很确定。 我出的题目都是基本源自各种游戏和来自实际工作，基本不用担心有重题。但目前还需要更广泛的涉猎去面对不同背景的面试者。假如熟悉他们的领域，上下文同步，会比较容易想到题目让面试者有更好的发挥。对于校招，还是要挖掘candidate的闪光点。 预先设计好题目。每个环节准备三个不同难度的题目，这样就可以根据面试者的表现提高或降低难度来不断逼近面试者的真实水平。 利用其它同事的总结。假如同事说某个方面强/弱，而你又觉得这项能力非常critical，简单验证一下。否则就可以选择别的领域继续了。 Don’t Dominant the Interview. 假如面试者最终仍不能给出有效的答案，跳过的时候点一点思考的方向即可，没有必要再详细解释解法（今天我给的答案都巨详细，应该耗费了不少时间）。还是应该留出更多的时间让面试者 Shine。 目前接触到的样本实在太少了，所以真的不知道哪种水平是average，那种水平是excellent。对于某些不是很突出的候选人，会犹豫给不给Hire。我只能根据自己的感觉了——Whether I want to work with him/her, or not. 初期这有可能导致门槛定得过高，不过我相信很快就会调整到一个合适的高度。 面试还是挺好玩的。","categories":[{"name":"Career","slug":"Career","permalink":"http://fwz.github.io/categories/Career/"},{"name":"Interview","slug":"Career/Interview","permalink":"http://fwz.github.io/categories/Career/Interview/"}],"tags":[{"name":"Interview","slug":"Interview","permalink":"http://fwz.github.io/tags/Interview/"}]},{"title":"An interactive tree in Processing","slug":"an-interactive-tree-in-Processing","date":"2012-06-07T05:28:09.000Z","updated":"2016-09-05T03:39:54.000Z","comments":true,"path":"2012/06/07/an-interactive-tree-in-Processing/","link":"","permalink":"http://fwz.github.io/2012/06/07/an-interactive-tree-in-Processing/","excerpt":"这是一份processing写就的树形可视化，大家可以猜猜需要用多少代码。有经验的朋友应该能看出来核心代码不会很多，大概40行。出处在这里。为了能在用js打开以及流畅的用户体验，我修改了里面代码的一些地方。我觉得这份代码有些奇妙的地方，所以学习了一下。","text":"这是一份processing写就的树形可视化，大家可以猜猜需要用多少代码。有经验的朋友应该能看出来核心代码不会很多，大概40行。出处在这里。为了能在用js打开以及流畅的用户体验，我修改了里面代码的一些地方。我觉得这份代码有些奇妙的地方，所以学习了一下。 Sketch Code float rotx = 0; float roty = 0; float desc_factor = sqrt(2)/2.0; float growth = 0; float prop_scale = 0; void setup() { size(600, 450, P2D); } void draw() { background(240); stroke(0); rotx += (radians(360./height*mouseX)-rotx)/10; roty += (radians(360./height*mouseY)-roty)/10; translate( width/2, height/3*2); line(0, 0, 0, width/3); // the trunk branch (height/4.0, 11); if (keyPressed) { if (key == 'j') prop_scale += 0.02; else if (key == 'k') prop_scale -=0.02; } growth += (1 - growth + prop_scale) / 10; } void branch(float len, int num) { len *= desc_factor; num -= 1; if (len >= 2 && num > 0) { pushMatrix(); rotate(rotx); line(0, 0, 0, -len); translate(0, -len); branch(len, num); popMatrix(); len *= growth; pushMatrix(); rotate(rotx - roty); line(0, 0, 0, -len); translate(0, -len); branch( len, num); popMatrix(); } } 简单介绍一下processing程序的工作方式，setup是程序开始之前需要做的事情，一般就是设置一下画布大小，设定画笔颜色等等，和awk的BEGIN块类似。然后processing不断地去运行draw()函数，每个draw()函数运行完毕，就会生成一个帧，而每秒钟跑多少帧取决于frameRate()函数，默认值好像是30。 从代码比较容易能够看到，使用的是DFS写成的递归生成树，随着鼠标的移动，树的形态会不断变化。 那么奇妙的地方在哪里呢？这棵树的交互非常平滑，当我们不再移动鼠标的时候，树枝会平滑地过渡到结果的点，为什么呢？ 首先我们看，这样一棵树，需要哪几个因素就可以确定下来，代码里面又是如何处理的。 两个分支长度的比值，growth变量确定 两个分支之间的夹角，branch里面使用(curlx 和 curly 的值决定） 递归的停止条件（这里是树枝的最短长度以及最大代数），branch定义。然后我们分析一下，最重要的树枝旋转平滑。其实就是缓动两个分支的角度。功能由这个式子完成 rotx += (radians(360./height*mouseX)-rotx)/delay; 我们可以考虑curlx 在draw()中每一个时刻的值所形成的序列是一个数列 {An}。同时，我们认为mouseX（鼠标在画布中的横坐标）在缓动过程中也是不变的，所以radians(360./heightmouseX)这个数可以定为常量C，delay也是常量,记为d,我硬编码为10了。那么我们可以得到An的通项公式：An+1 = An + (C - An) / d利用中学的数列知识可以将上面的式子化为： An+1 - C = (An - C) (d-1) / d，也就是说{An - C}是一个公差为 (d-1)/d 的等比数列。由于(d-1) / d &lt; 1，所以{An - C}会收敛于0，即最终An -&gt; C。因此这行代码就实现了对旋转角度的逐渐逼近。 假如觉得这种推论太坑爹的话，也可以理解为，假设当前到目的地的距离为S，每一帧会移动10/S，下一秒的S’=9S/10 growth是作者提供的一个用于调整左右枝的长短比例的值，同样使用上述的方式实现平滑的过渡。作者提供了一个可以通过鼠标滚轮进行的操作（抱歉这里processing.js不支持java的库）。我将其改成用”jk”键实现了。 很简单的实现，效果也很自然。假如在processing客户端中实现，树枝的层次可以开到17，仍然非常流畅的显示。","categories":[{"name":"Engineering","slug":"Engineering","permalink":"http://fwz.github.io/categories/Engineering/"},{"name":"Visualization","slug":"Engineering/Visualization","permalink":"http://fwz.github.io/categories/Engineering/Visualization/"}],"tags":[{"name":"Processing","slug":"Processing","permalink":"http://fwz.github.io/tags/Processing/"},{"name":"Visualization","slug":"Visualization","permalink":"http://fwz.github.io/tags/Visualization/"}]},{"title":"Plain english at work 读后感","slug":"【读书笔记】《plain-english-at-work》","date":"2012-06-07T04:38:12.000Z","updated":"2018-09-09T07:48:07.000Z","comments":true,"path":"2012/06/07/【读书笔记】《plain-english-at-work》/","link":"","permalink":"http://fwz.github.io/2012/06/07/【读书笔记】《plain-english-at-work》/","excerpt":"","text":"这是一本讲怎么使用plain english进行写作和演说的书，正适合我这种每天要敲英文的green hand.一本比较容易读下来的书，大概用了1周的地铁通勤时间，8小时左右读完。作者的意图是让读者了解到简洁明了地写作和演说的重要性。平时是用什么风格说话的，就应该用同样的风格写作，否则读者需要对内容进行“解码”，降低沟通效率。不要怕老板嫌弃你不够正式，老板只看内容是不是他关心的…… 关于写作，有3件事情需要注意：文风，组织，布局。 文风：尽量不要使用被动语态，要将抽象的事情具体化，灵活运用“？ ： —— ”等标点符号 组织：写好大纲，有效总起/总结 布局：认识不同字体的区别，使用标题对文章进行划分增加可读性，陈列平行观点/事例的时候使用列表，加入图表进行说明关于演说，有2个过程，分别涉及到： 设计阶段：组织内容，如何记住台词，设计增强视觉效果的图案，与观众的互动，使用幽默，排练的重要性等等 演说阶段：检查演说设备，有效地利用辅助设施，以及处理Q&amp;A这书讲的道理是比较简单的，甚至觉得有点像小学作文的大纲、重点句什么的了。这本书介绍了一些简单的手法来提高写作和演说的质量，例如现在我写邮件的时候会运用到里面讲到的layout和heading方面的技巧，有一定效果。但没有传递什么杀手级技巧。其实只要读完脑子里能够有plain english的觉悟，读这本书的目标就达到了。","categories":[{"name":"Read & Learn","slug":"Read-Learn","permalink":"http://fwz.github.io/categories/Read-Learn/"},{"name":"Book Review","slug":"Read-Learn/Book-Review","permalink":"http://fwz.github.io/categories/Read-Learn/Book-Review/"}],"tags":[{"name":"Plain English","slug":"Plain-English","permalink":"http://fwz.github.io/tags/Plain-English/"}]},{"title":"暗时间 读后感","slug":"【读书笔记】《暗时间》","date":"2012-05-16T15:08:21.000Z","updated":"2018-09-09T07:48:34.000Z","comments":true,"path":"2012/05/16/【读书笔记】《暗时间》/","link":"","permalink":"http://fwz.github.io/2012/05/16/【读书笔记】《暗时间》/","excerpt":"很早就拿到这本书了，冲着刘未鹏买的，翻开一看是刘未鹏的博客集。当时有点失望，因为在学校的时候就把mindhack.cn的文章大致读过一下，分享感想的时候也会拿出一些例子来用。不过我觉得有必要重新精读一次，过程中对如何思考、如何解决问题、如何记忆知识等方面的确有了更深刻的体会。建议每个理性的人、想要提高效率的人都读读这本书，这本书总会在某个地方给你当头一棒。 书是在上班的地铁里读的，就算是用碎片时间向作者致敬吧。 在我看来，这本书的最大价值在于告诉读者思考的意义，以及如何更好更高效地学习。在进入大学之前，多数的思考只为给出和标准答案一致的答案，过程公式化，因此往往出现“做题越多，成绩越好”的现象。然而进入大学之后，自由度变大，思考的质量就会对人造成差异巨大的影响， “你比别人看远一年，就多一年的时间去准备，这个差别是巨大的”。假如察觉自己也经常被问题卡住而毫无头绪，那就应该看看这本书，同时推荐《怎样解题》。 我自己的体会就是要主动激发思考，改善思考方法提高产出。 首先是主动性。 其实哪里都有问题可以思考，即使在玩游戏的时候。例如玩魔兽PVP，在游戏的设计和实现中也是有很多事情可以思考的，模型的设计（不知道有多少人会发现搔首弄姿的大树会保持镜头移开之前的姿势），多人对战时的数据传输，各个client如何处理延迟？Replay文件如何设计？多个单位如何同时寻路？友方如何共享单位和视野？高低地的视野如何实现？各个面板可能运用了哪些设计模式？物品的性价比与平衡性等等。这些问题都是很有趣的。甚至只考虑胜负本身，为什么大家初始处于平等的地位，但随着游戏的深入，有一方会取得优势或者胜势？是技巧还是策略？当取得优势后，对手需要多久才能反应过来以及作出应对？我们又可以如何去应对对手的变化？只要不断考虑这些问题，我们就会比其他不善于/勤于思考的玩家更容易赢得游戏。 然后是思考方法。昨天买了本《Proof from the book》，其中有一个关于抽屉原理的题目，我苦思冥想，不能解得。题目如下：","text":"很早就拿到这本书了，冲着刘未鹏买的，翻开一看是刘未鹏的博客集。当时有点失望，因为在学校的时候就把mindhack.cn的文章大致读过一下，分享感想的时候也会拿出一些例子来用。不过我觉得有必要重新精读一次，过程中对如何思考、如何解决问题、如何记忆知识等方面的确有了更深刻的体会。建议每个理性的人、想要提高效率的人都读读这本书，这本书总会在某个地方给你当头一棒。 书是在上班的地铁里读的，就算是用碎片时间向作者致敬吧。 在我看来，这本书的最大价值在于告诉读者思考的意义，以及如何更好更高效地学习。在进入大学之前，多数的思考只为给出和标准答案一致的答案，过程公式化，因此往往出现“做题越多，成绩越好”的现象。然而进入大学之后，自由度变大，思考的质量就会对人造成差异巨大的影响， “你比别人看远一年，就多一年的时间去准备，这个差别是巨大的”。假如察觉自己也经常被问题卡住而毫无头绪，那就应该看看这本书，同时推荐《怎样解题》。 我自己的体会就是要主动激发思考，改善思考方法提高产出。 首先是主动性。 其实哪里都有问题可以思考，即使在玩游戏的时候。例如玩魔兽PVP，在游戏的设计和实现中也是有很多事情可以思考的，模型的设计（不知道有多少人会发现搔首弄姿的大树会保持镜头移开之前的姿势），多人对战时的数据传输，各个client如何处理延迟？Replay文件如何设计？多个单位如何同时寻路？友方如何共享单位和视野？高低地的视野如何实现？各个面板可能运用了哪些设计模式？物品的性价比与平衡性等等。这些问题都是很有趣的。甚至只考虑胜负本身，为什么大家初始处于平等的地位，但随着游戏的深入，有一方会取得优势或者胜势？是技巧还是策略？当取得优势后，对手需要多久才能反应过来以及作出应对？我们又可以如何去应对对手的变化？只要不断考虑这些问题，我们就会比其他不善于/勤于思考的玩家更容易赢得游戏。 然后是思考方法。昨天买了本《Proof from the book》，其中有一个关于抽屉原理的题目，我苦思冥想，不能解得。题目如下：从{1,2,3，… ， n, … , 2n}中任意选出n+1个数，其中必有a,b两个数，使a%b=0。 有兴趣的话可以先想想…… 答案在于一个神来之笔，任何数都可以写成2^k * m 的形式，k&gt;=0 且 m 为奇数。根据抽屉原理，1-2n之间只会有n个不同的奇数，因此n+1个数中，至少有两个数的m值相同，因此可以形成整除。 算是典型的“Then a miracle happen.”类型解法吗？知道答案以后，我一直在问自己，为什么想不到？思维的链条在哪里断裂？我又应该怎样才能重新拼起来。 断裂： 2^k * m对我是一个陌生的概念，假如之前不知道这个式子的存在，需要对数字很敏感才可以发现这一点。 重组：知道答案了以后，可以尝试将思路往答案上靠。例如集合的秩是2n,选出n+1，恩，可能和2有关；n+1这个条件看起来很诡异，我们可以试试是不是能够选出n个，观察出什么规律来：取n=5, {1,3,5,7,9}，恩这个不行。{2,3,5,7,9}，也不行。试试小一点的，取n=4, {3,5,7,8}, cool ! 构造出来一个边界集合。还能构造吗？{2,3,5,7}可以，{3,4,5,7}也可以。看来2,4,8是一点线索。还能想吗?其实还可以构造如下的序列，取n=10，{11,12,13,14,15,16,17,18,19,20}。这时我发现每增加一个数的时候，就会有一个数是它的两倍，从而需要剔除，这时我想到了n和2n的关系，很可能和奇偶有关。 很遗憾，我在问了自己这些问题以后，觉得自己从这些线索推出这个解题的关键点的概率不会超过5%。不过我已经觉得思考的质量有了一点点提高了。 进一步讲，这个问题可能需要直觉，但更多的问题需要关键知识。缺乏这些关键知识，然后解题思路断裂。这种情况被称为“Unknown Unknown”，我们不可能用自己不知道的知识去解决问题。因此平时拓宽视野是很重要的，工作再多再忙，也要保持学习。 然后是思维的惯性，看到作者讲ATM的门的趣事，让我想起自己一次典型的思维错误： “我觉得我的推断是唯一靠谱的，从而根本无法看到或者设想另外一种可能性。某次在用电动牙刷刷牙的时候过去隔壁寝室串门，然后惊讶的发现，刷牙的时候，同学的显示器竟然会抖，心想这个电磁辐射也太厉害了。把牙刷关了，屏幕也就不抖了。于是开始开开关关逗同学玩，谁知他毫无反应。我气急败坏地问他你不觉得屏幕在抖吗，他说不觉得啊，这时候我愣了，不可能吧，难道我幻视？后来另外一个同学过来，我又问了他，他同样表示没有反应……我又愣了一阵子，才发觉原来是电动牙刷在抖我…… 最后，推荐我觉得很好的几篇： 暗时间 一直以来伴随我的一些学习习惯 为什么你从现在开始就应该写博客 遇到问题为什么应该自己动手 (must read) 什么才是你的不可替代性和核心竞争力 跟波利亚学解题 知其所以然 快排为什么这样快 康托尔、哥德尔、图灵——永恒的金色对角线&nbsp; 顺便讲讲读《黑客与画家》的感受 其实这本书完全是散文，想到当初有书店把它归到计算机安全类……作者的很多观念都是崭新的，用的例子小而贴切，读下来感觉很流畅。奇怪的是，这本书里面的文章的很多观点引起过大量的争议，但我并没有感受到新思想的强烈冲击。不知道是我只会接收别人的思考结晶作为知识，还是我潜意识里面也是这么想的。 推荐我觉得不容错过的几篇： 设计者的品味——优秀作品的秘诀是：非常严格的品味，再加上实现这种品味的能力。 拒绝平庸——利用（普通编程语言主导一切的）习惯势力削弱你的对手。 如何创造财富——金钱并不等于财富，创造有价值的东西才是创造财富。&nbsp; &nbsp;","categories":[{"name":"Read & Learn","slug":"Read-Learn","permalink":"http://fwz.github.io/categories/Read-Learn/"},{"name":"Book Review","slug":"Read-Learn/Book-Review","permalink":"http://fwz.github.io/categories/Read-Learn/Book-Review/"}],"tags":[{"name":"Thinking","slug":"Thinking","permalink":"http://fwz.github.io/tags/Thinking/"}]},{"title":"expect 脚本编程","slug":"expect-脚本编程","date":"2012-05-16T06:36:30.000Z","updated":"2016-09-05T03:39:54.000Z","comments":true,"path":"2012/05/16/expect-脚本编程/","link":"","permalink":"http://fwz.github.io/2012/05/16/expect-脚本编程/","excerpt":"在多个机器上批量执行任务脚本的时候，经常需要输ssh密码，但是出于安全性（明文密码命令容易被history出来）和交互性命令（ssh等命令需要强制用户输入而不能通过参数输入）的限制，需要找一些新的办法。拍脑袋想到的有2个。 1、将自己的ssh 公钥上传到这些机器上（但第一次上传的时候，依然需要输密码，又绕回去了） 2、使用ssh -F configfile （看了下man ssh 和 man ssh_config，感觉太难配） 网上搜索了一阵，发现 expect 脚本编程。大概来说，基本上登录交互的东西都可以搞定。 expect - programmed dialogue with interactive programs, Version 5 （一个可以编程的交互对话程序）下面上代码，完成登录，执行cmd command，返回结果，退出的功能。 1 #!/usr/bin/expect -f23 # – set all file list –4 #set db_box myssh-target5 set db_box [lrange $argv 0 0]6 set pw mypassword78 spawn ssh $db_box9 set timeout -11011 expect {12 “Are you sure you want to continue connecting (yes/no)? “ {13 send “yes\\r”;14 exp_continue15 }16 “hector@$db_box’s password: “ {17 send “$pw\\r”18 set timeout -119 expect “*bash-3.2$ “ {20 send “grep topic= yinst set | grep coke_queue_daemon.daemon_dir | cut -f2 -d &#39;:&#39;/subscription/ -r | cut -f2 -d ‘:’ | cut -f2 -d ‘=’\\r”21 send “exit\\r”22 }23 }24 }25 interact expect的精髓就在于这个词本身，你expect什么输出，对应什么输入，都可以控制。具体情况man写得很详细，但一般工作照着上面这个改就可以了。","text":"在多个机器上批量执行任务脚本的时候，经常需要输ssh密码，但是出于安全性（明文密码命令容易被history出来）和交互性命令（ssh等命令需要强制用户输入而不能通过参数输入）的限制，需要找一些新的办法。拍脑袋想到的有2个。 1、将自己的ssh 公钥上传到这些机器上（但第一次上传的时候，依然需要输密码，又绕回去了） 2、使用ssh -F configfile （看了下man ssh 和 man ssh_config，感觉太难配） 网上搜索了一阵，发现 expect 脚本编程。大概来说，基本上登录交互的东西都可以搞定。 expect - programmed dialogue with interactive programs, Version 5 （一个可以编程的交互对话程序）下面上代码，完成登录，执行cmd command，返回结果，退出的功能。 1 #!/usr/bin/expect -f23 # – set all file list –4 #set db_box myssh-target5 set db_box [lrange $argv 0 0]6 set pw mypassword78 spawn ssh $db_box9 set timeout -11011 expect {12 “Are you sure you want to continue connecting (yes/no)? “ {13 send “yes\\r”;14 exp_continue15 }16 “hector@$db_box’s password: “ {17 send “$pw\\r”18 set timeout -119 expect “*bash-3.2$ “ {20 send “grep topic= yinst set | grep coke_queue_daemon.daemon_dir | cut -f2 -d &#39;:&#39;/subscription/ -r | cut -f2 -d ‘:’ | cut -f2 -d ‘=’\\r”21 send “exit\\r”22 }23 }24 }25 interact expect的精髓就在于这个词本身，你expect什么输出，对应什么输入，都可以控制。具体情况man写得很详细，但一般工作照着上面这个改就可以了。 Note： [lrange $argv l r] 取命令行参数的第l个到第r个（从0开始）。 spawn ssh $db_box 通过spawn 执行ssh命令（ssh 命令其实是可以后接cmd命令的，但是直接将20行的cmd命令接到ssh后面会因为引号无法解析，所以采取了一种walk around 的方法在下面发送命令） timeout使用-1比较合适，表示一直等待响应。 这样就可以一次性在多个机器上执行操作了。","categories":[{"name":"Engineering","slug":"Engineering","permalink":"http://fwz.github.io/categories/Engineering/"},{"name":"Productivity","slug":"Engineering/Productivity","permalink":"http://fwz.github.io/categories/Engineering/Productivity/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://fwz.github.io/tags/Linux/"},{"name":"Expect","slug":"Expect","permalink":"http://fwz.github.io/tags/Expect/"},{"name":"Shell","slug":"Shell","permalink":"http://fwz.github.io/tags/Shell/"},{"name":"Automation","slug":"Automation","permalink":"http://fwz.github.io/tags/Automation/"}]},{"title":"反思近期的公开演讲","slug":"反思近期的演讲技能","date":"2012-04-07T15:54:12.000Z","updated":"2016-09-05T03:39:54.000Z","comments":true,"path":"2012/04/07/反思近期的演讲技能/","link":"","permalink":"http://fwz.github.io/2012/04/07/反思近期的演讲技能/","excerpt":"近期讲了好几次，讲得比较投入的是学院求职交流会：特意下了现场视频（http://bt.byr.cn/details.php?id=85810&amp;hit=16）来分析一下问题，一看果然问题很多：1 仪态方面，经常存在搓手、托眼镜等小动作，最不能忍的是驼背……想了想，多余的小动作可以通过插袋等强制性方法避免，剩下的就是要注意手势的运用需要明确，不要做出让观众分心但又毫无信息量的零碎动作，感谢Neo同学指出。","text":"近期讲了好几次，讲得比较投入的是学院求职交流会：特意下了现场视频（http://bt.byr.cn/details.php?id=85810&amp;hit=16）来分析一下问题，一看果然问题很多：1 仪态方面，经常存在搓手、托眼镜等小动作，最不能忍的是驼背……想了想，多余的小动作可以通过插袋等强制性方法避免，剩下的就是要注意手势的运用需要明确，不要做出让观众分心但又毫无信息量的零碎动作，感谢Neo同学指出。 2 普通话太挫了。好多人说我普通话不像南方来的，让我一度飘飘然。但是原来自己讲话口音如此之挫。这个无解了，就那样吧。 3 和观众的交流不足。36分钟的介绍，只有三次交流：A 有多少同学知道github？ B 有多少同学一年能读10本书或以上？ 最后提问…… 4 存在声音头重脚轻的情况，发现很多人说话都这样。反正要重点关注。 5 口头禅严重。“就是说……”一词保守估计至少出现了20次。和演讲中提倡的clear articulation有较大的差距，要大量练习。 6 结尾时没有强调主题（目前要做的事情和收获），应该给他们打支强心针。在演讲中鼓舞听众是除了令听众有所收获之外的一个重要目的。 7 内容。其实这份PPT的内容并非一般听众普遍希望听到的捷径，里面提到的东西都略显长远，有些更是我今后的愿景。所以可以说我的目标听众应该是研一的同学。研二的同学马上要开始找实习了，由于没有什么抱佛脚经验，就不想胡说了，而且这些东西网上一搜比比皆是，我还是希望能够来一出不一样的speech。而且有些概念对某些同学来说过于陌生，有可能让他们觉得互联网求职很可怕。因此，选题上虽然我选了自己真正觉得有意义的主题，但忽视了和听众的需求是否对口，越想越严重，这一点在以后要绝对避免。 8 好的地方也是有的，对于自己思考得比较多的领域，可以娓娓道来。PPT只做了25分钟，非常随意，但是还算言之有物，有干货。PPT 在这里， Be Different ，有兴趣阅读的同学，建议结合视频收看。演讲完了以后微博竟然多了20粉，后来大概有5-6个人和我私信邮件什么的聊了一下，还算是不错了，而且感觉弟妹们是一代更胜一代了，我要好好努力。anyway, 能够帮到哪怕3个人，也满足了。假如执行力足够的话，研二的同学在接下来这段时间，还是可以拉开差距的。","categories":[{"name":"Career","slug":"Career","permalink":"http://fwz.github.io/categories/Career/"},{"name":"Public Speaking","slug":"Career/Public-Speaking","permalink":"http://fwz.github.io/categories/Career/Public-Speaking/"}],"tags":[{"name":"Public Speaking","slug":"Public-Speaking","permalink":"http://fwz.github.io/tags/Public-Speaking/"}]},{"title":"互联网找工作总结1：准备，技巧和选择","slug":"找工作总结1：准备，技巧和选择","date":"2012-04-07T13:09:44.000Z","updated":"2016-09-05T03:39:54.000Z","comments":true,"path":"2012/04/07/找工作总结1：准备，技巧和选择/","link":"","permalink":"http://fwz.github.io/2012/04/07/找工作总结1：准备，技巧和选择/","excerpt":"文章在北邮人上首发，本文作了进一步的修缮。转载请注明出处。 找工作基本告一段落了，历时两个月，总结了一下过程和感想，希望对仍在奋战的同学或师弟师妹有所帮助。 找工作前：1 多动手解决实际问题，主动折腾实验室的项目或者自己感兴趣的事情。来自别人的知识很容易遗忘，来自自己的知识是最扎实的。也可以读读“如何用两年时间面试一个人”这篇文章和相关延伸文章，看看高水平的面试官都想要怎样的人。没事上北邮的OJ刷一下acm题对学习算法和现场写代码都很有帮助。我没有深入学习过算法，但是认为学算法在找工作中的性价比（特别是互联网）还是很高的。 2 确定自己的方向。首先要找到自己的核心竞争力，这个竞争力是多元的，有人算法厉害，有人懂设计模式工程能力超NB，有人做页面效果很炫丽……不妨问问自己，最擅长的是哪个，最喜欢做的是哪个。假如能够找到两个方面，你能在周围的人里面都做到前10%，那找到一个同时需要这两方面能力的工作，你的竞争力就是10%*10% = top 1%了（先抛开这两个东西是不是独立的吧呵呵）。 3 写简历。首先是布局，简历的制作和PPT是类似的，需要保持一致性和优美的格式，页面上的字体除了姓名以外不要超过2种。听听其他人的意见，多多修改，杜绝错字。然后是内容，内容的选择看个人，我学习的东西多而杂，为了凸显各个方面都有尝试，覆盖了很多内容，双刃剑。很多面试官对简历上写兴趣广泛的人都没什么好感，在他们看来这不是优点。同时还导致要准备面试官进行扩展提问，因此对照简历准备的时候需要花更大的精力补上盲点。另一方面，有些地方就喜欢要这样的人，觉得这样的人有热情，有潜力，有主动性。所以，大家还是分职位去准备简历吧。","text":"文章在北邮人上首发，本文作了进一步的修缮。转载请注明出处。 找工作基本告一段落了，历时两个月，总结了一下过程和感想，希望对仍在奋战的同学或师弟师妹有所帮助。 找工作前：1 多动手解决实际问题，主动折腾实验室的项目或者自己感兴趣的事情。来自别人的知识很容易遗忘，来自自己的知识是最扎实的。也可以读读“如何用两年时间面试一个人”这篇文章和相关延伸文章，看看高水平的面试官都想要怎样的人。没事上北邮的OJ刷一下acm题对学习算法和现场写代码都很有帮助。我没有深入学习过算法，但是认为学算法在找工作中的性价比（特别是互联网）还是很高的。 2 确定自己的方向。首先要找到自己的核心竞争力，这个竞争力是多元的，有人算法厉害，有人懂设计模式工程能力超NB，有人做页面效果很炫丽……不妨问问自己，最擅长的是哪个，最喜欢做的是哪个。假如能够找到两个方面，你能在周围的人里面都做到前10%，那找到一个同时需要这两方面能力的工作，你的竞争力就是10%*10% = top 1%了（先抛开这两个东西是不是独立的吧呵呵）。 3 写简历。首先是布局，简历的制作和PPT是类似的，需要保持一致性和优美的格式，页面上的字体除了姓名以外不要超过2种。听听其他人的意见，多多修改，杜绝错字。然后是内容，内容的选择看个人，我学习的东西多而杂，为了凸显各个方面都有尝试，覆盖了很多内容，双刃剑。很多面试官对简历上写兴趣广泛的人都没什么好感，在他们看来这不是优点。同时还导致要准备面试官进行扩展提问，因此对照简历准备的时候需要花更大的精力补上盲点。另一方面，有些地方就喜欢要这样的人，觉得这样的人有热情，有潜力，有主动性。所以，大家还是分职位去准备简历吧。 4 投简历。确定方向以后，就去投。但不要海投，面试跑来跑去挤公交车的感觉让人真的想投海，还不如省下点时间好好看书。当然，也不能全投特别好的公司，不然状态起不来，很容易就挂掉。初期可以先投一些公司练练手，预先感受一下面试的感觉，调整出一个比较好的状态。不过要注意的是，互联网每年的形势都不一样，去年如此今年不一定就这样。例如今年搜狗突然给力了，但我压根就没投（因为我觉得除了输入法和浏览器好像搜狗发展前景一般……），比较悲催。 5 可以到各个渠道去找工作信息，不要只是局限于论坛。例如现在好些技术牛人都会在微博上发招聘信息，感觉自己有希望的话，主动投一下。在牛人身边学习还是会快很多，无论是工作方式、习惯、视野等等，都不是一个普通团队可以企及的。另外，提供一个渠道让别人找到你。平时多干点事情，求职时通过各种媒介主动宣传一下自己，我的确因为自我推销收到过一个额外邀请。 找工作时准备：1 看书补充基础知识。《程序员面试宝典》之类过一遍总是好的。有理想有追求的同学可以看看《深入理解计算机系统》，《The C Programming language》，目标工作对C要求高的可以去试试看《C专家编程》。 2 重点看《编程之美》。请好好阅读，并认真写代码，调通并测试。面试过程中，发现不会的题有70%以上都在编程之美上……我吃亏的地方就是在找实习那一波草草的看过一次，找工作这一波没看，好些东西都忘了。面试悲剧大都起源于这里。编程珠玑系列、网络上的程序员面试系列博客有时间也可以多多关注，依然建议写代码。 3 找些好战友。在求职的过程中，战友的水平是很关键的。两个人的盲点互补，效果会比一个人默默看书好；讨论问题很容易得到正确的结果；假如一起参加笔试了，只有一个过了，霸面也能知道点消息；再阴暗点说，笔试也可以相互关照一下。 4 不要轻易松懈。前中期我找工作都比较顺利，也感觉各家的面试大概都那样了，所以减轻了给自己的压力，偶尔还打打游戏什么的。导致后期的面试无论是想算法还是写代码的状态，都不怎么好了，很多好的offer都在后头，想起来还是很可惜的，希望大家不要重蹈我的覆辙。觉得自己要懈怠下来的时候，就好好想想第一份工作的意义，再问问自己：按着目前的能力，可以如此奢侈地不努力吗？ 笔试时： 没什么好说的了，基本的门槛，不是要求太高的话，通常都能过。不会的也不要放弃，用意志坚持住，越用心做，机会就越大一分。我完美的图形学部分一点不会，但是感觉自己推理一下，还是可以推出来一点东西的，硬是答了半份…… 程序题时间充裕的话，最好先打个草稿吧。这样卷面能整洁一些，不然鬼画符一样，即使程序是对的，改卷的工程师也不乐意给满分。对于题面描述有漏洞或者根本理解不了的，我会幽默点吐槽，估计这样改卷的工程师心情也容易好一些。 面试时： 0、表现出自信。气场足是成功的开始。 1、遇到挫折时心态要放好。笔试和面试随机性都挺大的。笔试可以通过团队作战减少不稳定的因素，面试就不一样了，面试官不一样的情况下，很可能两个求职者的命运也截然不同，不管两个人的水平和最后的结果是否相对匹配。因此，没有必要因为别人过了自己没过而遗憾难过。无论过程中遇到什么挫折或者困难，都需要保持一个积极乐观的心态去应对，面试官很容易就能感受到求职者的情绪。 2、做好总结。我每去一次面试回来都会写一个总结，过了会写长一些，从成功中总结成功，反之会短一些，从失败中总结失败。但是一定要总结，特别是不会的题目，第一时间搞清楚记下来。根据生日攻击理论，假如总的面试题目有365道，那随机抽23道，就有50%的概率出现重复。不想被再次恶心到的话，就搞明白好了。能面试的算法题目大概来来去去也就这么几道。关于面试，过段时间会单开一帖，把我经历的所有面试整理出来。这里先放下承诺，以免日后没有动力。 3、写代码一定要review。释放内存，边界条件，循环退出条件等等。时间再紧也要拿一个case出来，把程序走一遍，走通了再让面试官看。只要活好，面试官不会嫌做得慢。 4、鼓起勇气霸笔霸面。霸笔情况太多了，我经常收不到通知，反正觉得公司靠谱的就去霸。除了微软这种明文禁止的没去，100%的成功率。霸面就比较讲技巧和RP——一是要表示出诚意和和蔼的态度，二是要把自己的亮点展示出来，三是礼貌的表示自己可以等，这样HR基本撵不走了。剩下的就是看RP了，一旦有同学没来面试官有空的话，机会就到了。 5、随身携带中英文简历。有时HR可能会忘了让你带简历，你真不带的话，万一……，在面试官那里，印象会比较吃亏。 腾讯HR面的时候忘了带，HR还稍微诘问了一下。 6、充分准备好英文相关内容，最好能找个有共同需求的同学一起联系。我口语还算OK，但第一次自我介绍的时候真的是挫得一塌糊涂，准备过后效果就完全不一样了。全英文面试基本就不能临急抱佛脚了，但最差也要mark几个关键技术的关键字，例如polymorphism啊，object-oriented programming啊，千万要会。 选择： 1、选行业，选公司真的没有一概的定论。各取所需，有人就爱技术昼夜编程，有人喜欢安稳的节奏，有人有伟大的事业追求……没有高低之分。只有适合不适合一说。现在我的准则有三个： a 可以做我喜欢做的事情。假如不认同自己的工作，每天去工作就是完成任务了。那会是很不爽的。当然每份工作都会有脏活累活，看看比例是不是能够接受吧。后来和腾讯的一个VP交流，他也说枯燥的东西肯定难以避免，但是假如你有能力从无聊的工作中找到有意思的部分，那也很不错。我真希望我能培养出这种能力。 b 周围有比我厉害的人。有了好同事，才能快速进步。再刨深一些就是要跟对老大，我定义好老大有三个条件：1、有眼光和阅历。2、对你欣赏，肯花时间给你建议，给你机会。3、不轻易跳槽。跟了好老大，行业平台等其实于我看都是浮云。 c work life balance. 这个倒是看自己的情况了，我除了编程以外运动音乐什么的都很爱好，所以目前还是选择了留一点时间，做做自己喜欢的事情，结识新的朋友，参加些有趣的活动等等。毕竟我争取找一份好的工作的初衷是为了给自己和家人更好的生活。有人也许会反驳，安逸的环境成长慢，这样的工作没有意义，或者这些事情是要分阶段考虑的等等，但还是要考虑身体和家人等，很多东西是不可逆的。当前的想法是这些，也有可能是我还没遇到让我奋不顾身的事业吧，仅供参考。ps 我一直不认为户口或者房子重要。这个问题我找了很多人听取他们的意见，他们第一口都说为了孩子。但是假如为了让孩子上个好学校就把自己全身心卖给房地产商和老板，反而是一件得不偿失的事情，我相信家庭的教育对孩子的成长最关键。 2、公司信息获取。八仙过海吧，上坛子问，找在公司里工作的师兄师姐问，或者加招聘群等等都可以。这个时候人脉的作用就体现出来了，平时多给师兄师姐打打下手没啥不好的，吃亏就是占便宜。 3、纠结的时候，相信自己的直觉。例如有人问搜狗和人搜，我觉得差不多的话就follow自己的第一感觉吧。纠结主要是因为两种原因：1 信息量太少，无法决策； 2 两者之间真的没有什么差别。 把握足够主干信息以后，一些细微的差别，可能就不用太较真了。 第一次写这么长的文章，大概写了90分钟了……希望大家理性探讨吧。 最后，感谢找工作阶段给过我帮助和指导的以下（且不限于）同学：dr,lala,quan,xiaohuo,wenqing,niki,卢MM，inter，还有一直鼓励我的GF。祝大家都能拿到满意的offer。 ————————————–","categories":[{"name":"Career","slug":"Career","permalink":"http://fwz.github.io/categories/Career/"},{"name":"Interview","slug":"Career/Interview","permalink":"http://fwz.github.io/categories/Career/Interview/"}],"tags":[{"name":"Career","slug":"Career","permalink":"http://fwz.github.io/tags/Career/"}]}]}